
✅ hippocampus_389.nii.gz: img max=1.0000, lbl sum=4131
✅ hippocampus_334.nii.gz: img max=1.0000, lbl sum=4114
✅ hippocampus_345.nii.gz: img max=1.0000, lbl sum=4275
✅ hippocampus_178.nii.gz: img max=1.0000, lbl sum=4204
✅ hippocampus_025.nii.gz: img max=1.0000, lbl sum=4756
✅ hippocampus_301.nii.gz: img max=1.0000, lbl sum=5325
✅ hippocampus_303.nii.gz: img max=1.0000, lbl sum=4830
✅ hippocampus_277.nii.gz: img max=1.0000, lbl sum=4913
✅ hippocampus_335.nii.gz: img max=1.0000, lbl sum=3801
✅ hippocampus_046.nii.gz: img max=1.0000, lbl sum=4973
✅ hippocampus_378.nii.gz: img max=1.0000, lbl sum=4334
✅ hippocampus_244.nii.gz: img max=1.0000, lbl sum=4719
✅ hippocampus_172.nii.gz: img max=1.0000, lbl sum=5865
✅ hippocampus_387.nii.gz: img max=1.0000, lbl sum=4651
✅ hippocampus_290.nii.gz: img max=1.0000, lbl sum=4875
✅ hippocampus_023.nii.gz: img max=1.0000, lbl sum=5388
✅ hippocampus_099.nii.gz: img max=1.0000, lbl sum=3818
✅ hippocampus_274.nii.gz: img max=1.0000, lbl sum=3839
✅ hippocampus_144.nii.gz: img max=1.0000, lbl sum=3715
✅ hippocampus_149.nii.gz: img max=1.0000, lbl sum=4666
✅ hippocampus_386.nii.gz: img max=1.0000, lbl sum=4457
✅ hippocampus_230.nii.gz: img max=1.0000, lbl sum=4774
✅ hippocampus_289.nii.gz: img max=1.0000, lbl sum=4194
✅ hippocampus_207.nii.gz: img max=1.0000, lbl sum=6161
✅ hippocampus_296.nii.gz: img max=1.0000, lbl sum=5738
✅ hippocampus_040.nii.gz: img max=1.0000, lbl sum=4984
✅ hippocampus_060.nii.gz: img max=1.0000, lbl sum=5092
✅ hippocampus_058.nii.gz: img max=1.0000, lbl sum=4920
✅ hippocampus_297.nii.gz: img max=1.0000, lbl sum=4021
✅ hippocampus_053.nii.gz: img max=1.0000, lbl sum=5388
✅ hippocampus_114.nii.gz: img max=1.0000, lbl sum=4949
✅ hippocampus_088.nii.gz: img max=1.0000, lbl sum=5646
✅ hippocampus_338.nii.gz: img max=1.0000, lbl sum=5239
✅ hippocampus_326.nii.gz: img max=1.0000, lbl sum=5831
✅ hippocampus_217.nii.gz: img max=1.0000, lbl sum=4184
✅ hippocampus_033.nii.gz: img max=1.0000, lbl sum=4991
✅ hippocampus_325.nii.gz: img max=1.0000, lbl sum=5770
✅ hippocampus_019.nii.gz: img max=1.0000, lbl sum=4824
✅ hippocampus_150.nii.gz: img max=1.0000, lbl sum=4571
✅ hippocampus_373.nii.gz: img max=1.0000, lbl sum=5496
✅ hippocampus_048.nii.gz: img max=1.0000, lbl sum=4587
✅ hippocampus_295.nii.gz: img max=1.0000, lbl sum=5349
✅ hippocampus_375.nii.gz: img max=1.0000, lbl sum=4758
✅ hippocampus_065.nii.gz: img max=1.0000, lbl sum=5570
✅ hippocampus_133.nii.gz: img max=1.0000, lbl sum=4919
✅ hippocampus_363.nii.gz: img max=1.0000, lbl sum=5174
✅ hippocampus_328.nii.gz: img max=1.0000, lbl sum=5543
✅ hippocampus_355.nii.gz: img max=1.0000, lbl sum=4851
✅ hippocampus_093.nii.gz: img max=1.0000, lbl sum=5722
✅ hippocampus_298.nii.gz: img max=1.0000, lbl sum=3916
✅ hippocampus_268.nii.gz: img max=1.0000, lbl sum=4671
✅ hippocampus_199.nii.gz: img max=1.0000, lbl sum=3982
✅ hippocampus_155.nii.gz: img max=1.0000, lbl sum=5064
✅ hippocampus_038.nii.gz: img max=1.0000, lbl sum=5279
✅ hippocampus_185.nii.gz: img max=1.0000, lbl sum=4385
✅ hippocampus_162.nii.gz: img max=1.0000, lbl sum=5040
✅ hippocampus_064.nii.gz: img max=1.0000, lbl sum=5276
✅ hippocampus_174.nii.gz: img max=1.0000, lbl sum=6100
✅ hippocampus_374.nii.gz: img max=1.0000, lbl sum=5631
✅ hippocampus_318.nii.gz: img max=1.0000, lbl sum=5003
✅ hippocampus_330.nii.gz: img max=1.0000, lbl sum=5762
✅ hippocampus_197.nii.gz: img max=1.0000, lbl sum=4970
✅ hippocampus_286.nii.gz: img max=1.0000, lbl sum=4782
✅ hippocampus_156.nii.gz: img max=1.0000, lbl sum=5098
✅ hippocampus_164.nii.gz: img max=1.0000, lbl sum=6038
✅ hippocampus_233.nii.gz: img max=1.0000, lbl sum=4741
✅ hippocampus_044.nii.gz: img max=1.0000, lbl sum=4748
✅ hippocampus_321.nii.gz: img max=1.0000, lbl sum=4299
✅ hippocampus_261.nii.gz: img max=1.0000, lbl sum=5743
✅ hippocampus_020.nii.gz: img max=1.0000, lbl sum=5076
✅ hippocampus_089.nii.gz: img max=1.0000, lbl sum=5647
✅ hippocampus_226.nii.gz: img max=1.0000, lbl sum=3728
✅ hippocampus_123.nii.gz: img max=1.0000, lbl sum=4746
✅ hippocampus_146.nii.gz: img max=1.0000, lbl sum=5027
✅ hippocampus_216.nii.gz: img max=1.0000, lbl sum=5002
✅ hippocampus_356.nii.gz: img max=1.0000, lbl sum=5129
✅ hippocampus_235.nii.gz: img max=1.0000, lbl sum=4601
✅ hippocampus_015.nii.gz: img max=1.0000, lbl sum=4127
✅ hippocampus_051.nii.gz: img max=1.0000, lbl sum=4697
✅ hippocampus_331.nii.gz: img max=1.0000, lbl sum=4827
✅ hippocampus_390.nii.gz: img max=1.0000, lbl sum=4618
✅ hippocampus_125.nii.gz: img max=1.0000, lbl sum=3795
✅ hippocampus_173.nii.gz: img max=1.0000, lbl sum=5575
✅ hippocampus_279.nii.gz: img max=1.0000, lbl sum=3503
✅ hippocampus_224.nii.gz: img max=1.0000, lbl sum=5607
✅ hippocampus_148.nii.gz: img max=1.0000, lbl sum=4201
✅ hippocampus_234.nii.gz: img max=1.0000, lbl sum=4883
✅ hippocampus_024.nii.gz: img max=1.0000, lbl sum=6049
✅ hippocampus_101.nii.gz: img max=1.0000, lbl sum=5300
✅ hippocampus_264.nii.gz: img max=1.0000, lbl sum=5750
✅ hippocampus_135.nii.gz: img max=1.0000, lbl sum=3895
✅ hippocampus_084.nii.gz: img max=1.0000, lbl sum=4910
✅ hippocampus_232.nii.gz: img max=1.0000, lbl sum=4212
✅ hippocampus_194.nii.gz: img max=1.0000, lbl sum=4427
✅ hippocampus_257.nii.gz: img max=1.0000, lbl sum=4538
✅ hippocampus_309.nii.gz: img max=1.0000, lbl sum=5442
✅ hippocampus_276.nii.gz: img max=1.0000, lbl sum=5230
✅ hippocampus_127.nii.gz: img max=1.0000, lbl sum=5404
✅ hippocampus_039.nii.gz: img max=1.0000, lbl sum=5278
✅ hippocampus_385.nii.gz: img max=1.0000, lbl sum=4681
✅ hippocampus_287.nii.gz: img max=1.0000, lbl sum=5634
✅ hippocampus_336.nii.gz: img max=1.0000, lbl sum=3888
✅ hippocampus_177.nii.gz: img max=1.0000, lbl sum=4132
✅ hippocampus_260.nii.gz: img max=1.0000, lbl sum=4815
✅ hippocampus_305.nii.gz: img max=1.0000, lbl sum=4409
✅ hippocampus_042.nii.gz: img max=1.0000, lbl sum=5823
✅ hippocampus_282.nii.gz: img max=1.0000, lbl sum=3699
✅ hippocampus_141.nii.gz: img max=1.0000, lbl sum=4193
✅ hippocampus_358.nii.gz: img max=1.0000, lbl sum=4255
✅ hippocampus_383.nii.gz: img max=1.0000, lbl sum=5063
✅ hippocampus_242.nii.gz: img max=1.0000, lbl sum=6289
✅ hippocampus_075.nii.gz: img max=1.0000, lbl sum=4874
✅ hippocampus_090.nii.gz: img max=1.0000, lbl sum=5958
✅ hippocampus_204.nii.gz: img max=1.0000, lbl sum=4104
✅ hippocampus_311.nii.gz: img max=1.0000, lbl sum=4837
✅ hippocampus_095.nii.gz: img max=1.0000, lbl sum=5379
✅ hippocampus_092.nii.gz: img max=1.0000, lbl sum=4796
✅ hippocampus_366.nii.gz: img max=1.0000, lbl sum=5476
✅ hippocampus_108.nii.gz: img max=1.0000, lbl sum=5821
✅ hippocampus_265.nii.gz: img max=1.0000, lbl sum=4763
✅ hippocampus_034.nii.gz: img max=1.0000, lbl sum=4920
✅ hippocampus_252.nii.gz: img max=1.0000, lbl sum=5090
✅ hippocampus_011.nii.gz: img max=1.0000, lbl sum=5000
✅ hippocampus_292.nii.gz: img max=1.0000, lbl sum=5047
✅ hippocampus_376.nii.gz: img max=1.0000, lbl sum=5537
✅ hippocampus_332.nii.gz: img max=1.0000, lbl sum=4851
✅ hippocampus_124.nii.gz: img max=1.0000, lbl sum=4668
✅ hippocampus_308.nii.gz: img max=1.0000, lbl sum=5297
✅ hippocampus_094.nii.gz: img max=1.0000, lbl sum=5696
✅ hippocampus_077.nii.gz: img max=1.0000, lbl sum=5452
✅ hippocampus_017.nii.gz: img max=1.0000, lbl sum=4821
✅ hippocampus_106.nii.gz: img max=1.0000, lbl sum=4472
✅ hippocampus_001.nii.gz: img max=1.0000, lbl sum=4572
✅ hippocampus_236.nii.gz: img max=1.0000, lbl sum=4583
✅ hippocampus_170.nii.gz: img max=1.0000, lbl sum=4429
✅ hippocampus_319.nii.gz: img max=1.0000, lbl sum=3464
✅ hippocampus_154.nii.gz: img max=1.0000, lbl sum=4915
✅ hippocampus_142.nii.gz: img max=1.0000, lbl sum=4072
✅ hippocampus_188.nii.gz: img max=1.0000, lbl sum=5302
✅ hippocampus_253.nii.gz: img max=1.0000, lbl sum=5303
✅ hippocampus_304.nii.gz: img max=1.0000, lbl sum=5056
✅ hippocampus_145.nii.gz: img max=1.0000, lbl sum=4998
✅ hippocampus_190.nii.gz: img max=1.0000, lbl sum=4953
✅ hippocampus_126.nii.gz: img max=1.0000, lbl sum=4640
✅ hippocampus_299.nii.gz: img max=1.0000, lbl sum=4887
✅ hippocampus_036.nii.gz: img max=1.0000, lbl sum=5169
✅ hippocampus_220.nii.gz: img max=1.0000, lbl sum=4010
✅ hippocampus_316.nii.gz: img max=1.0000, lbl sum=4610
✅ hippocampus_215.nii.gz: img max=1.0000, lbl sum=5066
✅ hippocampus_169.nii.gz: img max=1.0000, lbl sum=4355
✅ hippocampus_041.nii.gz: img max=1.0000, lbl sum=5749
✅ hippocampus_259.nii.gz: img max=1.0000, lbl sum=4502
✅ hippocampus_393.nii.gz: img max=1.0000, lbl sum=5500
✅ hippocampus_227.nii.gz: img max=1.0000, lbl sum=5093
✅ hippocampus_361.nii.gz: img max=1.0000, lbl sum=4477
✅ hippocampus_087.nii.gz: img max=1.0000, lbl sum=5481
✅ hippocampus_269.nii.gz: img max=1.0000, lbl sum=5153
✅ hippocampus_189.nii.gz: img max=1.0000, lbl sum=5129
✅ hippocampus_341.nii.gz: img max=1.0000, lbl sum=3845
✅ hippocampus_243.nii.gz: img max=1.0000, lbl sum=4491
✅ hippocampus_212.nii.gz: img max=1.0000, lbl sum=5418
✅ hippocampus_098.nii.gz: img max=1.0000, lbl sum=4265
✅ hippocampus_225.nii.gz: img max=1.0000, lbl sum=3831
✅ hippocampus_322.nii.gz: img max=1.0000, lbl sum=4921
✅ hippocampus_351.nii.gz: img max=1.0000, lbl sum=5626
✅ hippocampus_158.nii.gz: img max=1.0000, lbl sum=4950
✅ hippocampus_203.nii.gz: img max=1.0000, lbl sum=4128
✅ hippocampus_045.nii.gz: img max=1.0000, lbl sum=4490
✅ hippocampus_288.nii.gz: img max=1.0000, lbl sum=5840
✅ hippocampus_107.nii.gz: img max=1.0000, lbl sum=5981
✅ hippocampus_302.nii.gz: img max=1.0000, lbl sum=5503
✅ hippocampus_310.nii.gz: img max=1.0000, lbl sum=5723
✅ hippocampus_280.nii.gz: img max=1.0000, lbl sum=3859
✅ hippocampus_329.nii.gz: img max=1.0000, lbl sum=5413
✅ hippocampus_007.nii.gz: img max=1.0000, lbl sum=4902
✅ hippocampus_096.nii.gz: img max=1.0000, lbl sum=4790
✅ hippocampus_109.nii.gz: img max=1.0000, lbl sum=4849
✅ hippocampus_222.nii.gz: img max=1.0000, lbl sum=3677
✅ hippocampus_003.nii.gz: img max=1.0000, lbl sum=5156
✅ hippocampus_026.nii.gz: img max=1.0000, lbl sum=5393
✅ hippocampus_219.nii.gz: img max=1.0000, lbl sum=4122
✅ hippocampus_337.nii.gz: img max=1.0000, lbl sum=4955
✅ hippocampus_067.nii.gz: img max=1.0000, lbl sum=4101
✅ hippocampus_143.nii.gz: img max=1.0000, lbl sum=3601
✅ hippocampus_057.nii.gz: img max=1.0000, lbl sum=4156
✅ hippocampus_380.nii.gz: img max=1.0000, lbl sum=5140
✅ hippocampus_238.nii.gz: img max=1.0000, lbl sum=5672
✅ hippocampus_381.nii.gz: img max=1.0000, lbl sum=4754
✅ hippocampus_353.nii.gz: img max=1.0000, lbl sum=4095
✅ hippocampus_130.nii.gz: img max=1.0000, lbl sum=4865
✅ hippocampus_317.nii.gz: img max=1.0000, lbl sum=4914
✅ hippocampus_056.nii.gz: img max=1.0000, lbl sum=5501
✅ hippocampus_250.nii.gz: img max=1.0000, lbl sum=4963
✅ hippocampus_354.nii.gz: img max=1.0000, lbl sum=4295
✅ hippocampus_223.nii.gz: img max=1.0000, lbl sum=5427
✅ hippocampus_394.nii.gz: img max=1.0000, lbl sum=5641
✅ hippocampus_006.nii.gz: img max=1.0000, lbl sum=6212
Number of files in train, validation and test set are :   208 26 26
SPLIT STRUCTURE:
train: type=<class 'numpy.ndarray'>, len=208
val: type=<class 'list'>, len=26
test: type=<class 'list'>, len=26
Experiment started.
Training epoch 0...
/opt/conda/conda-bld/pytorch_1587428094786/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.

Epoch: 0 Train loss: 1.1247119903564453, 0.1% complete
..........
Epoch: 0 Train loss: 1.1147054433822632, 1.2% complete
..........
Epoch: 0 Train loss: 1.1053905487060547, 2.3% complete
..........
Epoch: 0 Train loss: 1.093398928642273, 3.4% complete
..........
Epoch: 0 Train loss: 1.0744258165359497, 4.4% complete
..........
Epoch: 0 Train loss: 1.040177822113037, 5.5% complete
..........
Epoch: 0 Train loss: 0.9513058066368103, 6.6% complete
..........
Epoch: 0 Train loss: 0.6955277919769287, 7.7% complete
..........
Epoch: 0 Train loss: 0.28967347741127014, 8.8% complete
..........
Epoch: 0 Train loss: 0.02272023633122444, 9.9% complete
..........
Epoch: 0 Train loss: 0.03928971663117409, 11.0% complete
..........
Epoch: 0 Train loss: 0.03995702043175697, 12.0% complete
..........
Epoch: 0 Train loss: 0.010518241673707962, 13.1% complete
..........
Epoch: 0 Train loss: 0.009552601724863052, 14.2% complete
..........
Epoch: 0 Train loss: 0.028643544763326645, 15.3% complete
..........
Epoch: 0 Train loss: 0.007774976082146168, 16.4% complete
..........
Epoch: 0 Train loss: 0.005774462595582008, 17.5% complete
..........
Epoch: 0 Train loss: 0.004727585706859827, 18.5% complete
..........
Epoch: 0 Train loss: 0.03492221236228943, 19.6% complete
..........
Epoch: 0 Train loss: 0.012036312371492386, 20.7% complete
..........
Epoch: 0 Train loss: 0.0042465259321033955, 21.8% complete
..........
Epoch: 0 Train loss: 0.012963430024683475, 22.9% complete
..........
Epoch: 0 Train loss: 0.024668646976351738, 24.0% complete
..........
Epoch: 0 Train loss: 0.02094665728509426, 25.1% complete
..........
Epoch: 0 Train loss: 0.005276360549032688, 26.1% complete
..........
Epoch: 0 Train loss: 0.019116537645459175, 27.2% complete
..........
Epoch: 0 Train loss: 0.0035413361620157957, 28.3% complete
..........
Epoch: 0 Train loss: 0.003000472905114293, 29.4% complete
..........
Epoch: 0 Train loss: 0.002299755811691284, 30.5% complete
..........
Epoch: 0 Train loss: 0.02546602115035057, 31.6% complete
..........
Epoch: 0 Train loss: 0.028684653341770172, 32.6% complete
..........
Epoch: 0 Train loss: 0.015218500979244709, 33.7% complete
..........
Epoch: 0 Train loss: 0.016012992709875107, 34.8% complete
..........
Epoch: 0 Train loss: 0.01716420240700245, 35.9% complete
..........
Epoch: 0 Train loss: 0.0036605491768568754, 37.0% complete
..........
Epoch: 0 Train loss: 0.018839724361896515, 38.1% complete
..........
Epoch: 0 Train loss: 0.021928837522864342, 39.2% complete
..........
Epoch: 0 Train loss: 0.02174595557153225, 40.2% complete
..........
Epoch: 0 Train loss: 0.006655029021203518, 41.3% complete
..........
Epoch: 0 Train loss: 0.01912611909210682, 42.4% complete
..........
Epoch: 0 Train loss: 0.002963884500786662, 43.5% complete
..........
Epoch: 0 Train loss: 0.01707226037979126, 44.6% complete
..........
Epoch: 0 Train loss: 0.020559079945087433, 45.7% complete
..........
Epoch: 0 Train loss: 0.02654823660850525, 46.7% complete
..........
Epoch: 0 Train loss: 0.0025450335815548897, 47.8% complete
..........
Epoch: 0 Train loss: 0.015741243958473206, 48.9% complete
..........
Epoch: 0 Train loss: 0.0009042737074196339, 50.0% complete
..........
Epoch: 0 Train loss: 0.011001093313097954, 51.1% complete
..........
Epoch: 0 Train loss: 0.011330130510032177, 52.2% complete
..........
Epoch: 0 Train loss: 0.00065740937134251, 53.3% complete
..........
Epoch: 0 Train loss: 0.002444145968183875, 54.3% complete
..........
Epoch: 0 Train loss: 0.010882980190217495, 55.4% complete
..........
Epoch: 0 Train loss: 0.005721613764762878, 56.5% complete
..........
Epoch: 0 Train loss: 0.03589257970452309, 57.6% complete
..........
Epoch: 0 Train loss: 0.024563582614064217, 58.7% complete
..........
Epoch: 0 Train loss: 0.0014058052329346538, 59.8% complete
..........
Epoch: 0 Train loss: 0.0022098373156040907, 60.8% complete
..........
Epoch: 0 Train loss: 0.012178766541182995, 61.9% complete
..........
Epoch: 0 Train loss: 0.023069966584444046, 63.0% complete
..........
Epoch: 0 Train loss: 0.013481405563652515, 64.1% complete
..........
Epoch: 0 Train loss: 0.0009577659657225013, 65.2% complete
..........
Epoch: 0 Train loss: 0.0018191953422501683, 66.3% complete
..........
Epoch: 0 Train loss: 0.00011194792750757188, 67.4% complete
..........
Epoch: 0 Train loss: 0.018066398799419403, 68.4% complete
..........
Epoch: 0 Train loss: 0.0023418960627168417, 69.5% complete
..........
Epoch: 0 Train loss: 0.007933053188025951, 70.6% complete
..........
Epoch: 0 Train loss: 0.013405116274952888, 71.7% complete
..........
Epoch: 0 Train loss: 0.006299702916294336, 72.8% complete
..........
Epoch: 0 Train loss: 0.012841612100601196, 73.9% complete
..........
Epoch: 0 Train loss: 0.016113117337226868, 74.9% complete
..........
Epoch: 0 Train loss: 0.0009650547872297466, 76.0% complete
..........
Epoch: 0 Train loss: 0.0003333127242513001, 77.1% complete
..........
Epoch: 0 Train loss: 0.0092531843110919, 78.2% complete
..........
Epoch: 0 Train loss: 0.0037163293454796076, 79.3% complete
..........
Epoch: 0 Train loss: 0.005696856416761875, 80.4% complete
..........
Epoch: 0 Train loss: 0.0009741288959048688, 81.5% complete
..........
Epoch: 0 Train loss: 0.015247705392539501, 82.5% complete
..........
Epoch: 0 Train loss: 0.021624034270644188, 83.6% complete
..........
Epoch: 0 Train loss: 0.012072671204805374, 84.7% complete
..........
Epoch: 0 Train loss: 0.016711970791220665, 85.8% complete
..........
Epoch: 0 Train loss: 0.019182797521352768, 86.9% complete
..........
Epoch: 0 Train loss: 0.014958448708057404, 88.0% complete
..........
Epoch: 0 Train loss: 0.01804172433912754, 89.0% complete
..........
Epoch: 0 Train loss: 0.0001357598084723577, 90.1% complete
..........
Epoch: 0 Train loss: 0.014448828995227814, 91.2% complete
..........
Epoch: 0 Train loss: 0.00038311982643790543, 92.3% complete
..........
Epoch: 0 Train loss: 0.00020662715542130172, 93.4% complete
..........
Epoch: 0 Train loss: 0.002656115684658289, 94.5% complete
..........
Epoch: 0 Train loss: 0.000878460647072643, 95.6% complete
..........
Epoch: 0 Train loss: 0.008245503529906273, 96.6% complete
..........
Epoch: 0 Train loss: 9.335683716926724e-05, 97.7% complete
..........
Epoch: 0 Train loss: 0.021157611161470413, 98.8% complete
..........
Epoch: 0 Train loss: 0.013217201456427574, 99.9% complete
..
Training complete
Validating epoch 0...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003225940279662609
Batch 0 Loss 0.003225940279662609
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004604194255080074
Batch 1 Loss 0.0004604194255080074
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00018613194697536528
Batch 2 Loss 0.00018613194697536528
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00682710250839591
Batch 3 Loss 0.00682710250839591
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008527116850018501
Batch 4 Loss 0.008527116850018501
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013357131741940975
Batch 5 Loss 0.013357131741940975
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00010188285523327067
Batch 6 Loss 0.00010188285523327067
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013535285368561745
Batch 7 Loss 0.013535285368561745
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016146308043971658
Batch 8 Loss 0.0016146308043971658
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002395232440903783
Batch 9 Loss 0.002395232440903783
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016366533236578107
Batch 10 Loss 0.0016366533236578107
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034733435604721308
Batch 11 Loss 0.0034733435604721308
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007524421089328825
Batch 12 Loss 0.0007524421089328825
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017774560255929828
Batch 13 Loss 0.0017774560255929828
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00035809155087918043
Batch 14 Loss 0.00035809155087918043
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013847230933606625
Batch 15 Loss 0.013847230933606625
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035095803905278444
Batch 16 Loss 0.0035095803905278444
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.016387294977903366
Batch 17 Loss 0.016387294977903366
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008890358731150627
Batch 18 Loss 0.008890358731150627
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010934928432106972
Batch 19 Loss 0.010934928432106972
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00737552298232913
Batch 20 Loss 0.00737552298232913
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009635663009248674
Batch 21 Loss 0.0009635663009248674
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 9.379256516695023e-05
Batch 22 Loss 9.379256516695023e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015364602208137512
Batch 23 Loss 0.015364602208137512
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012967373244464397
Batch 24 Loss 0.012967373244464397
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003288211883045733
Batch 25 Loss 0.0003288211883045733
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011167126707732677
Batch 26 Loss 0.011167126707732677
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010296209948137403
Batch 27 Loss 0.0010296209948137403
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0142715685069561
Batch 28 Loss 0.0142715685069561
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062940833158791065
Batch 29 Loss 0.0062940833158791065
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0023080341052263975
Batch 30 Loss 0.0023080341052263975
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0022948775440454483
Batch 31 Loss 0.0022948775440454483
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005471235606819391
Batch 32 Loss 0.005471235606819391
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015299774706363678
Batch 33 Loss 0.015299774706363678
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006102646701037884
Batch 34 Loss 0.006102646701037884
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01157030463218689
Batch 35 Loss 0.01157030463218689
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007159017026424408
Batch 36 Loss 0.007159017026424408
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010605270974338055
Batch 37 Loss 0.010605270974338055
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00020534217765089124
Batch 38 Loss 0.00020534217765089124
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030021227430552244
Batch 39 Loss 0.0030021227430552244
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0147995725274086
Batch 40 Loss 0.0147995725274086
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001284189522266388
Batch 41 Loss 0.001284189522266388
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.019192667677998543
Batch 42 Loss 0.019192667677998543
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011213783000130206
Batch 43 Loss 0.00011213783000130206
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004958862904459238
Batch 44 Loss 0.004958862904459238
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01334681361913681
Batch 45 Loss 0.01334681361913681
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034224377013742924
Batch 46 Loss 0.0034224377013742924
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011655866401270032
Batch 47 Loss 0.0011655866401270032
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014181667938828468
Batch 48 Loss 0.014181667938828468
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014344793744385242
Batch 49 Loss 0.014344793744385242
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003199582453817129
Batch 50 Loss 0.003199582453817129
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012590696103870869
Batch 51 Loss 0.012590696103870869
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017612128285691142
Batch 52 Loss 0.0017612128285691142
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005790324416011572
Batch 53 Loss 0.005790324416011572
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013652929104864597
Batch 54 Loss 0.013652929104864597
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011082098353654146
Batch 55 Loss 0.00011082098353654146
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004714958486147225
Batch 56 Loss 0.0004714958486147225
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007136939675547183
Batch 57 Loss 0.0007136939675547183
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01348122674971819
Batch 58 Loss 0.01348122674971819
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01472569815814495
Batch 59 Loss 0.01472569815814495
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009158042958006263
Batch 60 Loss 0.0009158042958006263
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013872857205569744
Batch 61 Loss 0.013872857205569744
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014617842680308968
Batch 62 Loss 0.00014617842680308968
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006508518126793206
Batch 63 Loss 0.0006508518126793206
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007704460294917226
Batch 64 Loss 0.0007704460294917226
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011399305425584316
Batch 65 Loss 0.011399305425584316
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010053940117359161
Batch 66 Loss 0.010053940117359161
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004034731537103653
Batch 67 Loss 0.004034731537103653
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014681720174849033
Batch 68 Loss 0.014681720174849033
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005719716427847743
Batch 69 Loss 0.0005719716427847743
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008974126540124416
Batch 70 Loss 0.008974126540124416
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013014638796448708
Batch 71 Loss 0.013014638796448708
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007785778143443167
Batch 72 Loss 0.0007785778143443167
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01129557378590107
Batch 73 Loss 0.01129557378590107
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013529835268855095
Batch 74 Loss 0.013529835268855095
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0052270726300776005
Batch 75 Loss 0.0052270726300776005
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0043237772770226
Batch 76 Loss 0.0043237772770226
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011217081919312477
Batch 77 Loss 0.011217081919312477
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01316542737185955
Batch 78 Loss 0.01316542737185955
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 8.923120913095772e-05
Batch 79 Loss 8.923120913095772e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003354886139277369
Batch 80 Loss 0.0003354886139277369
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014389625750482082
Batch 81 Loss 0.014389625750482082
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0152709586545825
Batch 82 Loss 0.0152709586545825
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01620827428996563
Batch 83 Loss 0.01620827428996563
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01690407656133175
Batch 84 Loss 0.01690407656133175
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00387944420799613
Batch 85 Loss 0.00387944420799613
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001281808508792892
Batch 86 Loss 0.0001281808508792892
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016413659323006868
Batch 87 Loss 0.0016413659323006868
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008599694119766355
Batch 88 Loss 0.0008599694119766355
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.018294889479875565
Batch 89 Loss 0.018294889479875565
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006725330371409655
Batch 90 Loss 0.006725330371409655
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012926450930535793
Batch 91 Loss 0.012926450930535793
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005903329234570265
Batch 92 Loss 0.005903329234570265
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01525120809674263
Batch 93 Loss 0.01525120809674263
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012554261833429337
Batch 94 Loss 0.012554261833429337
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014107473194599152
Batch 95 Loss 0.014107473194599152
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015382540412247181
Batch 96 Loss 0.015382540412247181
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014313274063169956
Batch 97 Loss 0.014313274063169956
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021601621992886066
Batch 98 Loss 0.0021601621992886066
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028734521474689245
Batch 99 Loss 0.0028734521474689245
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028774491511285305
Batch 100 Loss 0.0028774491511285305
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007373063825070858
Batch 101 Loss 0.007373063825070858
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001257186522707343
Batch 102 Loss 0.0001257186522707343
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 9.153492283076048e-05
Batch 103 Loss 9.153492283076048e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006532433442771435
Batch 104 Loss 0.006532433442771435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006932460702955723
Batch 105 Loss 0.006932460702955723
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00790858268737793
Batch 106 Loss 0.00790858268737793
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002298122417414561
Batch 107 Loss 0.0002298122417414561
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004299890249967575
Batch 108 Loss 0.004299890249967575
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012834104709327221
Batch 109 Loss 0.012834104709327221
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006167142651975155
Batch 110 Loss 0.006167142651975155
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 8.632607932668179e-05
Batch 111 Loss 8.632607932668179e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002573262434452772
Batch 112 Loss 0.002573262434452772
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006878082640469074
Batch 113 Loss 0.0006878082640469074
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 1...

Epoch: 1 Train loss: 0.016017889603972435, 0.1% complete
..........
Epoch: 1 Train loss: 0.010978433303534985, 1.2% complete
..........
Epoch: 1 Train loss: 0.00033036834793165326, 2.3% complete
..........
Epoch: 1 Train loss: 0.010972581803798676, 3.4% complete
..........
Epoch: 1 Train loss: 0.0067398459650576115, 4.4% complete
..........
Epoch: 1 Train loss: 0.018903151154518127, 5.5% complete
..........
Epoch: 1 Train loss: 0.006075527984648943, 6.6% complete
..........
Epoch: 1 Train loss: 0.007513261865824461, 7.7% complete
..........
Epoch: 1 Train loss: 0.005656191613525152, 8.8% complete
..........
Epoch: 1 Train loss: 0.008795526809990406, 9.9% complete
..........
Epoch: 1 Train loss: 0.010280808433890343, 11.0% complete
..........
Epoch: 1 Train loss: 0.016005832701921463, 12.0% complete
..........
Epoch: 1 Train loss: 0.01358393020927906, 13.1% complete
..........
Epoch: 1 Train loss: 0.0011769715929403901, 14.2% complete
..........
Epoch: 1 Train loss: 0.01502146478742361, 15.3% complete
..........
Epoch: 1 Train loss: 0.013841663487255573, 16.4% complete
..........
Epoch: 1 Train loss: 0.010275211185216904, 17.5% complete
..........
Epoch: 1 Train loss: 0.00033225753577426076, 18.5% complete
..........
Epoch: 1 Train loss: 0.012009472586214542, 19.6% complete
..........
Epoch: 1 Train loss: 0.005819105077534914, 20.7% complete
..........
Epoch: 1 Train loss: 0.009139140136539936, 21.8% complete
..........
Epoch: 1 Train loss: 0.005827030166983604, 22.9% complete
..........
Epoch: 1 Train loss: 0.01242753304541111, 24.0% complete
..........
Epoch: 1 Train loss: 8.43256275402382e-05, 25.1% complete
..........
Epoch: 1 Train loss: 0.004796947818249464, 26.1% complete
..........
Epoch: 1 Train loss: 0.0004949364811182022, 27.2% complete
..........
Epoch: 1 Train loss: 0.0005374484462663531, 28.3% complete
..........
Epoch: 1 Train loss: 0.01367025263607502, 29.4% complete
..........
Epoch: 1 Train loss: 9.382196731166914e-05, 30.5% complete
..........
Epoch: 1 Train loss: 0.013431801460683346, 31.6% complete
..........
Epoch: 1 Train loss: 0.0028659955132752657, 32.6% complete
..........
Epoch: 1 Train loss: 5.8550947869662195e-05, 33.7% complete
..........
Epoch: 1 Train loss: 0.010299021378159523, 34.8% complete
..........
Epoch: 1 Train loss: 0.007970652543008327, 35.9% complete
..........
Epoch: 1 Train loss: 0.00668724812567234, 37.0% complete
..........
Epoch: 1 Train loss: 0.011420552618801594, 38.1% complete
..........
Epoch: 1 Train loss: 0.011699176393449306, 39.2% complete
..........
Epoch: 1 Train loss: 0.000862152490299195, 40.2% complete
..........
Epoch: 1 Train loss: 0.004204072058200836, 41.3% complete
..........
Epoch: 1 Train loss: 0.00026149844052270055, 42.4% complete
..........
Epoch: 1 Train loss: 0.00039394819759763777, 43.5% complete
..........
Epoch: 1 Train loss: 0.0005754707963205874, 44.6% complete
..........
Epoch: 1 Train loss: 0.01039509940892458, 45.7% complete
..........
Epoch: 1 Train loss: 0.012217986397445202, 46.7% complete
..........
Epoch: 1 Train loss: 0.00048210826935246587, 47.8% complete
..........
Epoch: 1 Train loss: 5.959430563962087e-05, 48.9% complete
..........
Epoch: 1 Train loss: 0.00030628519016318023, 50.0% complete
..........
Epoch: 1 Train loss: 0.013339545577764511, 51.1% complete
..........
Epoch: 1 Train loss: 5.7071709306910634e-05, 52.2% complete
..........
Epoch: 1 Train loss: 6.766481965314597e-05, 53.3% complete
..........
Epoch: 1 Train loss: 2.6170837372774258e-05, 54.3% complete
..........
Epoch: 1 Train loss: 0.013416063040494919, 55.4% complete
..........
Epoch: 1 Train loss: 0.004245398566126823, 56.5% complete
..........
Epoch: 1 Train loss: 0.00045580510050058365, 57.6% complete
..........
Epoch: 1 Train loss: 0.007105496246367693, 58.7% complete
..........
Epoch: 1 Train loss: 0.002261389046907425, 59.8% complete
..........
Epoch: 1 Train loss: 0.013072269968688488, 60.8% complete
..........
Epoch: 1 Train loss: 0.004561540670692921, 61.9% complete
..........
Epoch: 1 Train loss: 0.010683423839509487, 63.0% complete
..........
Epoch: 1 Train loss: 0.013174783438444138, 64.1% complete
..........
Epoch: 1 Train loss: 0.013371430337429047, 65.2% complete
..........
Epoch: 1 Train loss: 0.00011570324568310753, 66.3% complete
..........
Epoch: 1 Train loss: 0.00906427949666977, 67.4% complete
..........
Epoch: 1 Train loss: 0.011802207678556442, 68.4% complete
..........
Epoch: 1 Train loss: 7.543693936895579e-05, 69.5% complete
..........
Epoch: 1 Train loss: 0.01348819863051176, 70.6% complete
..........
Epoch: 1 Train loss: 0.0028625347185879946, 71.7% complete
..........
Epoch: 1 Train loss: 0.0011589592322707176, 72.8% complete
..........
Epoch: 1 Train loss: 0.011349868960678577, 73.9% complete
..........
Epoch: 1 Train loss: 0.01061293575912714, 74.9% complete
..........
Epoch: 1 Train loss: 0.012550047598779202, 76.0% complete
..........
Epoch: 1 Train loss: 0.006343151908367872, 77.1% complete
..........
Epoch: 1 Train loss: 0.013330964371562004, 78.2% complete
..........
Epoch: 1 Train loss: 0.0007143060211092234, 79.3% complete
..........
Epoch: 1 Train loss: 0.01340027991682291, 80.4% complete
..........
Epoch: 1 Train loss: 0.016190484166145325, 81.5% complete
..........
Epoch: 1 Train loss: 0.005408000200986862, 82.5% complete
..........
Epoch: 1 Train loss: 3.372806168044917e-05, 83.6% complete
..........
Epoch: 1 Train loss: 0.010113672353327274, 84.7% complete
..........
Epoch: 1 Train loss: 0.0158050786703825, 85.8% complete
..........
Epoch: 1 Train loss: 0.0006179924821481109, 86.9% complete
..........
Epoch: 1 Train loss: 0.010534434579312801, 88.0% complete
..........
Epoch: 1 Train loss: 0.014264619909226894, 89.0% complete
..........
Epoch: 1 Train loss: 0.006429169327020645, 90.1% complete
..........
Epoch: 1 Train loss: 0.006589181255549192, 91.2% complete
..........
Epoch: 1 Train loss: 0.013219893909990788, 92.3% complete
..........
Epoch: 1 Train loss: 0.01089763455092907, 93.4% complete
..........
Epoch: 1 Train loss: 0.012996146455407143, 94.5% complete
..........
Epoch: 1 Train loss: 0.011422096751630306, 95.6% complete
..........
Epoch: 1 Train loss: 0.005457681138068438, 96.6% complete
..........
Epoch: 1 Train loss: 0.00015004095621407032, 97.7% complete
..........
Epoch: 1 Train loss: 0.005482407286763191, 98.8% complete
..........
Epoch: 1 Train loss: 0.013778606429696083, 99.9% complete
..
Training complete
Validating epoch 1...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008451828733086586
Batch 0 Loss 0.008451828733086586
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00926254317164421
Batch 1 Loss 0.00926254317164421
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011241953819990158
Batch 2 Loss 0.011241953819990158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008728019893169403
Batch 3 Loss 0.008728019893169403
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014594054780900478
Batch 4 Loss 0.014594054780900478
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003427090821787715
Batch 5 Loss 0.003427090821787715
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012975292280316353
Batch 6 Loss 0.012975292280316353
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007993367500603199
Batch 7 Loss 0.007993367500603199
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6061075914185494e-05
Batch 8 Loss 2.6061075914185494e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002663371153175831
Batch 9 Loss 0.002663371153175831
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0125637361779809
Batch 10 Loss 0.0125637361779809
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007741253823041916
Batch 11 Loss 0.007741253823041916
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010050198761746287
Batch 12 Loss 0.0010050198761746287
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008677249774336815
Batch 13 Loss 0.008677249774336815
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029677217826247215
Batch 14 Loss 0.0029677217826247215
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008666738867759705
Batch 15 Loss 0.008666738867759705
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013169631361961365
Batch 16 Loss 0.013169631361961365
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033273082226514816
Batch 17 Loss 0.0033273082226514816
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01089787669479847
Batch 18 Loss 0.01089787669479847
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011457170359790325
Batch 19 Loss 0.011457170359790325
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007987732533365488
Batch 20 Loss 0.0007987732533365488
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006075139623135328
Batch 21 Loss 0.006075139623135328
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0020126011222600937
Batch 22 Loss 0.0020126011222600937
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009748456999659538
Batch 23 Loss 0.009748456999659538
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 2.648982626851648e-05
Batch 24 Loss 2.648982626851648e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003703743568621576
Batch 25 Loss 0.0003703743568621576
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00892266072332859
Batch 26 Loss 0.00892266072332859
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003788762027397752
Batch 27 Loss 0.003788762027397752
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011598731391131878
Batch 28 Loss 0.011598731391131878
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 3.267428473918699e-05
Batch 29 Loss 3.267428473918699e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00449807895347476
Batch 30 Loss 0.00449807895347476
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003944199997931719
Batch 31 Loss 0.003944199997931719
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 4.946357512380928e-05
Batch 32 Loss 4.946357512380928e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011802533641457558
Batch 33 Loss 0.011802533641457558
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010813896544277668
Batch 34 Loss 0.010813896544277668
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013474393635988235
Batch 35 Loss 0.013474393635988235
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005622328608296812
Batch 36 Loss 0.0005622328608296812
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00918099656701088
Batch 37 Loss 0.00918099656701088
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01086190901696682
Batch 38 Loss 0.01086190901696682
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009889733046293259
Batch 39 Loss 0.009889733046293259
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 2.072304778266698e-05
Batch 40 Loss 2.072304778266698e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003437589621171355
Batch 41 Loss 0.0003437589621171355
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014044360257685184
Batch 42 Loss 0.014044360257685184
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001235866453498602
Batch 43 Loss 0.001235866453498602
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002076138975098729
Batch 44 Loss 0.002076138975098729
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011055562645196915
Batch 45 Loss 0.011055562645196915
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012360472232103348
Batch 46 Loss 0.012360472232103348
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00952514261007309
Batch 47 Loss 0.00952514261007309
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008385748602449894
Batch 48 Loss 0.008385748602449894
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 2.069081165245734e-05
Batch 49 Loss 2.069081165245734e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002505226992070675
Batch 50 Loss 0.002505226992070675
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018684925744310021
Batch 51 Loss 0.0018684925744310021
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009167538024485111
Batch 52 Loss 0.009167538024485111
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010164445266127586
Batch 53 Loss 0.010164445266127586
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01277322880923748
Batch 54 Loss 0.01277322880923748
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009720386937260628
Batch 55 Loss 0.009720386937260628
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009147215634584427
Batch 56 Loss 0.009147215634584427
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010650667361915112
Batch 57 Loss 0.010650667361915112
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007327861967496574
Batch 58 Loss 0.0007327861967496574
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012497507967054844
Batch 59 Loss 0.012497507967054844
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 2.59235966950655e-05
Batch 60 Loss 2.59235966950655e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 4.6697648940607905e-05
Batch 61 Loss 4.6697648940607905e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01016194000840187
Batch 62 Loss 0.01016194000840187
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012535483110696077
Batch 63 Loss 0.0012535483110696077
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008658023434691131
Batch 64 Loss 0.0008658023434691131
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008919898420572281
Batch 65 Loss 0.008919898420572281
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 2.0339615730335936e-05
Batch 66 Loss 2.0339615730335936e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013751274906098843
Batch 67 Loss 0.013751274906098843
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006545502110384405
Batch 68 Loss 0.0006545502110384405
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00026325962971895933
Batch 69 Loss 0.00026325962971895933
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010777607560157776
Batch 70 Loss 0.010777607560157776
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011268489994108677
Batch 71 Loss 0.011268489994108677
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 2.885501817218028e-05
Batch 72 Loss 2.885501817218028e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00790918804705143
Batch 73 Loss 0.00790918804705143
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017687228682916611
Batch 74 Loss 0.00017687228682916611
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00798620842397213
Batch 75 Loss 0.00798620842397213
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003547264961525798
Batch 76 Loss 0.003547264961525798
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012732078321278095
Batch 77 Loss 0.012732078321278095
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009020375087857246
Batch 78 Loss 0.009020375087857246
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0023425922263413668
Batch 79 Loss 0.0023425922263413668
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002219472313299775
Batch 80 Loss 0.002219472313299775
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010426855646073818
Batch 81 Loss 0.010426855646073818
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013372983783483505
Batch 82 Loss 0.013372983783483505
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005444406531751156
Batch 83 Loss 0.005444406531751156
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011291098780930042
Batch 84 Loss 0.011291098780930042
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009380881674587727
Batch 85 Loss 0.009380881674587727
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007665181532502174
Batch 86 Loss 0.007665181532502174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014935572631657124
Batch 87 Loss 0.014935572631657124
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009776364080607891
Batch 88 Loss 0.009776364080607891
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006729405838996172
Batch 89 Loss 0.006729405838996172
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 4.089330104761757e-05
Batch 90 Loss 4.089330104761757e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006113463896326721
Batch 91 Loss 0.0006113463896326721
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007419521571137011
Batch 92 Loss 0.0007419521571137011
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015355789102613926
Batch 93 Loss 0.015355789102613926
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00029344833455979824
Batch 94 Loss 0.00029344833455979824
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008497270755469799
Batch 95 Loss 0.008497270755469799
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009870576672255993
Batch 96 Loss 0.009870576672255993
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00970191229134798
Batch 97 Loss 0.00970191229134798
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009870413690805435
Batch 98 Loss 0.009870413690805435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0020197012927383184
Batch 99 Loss 0.0020197012927383184
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001308837323449552
Batch 100 Loss 0.001308837323449552
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011545604094862938
Batch 101 Loss 0.011545604094862938
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011884170584380627
Batch 102 Loss 0.011884170584380627
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002332600561203435
Batch 103 Loss 0.0002332600561203435
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0106952590867877
Batch 104 Loss 0.0106952590867877
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00988931767642498
Batch 105 Loss 0.00988931767642498
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006923312903381884
Batch 106 Loss 0.0006923312903381884
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014686967246234417
Batch 107 Loss 0.014686967246234417
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0099638095125556
Batch 108 Loss 0.0099638095125556
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00039112201193347573
Batch 109 Loss 0.00039112201193347573
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012765001505613327
Batch 110 Loss 0.012765001505613327
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010943382047116756
Batch 111 Loss 0.010943382047116756
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008524258621037006
Batch 112 Loss 0.008524258621037006
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8514816474635154e-05
Batch 113 Loss 1.8514816474635154e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 2...

Epoch: 2 Train loss: 0.01006740890443325, 0.1% complete
..........
Epoch: 2 Train loss: 0.008481468074023724, 1.2% complete
..........
Epoch: 2 Train loss: 0.0077416859567165375, 2.3% complete
..........
Epoch: 2 Train loss: 0.014943367801606655, 3.4% complete
..........
Epoch: 2 Train loss: 0.0032163311261683702, 4.4% complete
..........
Epoch: 2 Train loss: 0.00014156464021652937, 5.5% complete
..........
Epoch: 2 Train loss: 0.012744386680424213, 6.6% complete
..........
Epoch: 2 Train loss: 0.007010674104094505, 7.7% complete
..........
Epoch: 2 Train loss: 0.00711054215207696, 8.8% complete
..........
Epoch: 2 Train loss: 3.127413219772279e-05, 9.9% complete
..........
Epoch: 2 Train loss: 2.5166591512970626e-05, 11.0% complete
..........
Epoch: 2 Train loss: 0.010824798606336117, 12.0% complete
..........
Epoch: 2 Train loss: 0.00012582034105435014, 13.1% complete
..........
Epoch: 2 Train loss: 0.006040956359356642, 14.2% complete
..........
Epoch: 2 Train loss: 6.337489321595058e-05, 15.3% complete
..........
Epoch: 2 Train loss: 0.0013907436514273286, 16.4% complete
..........
Epoch: 2 Train loss: 0.004725034814327955, 17.5% complete
..........
Epoch: 2 Train loss: 0.01158409658819437, 18.5% complete
..........
Epoch: 2 Train loss: 0.011500459164381027, 19.6% complete
..........
Epoch: 2 Train loss: 2.4479777493979782e-05, 20.7% complete
..........
Epoch: 2 Train loss: 0.0008115200907923281, 21.8% complete
..........
Epoch: 2 Train loss: 0.008342630229890347, 22.9% complete
..........
Epoch: 2 Train loss: 0.00034676550421863794, 24.0% complete
..........
Epoch: 2 Train loss: 0.007329393643885851, 25.1% complete
..........
Epoch: 2 Train loss: 0.00016851162945386022, 26.1% complete
..........
Epoch: 2 Train loss: 0.00047976558562368155, 27.2% complete
..........
Epoch: 2 Train loss: 0.002717579947784543, 28.3% complete
..........
Epoch: 2 Train loss: 6.443662277888507e-05, 29.4% complete
..........
Epoch: 2 Train loss: 0.01775851845741272, 30.5% complete
..........
Epoch: 2 Train loss: 0.00036966826883144677, 31.6% complete
..........
Epoch: 2 Train loss: 0.0047525325790047646, 32.6% complete
..........
Epoch: 2 Train loss: 4.960643855156377e-05, 33.7% complete
..........
Epoch: 2 Train loss: 0.0009613882866688073, 34.8% complete
..........
Epoch: 2 Train loss: 0.008152171038091183, 35.9% complete
..........
Epoch: 2 Train loss: 0.0025990880094468594, 37.0% complete
..........
Epoch: 2 Train loss: 0.004123340360820293, 38.1% complete
..........
Epoch: 2 Train loss: 0.00012864850577898324, 39.2% complete
..........
Epoch: 2 Train loss: 1.981211971724406e-05, 40.2% complete
..........
Epoch: 2 Train loss: 0.0007060084026306868, 41.3% complete
..........
Epoch: 2 Train loss: 0.011791442520916462, 42.4% complete
..........
Epoch: 2 Train loss: 1.748502836562693e-05, 43.5% complete
..........
Epoch: 2 Train loss: 0.0006518370355479419, 44.6% complete
..........
Epoch: 2 Train loss: 0.0211501382291317, 45.7% complete
..........
Epoch: 2 Train loss: 0.011048992164433002, 46.7% complete
..........
Epoch: 2 Train loss: 0.006302314344793558, 47.8% complete
..........
Epoch: 2 Train loss: 0.011644426733255386, 48.9% complete
..........
Epoch: 2 Train loss: 0.01250561885535717, 50.0% complete
..........
Epoch: 2 Train loss: 0.002775175031274557, 51.1% complete
..........
Epoch: 2 Train loss: 0.0020929777529090643, 52.2% complete
..........
Epoch: 2 Train loss: 0.01039388868957758, 53.3% complete
..........
Epoch: 2 Train loss: 0.0001833142596296966, 54.3% complete
..........
Epoch: 2 Train loss: 7.436706073349342e-05, 55.4% complete
..........
Epoch: 2 Train loss: 0.011533786542713642, 56.5% complete
..........
Epoch: 2 Train loss: 0.0002947875182144344, 57.6% complete
..........
Epoch: 2 Train loss: 4.375467688078061e-05, 58.7% complete
..........
Epoch: 2 Train loss: 0.009741510264575481, 59.8% complete
..........
Epoch: 2 Train loss: 4.9774200306274e-05, 60.8% complete
..........
Epoch: 2 Train loss: 0.009649094194173813, 61.9% complete
..........
Epoch: 2 Train loss: 0.0046623037196695805, 63.0% complete
..........
Epoch: 2 Train loss: 0.01044534519314766, 64.1% complete
..........
Epoch: 2 Train loss: 0.012723112478852272, 65.2% complete
..........
Epoch: 2 Train loss: 0.0034836120903491974, 66.3% complete
..........
Epoch: 2 Train loss: 4.338275175541639e-05, 67.4% complete
..........
Epoch: 2 Train loss: 0.01292463205754757, 68.4% complete
..........
Epoch: 2 Train loss: 1.1436306522227824e-05, 69.5% complete
..........
Epoch: 2 Train loss: 1.5197729226201773e-05, 70.6% complete
..........
Epoch: 2 Train loss: 0.008943729102611542, 71.7% complete
..........
Epoch: 2 Train loss: 0.0001534409384476021, 72.8% complete
..........
Epoch: 2 Train loss: 0.013982439413666725, 73.9% complete
..........
Epoch: 2 Train loss: 0.0026343590579926968, 74.9% complete
..........
Epoch: 2 Train loss: 0.003984010312706232, 76.0% complete
..........
Epoch: 2 Train loss: 0.011827128939330578, 77.1% complete
..........
Epoch: 2 Train loss: 0.01260529924184084, 78.2% complete
..........
Epoch: 2 Train loss: 0.00028006150387227535, 79.3% complete
..........
Epoch: 2 Train loss: 0.0021252299193292856, 80.4% complete
..........
Epoch: 2 Train loss: 0.005722597241401672, 81.5% complete
..........
Epoch: 2 Train loss: 0.0012696731137111783, 82.5% complete
..........
Epoch: 2 Train loss: 0.003913617692887783, 83.6% complete
..........
Epoch: 2 Train loss: 0.004821861162781715, 84.7% complete
..........
Epoch: 2 Train loss: 0.009887082502245903, 85.8% complete
..........
Epoch: 2 Train loss: 0.008134143427014351, 86.9% complete
..........
Epoch: 2 Train loss: 0.0032362686470150948, 88.0% complete
..........
Epoch: 2 Train loss: 7.145878043957055e-06, 89.0% complete
..........
Epoch: 2 Train loss: 5.017333023715764e-06, 90.1% complete
..........
Epoch: 2 Train loss: 0.011860783211886883, 91.2% complete
..........
Epoch: 2 Train loss: 0.0059697628021240234, 92.3% complete
..........
Epoch: 2 Train loss: 3.244700201321393e-05, 93.4% complete
..........
Epoch: 2 Train loss: 0.006969769485294819, 94.5% complete
..........
Epoch: 2 Train loss: 0.006410720758140087, 95.6% complete
..........
Epoch: 2 Train loss: 0.006439455784857273, 96.6% complete
..........
Epoch: 2 Train loss: 0.008721834979951382, 97.7% complete
..........
Epoch: 2 Train loss: 1.3722466974286363e-05, 98.8% complete
..........
Epoch: 2 Train loss: 8.80412699189037e-06, 99.9% complete
..
Training complete
Validating epoch 2...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002517850080039352
Batch 0 Loss 0.0002517850080039352
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005174702033400536
Batch 1 Loss 0.005174702033400536
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3671560736838728e-05
Batch 2 Loss 1.3671560736838728e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 8.415445336140692e-06
Batch 3 Loss 8.415445336140692e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 8.383781096199527e-05
Batch 4 Loss 8.383781096199527e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01019095815718174
Batch 5 Loss 0.01019095815718174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 1.607951708137989e-05
Batch 6 Loss 1.607951708137989e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059152995236217976
Batch 7 Loss 0.0059152995236217976
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008435009978711605
Batch 8 Loss 0.008435009978711605
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014808852574788034
Batch 9 Loss 0.00014808852574788034
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 5.817048077005893e-06
Batch 10 Loss 5.817048077005893e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0109848752617836
Batch 11 Loss 0.0109848752617836
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008839544840157032
Batch 12 Loss 0.008839544840157032
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006073383614420891
Batch 13 Loss 0.006073383614420891
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00933551974594593
Batch 14 Loss 0.00933551974594593
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 7.1284084697254e-06
Batch 15 Loss 7.1284084697254e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008621971122920513
Batch 16 Loss 0.008621971122920513
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 5.499699909705669e-05
Batch 17 Loss 5.499699909705669e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01205719355493784
Batch 18 Loss 0.01205719355493784
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 7.455091690644622e-06
Batch 19 Loss 7.455091690644622e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.02048277109861374
Batch 20 Loss 0.02048277109861374
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00767772551625967
Batch 21 Loss 0.00767772551625967
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 8.417446224484593e-06
Batch 22 Loss 8.417446224484593e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00441389437764883
Batch 23 Loss 0.00441389437764883
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 2.092314389301464e-05
Batch 24 Loss 2.092314389301464e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 6.53426832286641e-06
Batch 25 Loss 6.53426832286641e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010393436998128891
Batch 26 Loss 0.010393436998128891
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00806458666920662
Batch 27 Loss 0.00806458666920662
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004628248978406191
Batch 28 Loss 0.004628248978406191
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0060314456932246685
Batch 29 Loss 0.0060314456932246685
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011603489518165588
Batch 30 Loss 0.011603489518165588
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009607614949345589
Batch 31 Loss 0.009607614949345589
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016962038353085518
Batch 32 Loss 0.0016962038353085518
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 4.356377758085728e-06
Batch 33 Loss 4.356377758085728e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 8.77447018865496e-06
Batch 34 Loss 8.77447018865496e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 8.66615300765261e-06
Batch 35 Loss 8.66615300765261e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0381481843069196e-05
Batch 36 Loss 1.0381481843069196e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010495862923562527
Batch 37 Loss 0.010495862923562527
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01055942289531231
Batch 38 Loss 0.01055942289531231
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003983417060226202
Batch 39 Loss 0.003983417060226202
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007760690525174141
Batch 40 Loss 0.007760690525174141
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0044110179878771305
Batch 41 Loss 0.0044110179878771305
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008477611467242241
Batch 42 Loss 0.008477611467242241
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006749536842107773
Batch 43 Loss 0.006749536842107773
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018247680272907019
Batch 44 Loss 0.0018247680272907019
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 6.855545507278293e-06
Batch 45 Loss 6.855545507278293e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 6.828304321970791e-06
Batch 46 Loss 6.828304321970791e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009029775857925415
Batch 47 Loss 0.009029775857925415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010529149323701859
Batch 48 Loss 0.010529149323701859
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005704366602003574
Batch 49 Loss 0.005704366602003574
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011011934111593291
Batch 50 Loss 0.00011011934111593291
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00922524556517601
Batch 51 Loss 0.00922524556517601
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1220035958103836e-05
Batch 52 Loss 1.1220035958103836e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017849262803792953
Batch 53 Loss 0.0017849262803792953
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008299726992845535
Batch 54 Loss 0.008299726992845535
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011762834154069424
Batch 55 Loss 0.011762834154069424
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 6.519599992316216e-06
Batch 56 Loss 6.519599992316216e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2900727344676852e-05
Batch 57 Loss 2.2900727344676852e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007829898968338966
Batch 58 Loss 0.007829898968338966
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010923873633146286
Batch 59 Loss 0.010923873633146286
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008064295398071408
Batch 60 Loss 0.0008064295398071408
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013801692984998226
Batch 61 Loss 0.013801692984998226
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007449508644640446
Batch 62 Loss 0.007449508644640446
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005442156456410885
Batch 63 Loss 0.005442156456410885
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005148288793861866
Batch 64 Loss 0.005148288793861866
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005527186673134565
Batch 65 Loss 0.005527186673134565
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003861164441332221
Batch 66 Loss 0.003861164441332221
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036853489000350237
Batch 67 Loss 0.0036853489000350237
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6789344954304397e-05
Batch 68 Loss 1.6789344954304397e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 8.226634236052632e-06
Batch 69 Loss 8.226634236052632e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0091328676789999
Batch 70 Loss 0.0091328676789999
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 9.384937584400177e-06
Batch 71 Loss 9.384937584400177e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005120014306157827
Batch 72 Loss 0.005120014306157827
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 4.72195097245276e-06
Batch 73 Loss 4.72195097245276e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 3.7006619095336646e-05
Batch 74 Loss 3.7006619095336646e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 5.64849324291572e-06
Batch 75 Loss 5.64849324291572e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 5.138965207152069e-06
Batch 76 Loss 5.138965207152069e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 6.41780934529379e-06
Batch 77 Loss 6.41780934529379e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015285094268620014
Batch 78 Loss 0.015285094268620014
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010828492231667042
Batch 79 Loss 0.010828492231667042
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 4.421155608724803e-06
Batch 80 Loss 4.421155608724803e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008930675685405731
Batch 81 Loss 0.008930675685405731
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009747355245053768
Batch 82 Loss 0.009747355245053768
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4246332284528762e-05
Batch 83 Loss 1.4246332284528762e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 5.9821832110174e-06
Batch 84 Loss 5.9821832110174e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00959642231464386
Batch 85 Loss 0.00959642231464386
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010004050098359585
Batch 86 Loss 0.010004050098359585
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009177573956549168
Batch 87 Loss 0.009177573956549168
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010685665532946587
Batch 88 Loss 0.010685665532946587
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 5.098489054944366e-06
Batch 89 Loss 5.098489054944366e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008621563203632832
Batch 90 Loss 0.008621563203632832
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036752752494066954
Batch 91 Loss 0.0036752752494066954
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 4.659654223360121e-06
Batch 92 Loss 4.659654223360121e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 9.238709753844887e-05
Batch 93 Loss 9.238709753844887e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 6.89276130287908e-05
Batch 94 Loss 6.89276130287908e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 6.130976544227451e-06
Batch 95 Loss 6.130976544227451e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008733577094972134
Batch 96 Loss 0.008733577094972134
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 7.675762026337907e-05
Batch 97 Loss 7.675762026337907e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008609660901129246
Batch 98 Loss 0.008609660901129246
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1964693840127438e-05
Batch 99 Loss 1.1964693840127438e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002860742388293147
Batch 100 Loss 0.002860742388293147
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015802189707756042
Batch 101 Loss 0.015802189707756042
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0015415187226608396
Batch 102 Loss 0.0015415187226608396
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00955820269882679
Batch 103 Loss 0.00955820269882679
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003555020783096552
Batch 104 Loss 0.003555020783096552
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005526221357285976
Batch 105 Loss 0.005526221357285976
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001775404205545783
Batch 106 Loss 0.001775404205545783
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4816425391472876e-05
Batch 107 Loss 1.4816425391472876e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 5.656352004734799e-05
Batch 108 Loss 5.656352004734799e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055445414036512375
Batch 109 Loss 0.0055445414036512375
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0052057248540222645
Batch 110 Loss 0.0052057248540222645
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004144201520830393
Batch 111 Loss 0.004144201520830393
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 8.19262204458937e-05
Batch 112 Loss 8.19262204458937e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 7.062146323733032e-06
Batch 113 Loss 7.062146323733032e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 3...

Epoch: 3 Train loss: 0.0010085694957524538, 0.1% complete
..........
Epoch: 3 Train loss: 0.0077058179304003716, 1.2% complete
..........
Epoch: 3 Train loss: 5.979141860734671e-06, 2.3% complete
..........
Epoch: 3 Train loss: 0.0041800797916948795, 3.4% complete
..........
Epoch: 3 Train loss: 0.000680477125570178, 4.4% complete
..........
Epoch: 3 Train loss: 0.0013435001019388437, 5.5% complete
..........
Epoch: 3 Train loss: 0.0005037143127992749, 6.6% complete
..........
Epoch: 3 Train loss: 3.583709622034803e-05, 7.7% complete
..........
Epoch: 3 Train loss: 0.005215696059167385, 8.8% complete
..........
Epoch: 3 Train loss: 0.00010805556667037308, 9.9% complete
..........
Epoch: 3 Train loss: 0.01082588266581297, 11.0% complete
..........
Epoch: 3 Train loss: 5.2299808885436505e-05, 12.0% complete
..........
Epoch: 3 Train loss: 0.010097099468111992, 13.1% complete
..........
Epoch: 3 Train loss: 1.4745652151759714e-05, 14.2% complete
..........
Epoch: 3 Train loss: 0.0052145253866910934, 15.3% complete
..........
Epoch: 3 Train loss: 0.007890619337558746, 16.4% complete
..........
Epoch: 3 Train loss: 0.006221297662705183, 17.5% complete
..........
Epoch: 3 Train loss: 1.2927041098009795e-05, 18.5% complete
..........
Epoch: 3 Train loss: 1.0100542567670345e-05, 19.6% complete
..........
Epoch: 3 Train loss: 5.846128624398261e-05, 20.7% complete
..........
Epoch: 3 Train loss: 0.006709426641464233, 21.8% complete
..........
Epoch: 3 Train loss: 0.0008206584607250988, 22.9% complete
..........
Epoch: 3 Train loss: 4.467739927349612e-05, 24.0% complete
..........
Epoch: 3 Train loss: 0.019366314634680748, 25.1% complete
..........
Epoch: 3 Train loss: 0.00893765315413475, 26.1% complete
..........
Epoch: 3 Train loss: 0.0010639030952006578, 27.2% complete
..........
Epoch: 3 Train loss: 0.006474333815276623, 28.3% complete
..........
Epoch: 3 Train loss: 0.013905348256230354, 29.4% complete
..........
Epoch: 3 Train loss: 0.008575181476771832, 30.5% complete
..........
Epoch: 3 Train loss: 0.0015110778622329235, 31.6% complete
..........
Epoch: 3 Train loss: 5.82052452955395e-05, 32.6% complete
..........
Epoch: 3 Train loss: 0.0061546857468783855, 33.7% complete
..........
Epoch: 3 Train loss: 0.03229470178484917, 34.8% complete
..........
Epoch: 3 Train loss: 0.01014699973165989, 35.9% complete
..........
Epoch: 3 Train loss: 0.00990252010524273, 37.0% complete
..........
Epoch: 3 Train loss: 0.012196485884487629, 38.1% complete
..........
Epoch: 3 Train loss: 0.0034673763439059258, 39.2% complete
..........
Epoch: 3 Train loss: 0.0011386844562366605, 40.2% complete
..........
Epoch: 3 Train loss: 8.064064604695886e-05, 41.3% complete
..........
Epoch: 3 Train loss: 0.009309406392276287, 42.4% complete
..........
Epoch: 3 Train loss: 5.077978130429983e-06, 43.5% complete
..........
Epoch: 3 Train loss: 0.008957617916166782, 44.6% complete
..........
Epoch: 3 Train loss: 0.0004996831412427127, 45.7% complete
..........
Epoch: 3 Train loss: 0.007759500294923782, 46.7% complete
..........
Epoch: 3 Train loss: 0.006234250031411648, 47.8% complete
..........
Epoch: 3 Train loss: 3.7202989915385842e-06, 48.9% complete
..........
Epoch: 3 Train loss: 0.003121906891465187, 50.0% complete
..........
Epoch: 3 Train loss: 0.00015001624706201255, 51.1% complete
..........
Epoch: 3 Train loss: 0.004134698770940304, 52.2% complete
..........
Epoch: 3 Train loss: 0.01051191333681345, 53.3% complete
..........
Epoch: 3 Train loss: 0.001093778177164495, 54.3% complete
..........
Epoch: 3 Train loss: 0.01161027979105711, 55.4% complete
..........
Epoch: 3 Train loss: 0.003450720803812146, 56.5% complete
..........
Epoch: 3 Train loss: 0.0009543095366097987, 57.6% complete
..........
Epoch: 3 Train loss: 0.009531211107969284, 58.7% complete
..........
Epoch: 3 Train loss: 0.013085774146020412, 59.8% complete
..........
Epoch: 3 Train loss: 2.457651135046035e-05, 60.8% complete
..........
Epoch: 3 Train loss: 8.283321221824735e-06, 61.9% complete
..........
Epoch: 3 Train loss: 0.006065087858587503, 63.0% complete
..........
Epoch: 3 Train loss: 0.007235684897750616, 64.1% complete
..........
Epoch: 3 Train loss: 0.0032229400239884853, 65.2% complete
..........
Epoch: 3 Train loss: 0.0007262747385539114, 66.3% complete
..........
Epoch: 3 Train loss: 0.0070906151086091995, 67.4% complete
..........
Epoch: 3 Train loss: 4.125053237657994e-06, 68.4% complete
..........
Epoch: 3 Train loss: 0.010544590651988983, 69.5% complete
..........
Epoch: 3 Train loss: 0.005966964177787304, 70.6% complete
..........
Epoch: 3 Train loss: 0.00800678227096796, 71.7% complete
..........
Epoch: 3 Train loss: 0.01068141870200634, 72.8% complete
..........
Epoch: 3 Train loss: 0.011560700833797455, 73.9% complete
..........
Epoch: 3 Train loss: 0.00046617063344456255, 74.9% complete
..........
Epoch: 3 Train loss: 0.008685012347996235, 76.0% complete
..........
Epoch: 3 Train loss: 0.00491676852107048, 77.1% complete
..........
Epoch: 3 Train loss: 0.005293938796967268, 78.2% complete
..........
Epoch: 3 Train loss: 0.007923622615635395, 79.3% complete
..........
Epoch: 3 Train loss: 0.010382389649748802, 80.4% complete
..........
Epoch: 3 Train loss: 4.437386814970523e-05, 81.5% complete
..........
Epoch: 3 Train loss: 0.006530263926833868, 82.5% complete
..........
Epoch: 3 Train loss: 0.004065524321049452, 83.6% complete
..........
Epoch: 3 Train loss: 0.010017604567110538, 84.7% complete
..........
Epoch: 3 Train loss: 0.008246955461800098, 85.8% complete
..........
Epoch: 3 Train loss: 4.572699253913015e-06, 86.9% complete
..........
Epoch: 3 Train loss: 0.011918192729353905, 88.0% complete
..........
Epoch: 3 Train loss: 1.355602580588311e-05, 89.0% complete
..........
Epoch: 3 Train loss: 3.921516326954588e-05, 90.1% complete
..........
Epoch: 3 Train loss: 0.008455916307866573, 91.2% complete
..........
Epoch: 3 Train loss: 1.3268749171402305e-05, 92.3% complete
..........
Epoch: 3 Train loss: 0.011259065940976143, 93.4% complete
..........
Epoch: 3 Train loss: 0.00016188673907890916, 94.5% complete
..........
Epoch: 3 Train loss: 6.898257561260834e-05, 95.6% complete
..........
Epoch: 3 Train loss: 0.0003830083296634257, 96.6% complete
..........
Epoch: 3 Train loss: 0.004931455012410879, 97.7% complete
..........
Epoch: 3 Train loss: 0.008233839645981789, 98.8% complete
..........
Epoch: 3 Train loss: 0.009769575670361519, 99.9% complete
..
Training complete
Validating epoch 3...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004551389720290899
Batch 0 Loss 0.004551389720290899
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011092200875282288
Batch 1 Loss 0.011092200875282288
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008131427690386772
Batch 2 Loss 0.008131427690386772
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 9.14972770260647e-06
Batch 3 Loss 9.14972770260647e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005801354069262743
Batch 4 Loss 0.005801354069262743
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 8.035647624637932e-06
Batch 5 Loss 8.035647624637932e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 8.176176197594032e-05
Batch 6 Loss 8.176176197594032e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 2.7519199647940695e-05
Batch 7 Loss 2.7519199647940695e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 7.017630559857935e-05
Batch 8 Loss 7.017630559857935e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 3.5212779039284214e-05
Batch 9 Loss 3.5212779039284214e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008121045306324959
Batch 10 Loss 0.008121045306324959
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009573378600180149
Batch 11 Loss 0.009573378600180149
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008718565106391907
Batch 12 Loss 0.008718565106391907
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002886852715164423
Batch 13 Loss 0.002886852715164423
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 2.775269967969507e-05
Batch 14 Loss 2.775269967969507e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007057978305965662
Batch 15 Loss 0.007057978305965662
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005338022951036692
Batch 16 Loss 0.005338022951036692
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 8.143637387547642e-06
Batch 17 Loss 8.143637387547642e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 7.910835847724229e-06
Batch 18 Loss 7.910835847724229e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 3.9153917896328494e-05
Batch 19 Loss 3.9153917896328494e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004544796422123909
Batch 20 Loss 0.004544796422123909
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009004940278828144
Batch 21 Loss 0.009004940278828144
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1951393389608711e-05
Batch 22 Loss 1.1951393389608711e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011187704280018806
Batch 23 Loss 0.011187704280018806
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2727425200864673e-05
Batch 24 Loss 1.2727425200864673e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006195580121129751
Batch 25 Loss 0.006195580121129751
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006903891917318106
Batch 26 Loss 0.006903891917318106
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 8.498725946992636e-06
Batch 27 Loss 8.498725946992636e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004617833066731691
Batch 28 Loss 0.004617833066731691
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7332691641058773e-05
Batch 29 Loss 1.7332691641058773e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 4.163156700087711e-05
Batch 30 Loss 4.163156700087711e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006892222445458174
Batch 31 Loss 0.006892222445458174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003166508162394166
Batch 32 Loss 0.003166508162394166
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1134623491670936e-05
Batch 33 Loss 1.1134623491670936e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 6.643153028562665e-06
Batch 34 Loss 6.643153028562665e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00976093765348196
Batch 35 Loss 0.00976093765348196
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 9.438575216336176e-05
Batch 36 Loss 9.438575216336176e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0075192623771727085
Batch 37 Loss 0.0075192623771727085
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007106634322553873
Batch 38 Loss 0.007106634322553873
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009920972399413586
Batch 39 Loss 0.009920972399413586
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012300225906074047
Batch 40 Loss 0.012300225906074047
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002505213487893343
Batch 41 Loss 0.002505213487893343
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006175253540277481
Batch 42 Loss 0.006175253540277481
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007027806248515844
Batch 43 Loss 0.007027806248515844
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033029012847691774
Batch 44 Loss 0.0033029012847691774
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0044241915456950665
Batch 45 Loss 0.0044241915456950665
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00471343332901597
Batch 46 Loss 0.00471343332901597
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003767584916204214
Batch 47 Loss 0.003767584916204214
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004815149120986462
Batch 48 Loss 0.004815149120986462
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010195898823440075
Batch 49 Loss 0.010195898823440075
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036530850920826197
Batch 50 Loss 0.0036530850920826197
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003111351514235139
Batch 51 Loss 0.003111351514235139
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 2.451812179060653e-05
Batch 52 Loss 2.451812179060653e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006960500031709671
Batch 53 Loss 0.006960500031709671
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008938003331422806
Batch 54 Loss 0.008938003331422806
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008183474652469158
Batch 55 Loss 0.008183474652469158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005335787311196327
Batch 56 Loss 0.005335787311196327
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006574814673513174
Batch 57 Loss 0.006574814673513174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004708734340965748
Batch 58 Loss 0.004708734340965748
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015400413423776627
Batch 59 Loss 0.015400413423776627
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 7.278656994458288e-06
Batch 60 Loss 7.278656994458288e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00559182558208704
Batch 61 Loss 0.00559182558208704
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 1.867163518909365e-05
Batch 62 Loss 1.867163518909365e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 2.284306538058445e-05
Batch 63 Loss 2.284306538058445e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008521055802702904
Batch 64 Loss 0.008521055802702904
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00917114783078432
Batch 65 Loss 0.00917114783078432
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00020736755686812103
Batch 66 Loss 0.00020736755686812103
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 7.111411832738668e-06
Batch 67 Loss 7.111411832738668e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007055747322738171
Batch 68 Loss 0.007055747322738171
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025219982489943504
Batch 69 Loss 0.0025219982489943504
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3700409908778965e-05
Batch 70 Loss 1.3700409908778965e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007563451305031776
Batch 71 Loss 0.007563451305031776
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011133480817079544
Batch 72 Loss 0.011133480817079544
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 7.24657584214583e-05
Batch 73 Loss 7.24657584214583e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 3.768103488255292e-05
Batch 74 Loss 3.768103488255292e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007618207484483719
Batch 75 Loss 0.007618207484483719
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 8.961345884017646e-06
Batch 76 Loss 8.961345884017646e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010147764347493649
Batch 77 Loss 0.010147764347493649
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003264730330556631
Batch 78 Loss 0.003264730330556631
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010077939368784428
Batch 79 Loss 0.010077939368784428
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 7.039678166620433e-06
Batch 80 Loss 7.039678166620433e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 7.352587999776006e-06
Batch 81 Loss 7.352587999776006e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007650749292224646
Batch 82 Loss 0.007650749292224646
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00807215180248022
Batch 83 Loss 0.00807215180248022
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007343363016843796
Batch 84 Loss 0.007343363016843796
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6862712072907016e-05
Batch 85 Loss 1.6862712072907016e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007385488133877516
Batch 86 Loss 0.007385488133877516
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006521943956613541
Batch 87 Loss 0.006521943956613541
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00018567382358014584
Batch 88 Loss 0.00018567382358014584
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0073924497701227665
Batch 89 Loss 0.0073924497701227665
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00897123757749796
Batch 90 Loss 0.00897123757749796
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004351270385086536
Batch 91 Loss 0.004351270385086536
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010609051212668419
Batch 92 Loss 0.010609051212668419
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 7.633818313479424e-06
Batch 93 Loss 7.633818313479424e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0894065781030804e-05
Batch 94 Loss 1.0894065781030804e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00835470762103796
Batch 95 Loss 0.00835470762103796
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01319082360714674
Batch 96 Loss 0.01319082360714674
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 9.116542059928179e-06
Batch 97 Loss 9.116542059928179e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011508929310366511
Batch 98 Loss 0.0011508929310366511
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 2.0955347281415015e-05
Batch 99 Loss 2.0955347281415015e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008729537948966026
Batch 100 Loss 0.008729537948966026
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016794211696833372
Batch 101 Loss 0.0016794211696833372
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007211180403828621
Batch 102 Loss 0.007211180403828621
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008412277325987816
Batch 103 Loss 0.008412277325987816
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009835216216742992
Batch 104 Loss 0.009835216216742992
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 6.886832125019282e-06
Batch 105 Loss 6.886832125019282e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 5.0728391215670854e-05
Batch 106 Loss 5.0728391215670854e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007049277424812317
Batch 107 Loss 0.007049277424812317
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026456257328391075
Batch 108 Loss 0.0026456257328391075
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007341069635003805
Batch 109 Loss 0.007341069635003805
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003812502371147275
Batch 110 Loss 0.003812502371147275
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00884599331766367
Batch 111 Loss 0.00884599331766367
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0607021977193654e-05
Batch 112 Loss 1.0607021977193654e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 6.773298082407564e-06
Batch 113 Loss 6.773298082407564e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 4...

Epoch: 4 Train loss: 0.0019759642891585827, 0.1% complete
..........
Epoch: 4 Train loss: 0.011474967934191227, 1.2% complete
..........
Epoch: 4 Train loss: 4.840247493120842e-05, 2.3% complete
..........
Epoch: 4 Train loss: 1.3121432857587934e-05, 3.4% complete
..........
Epoch: 4 Train loss: 0.005951039493083954, 4.4% complete
..........
Epoch: 4 Train loss: 0.008237196132540703, 5.5% complete
..........
Epoch: 4 Train loss: 0.0084190359339118, 6.6% complete
..........
Epoch: 4 Train loss: 0.0003273810725659132, 7.7% complete
..........
Epoch: 4 Train loss: 0.006933888886123896, 8.8% complete
..........
Epoch: 4 Train loss: 0.004982141777873039, 9.9% complete
..........
Epoch: 4 Train loss: 0.008110586553812027, 11.0% complete
..........
Epoch: 4 Train loss: 5.4994161473587155e-06, 12.0% complete
..........
Epoch: 4 Train loss: 0.004043160006403923, 13.1% complete
..........
Epoch: 4 Train loss: 6.966401997487992e-06, 14.2% complete
..........
Epoch: 4 Train loss: 0.00888750422745943, 15.3% complete
..........
Epoch: 4 Train loss: 0.001766397850587964, 16.4% complete
..........
Epoch: 4 Train loss: 0.007445672992616892, 17.5% complete
..........
Epoch: 4 Train loss: 0.007887004874646664, 18.5% complete
..........
Epoch: 4 Train loss: 0.006815719418227673, 19.6% complete
..........
Epoch: 4 Train loss: 5.703805072698742e-06, 20.7% complete
..........
Epoch: 4 Train loss: 0.011775370687246323, 21.8% complete
..........
Epoch: 4 Train loss: 0.010873219929635525, 22.9% complete
..........
Epoch: 4 Train loss: 0.004645381588488817, 24.0% complete
..........
Epoch: 4 Train loss: 3.4500764741096646e-05, 25.1% complete
..........
Epoch: 4 Train loss: 4.462736251298338e-05, 26.1% complete
..........
Epoch: 4 Train loss: 0.0009818098042160273, 27.2% complete
..........
Epoch: 4 Train loss: 0.0021078740246593952, 28.3% complete
..........
Epoch: 4 Train loss: 0.0024249639827758074, 29.4% complete
..........
Epoch: 4 Train loss: 0.008303086273372173, 30.5% complete
..........
Epoch: 4 Train loss: 0.009461178444325924, 31.6% complete
..........
Epoch: 4 Train loss: 5.6978151405928656e-05, 32.6% complete
..........
Epoch: 4 Train loss: 6.669397407677025e-06, 33.7% complete
..........
Epoch: 4 Train loss: 0.007873402908444405, 34.8% complete
..........
Epoch: 4 Train loss: 4.411798727232963e-06, 35.9% complete
..........
Epoch: 4 Train loss: 0.0017113847425207496, 37.0% complete
..........
Epoch: 4 Train loss: 1.3178083463571966e-05, 38.1% complete
..........
Epoch: 4 Train loss: 4.2967501940438524e-05, 39.2% complete
..........
Epoch: 4 Train loss: 0.011569637805223465, 40.2% complete
..........
Epoch: 4 Train loss: 0.009650972671806812, 41.3% complete
..........
Epoch: 4 Train loss: 2.749163832049817e-05, 42.4% complete
..........
Epoch: 4 Train loss: 0.007471081335097551, 43.5% complete
..........
Epoch: 4 Train loss: 0.004798031412065029, 44.6% complete
..........
Epoch: 4 Train loss: 1.5279481885954738e-05, 45.7% complete
..........
Epoch: 4 Train loss: 0.0036369627341628075, 46.7% complete
..........
Epoch: 4 Train loss: 0.004892750643193722, 47.8% complete
..........
Epoch: 4 Train loss: 1.170542964246124e-05, 48.9% complete
..........
Epoch: 4 Train loss: 0.007047303952276707, 50.0% complete
..........
Epoch: 4 Train loss: 0.0030514157842844725, 51.1% complete
..........
Epoch: 4 Train loss: 0.008569425903260708, 52.2% complete
..........
Epoch: 4 Train loss: 0.00014887299039401114, 53.3% complete
..........
Epoch: 4 Train loss: 0.008365349844098091, 54.3% complete
..........
Epoch: 4 Train loss: 0.008532467298209667, 55.4% complete
..........
Epoch: 4 Train loss: 0.006995140574872494, 56.5% complete
..........
Epoch: 4 Train loss: 0.005609120242297649, 57.6% complete
..........
Epoch: 4 Train loss: 3.453133103903383e-06, 58.7% complete
..........
Epoch: 4 Train loss: 4.861230627284385e-05, 59.8% complete
..........
Epoch: 4 Train loss: 0.004868306685239077, 60.8% complete
..........
Epoch: 4 Train loss: 0.014024737291038036, 61.9% complete
..........
Epoch: 4 Train loss: 0.004837801679968834, 63.0% complete
..........
Epoch: 4 Train loss: 1.2220574717503041e-05, 64.1% complete
..........
Epoch: 4 Train loss: 0.00632471265271306, 65.2% complete
..........
Epoch: 4 Train loss: 0.0007849402027204633, 66.3% complete
..........
Epoch: 4 Train loss: 4.085777618456632e-06, 67.4% complete
..........
Epoch: 4 Train loss: 0.0035660776775330305, 68.4% complete
..........
Epoch: 4 Train loss: 0.0045237671583890915, 69.5% complete
..........
Epoch: 4 Train loss: 3.636377368820831e-05, 70.6% complete
..........
Epoch: 4 Train loss: 0.004096252378076315, 71.7% complete
..........
Epoch: 4 Train loss: 0.0036522510927170515, 72.8% complete
..........
Epoch: 4 Train loss: 0.011230982840061188, 73.9% complete
..........
Epoch: 4 Train loss: 6.3401683291886e-05, 74.9% complete
..........
Epoch: 4 Train loss: 0.008648823946714401, 76.0% complete
..........
Epoch: 4 Train loss: 6.544301868416369e-06, 77.1% complete
..........
Epoch: 4 Train loss: 9.866053005680442e-06, 78.2% complete
..........
Epoch: 4 Train loss: 0.009251843206584454, 79.3% complete
..........
Epoch: 4 Train loss: 0.00993331428617239, 80.4% complete
..........
Epoch: 4 Train loss: 2.021645923377946e-05, 81.5% complete
..........
Epoch: 4 Train loss: 0.0018013218650594354, 82.5% complete
..........
Epoch: 4 Train loss: 0.006702959071844816, 83.6% complete
..........
Epoch: 4 Train loss: 1.0767951607704163e-05, 84.7% complete
..........
Epoch: 4 Train loss: 0.007122596725821495, 85.8% complete
..........
Epoch: 4 Train loss: 0.00911854486912489, 86.9% complete
..........
Epoch: 4 Train loss: 0.007462583016604185, 88.0% complete
..........
Epoch: 4 Train loss: 5.425856215879321e-06, 89.0% complete
..........
Epoch: 4 Train loss: 0.007648535072803497, 90.1% complete
..........
Epoch: 4 Train loss: 2.831895835697651e-05, 91.2% complete
..........
Epoch: 4 Train loss: 0.007829754613339901, 92.3% complete
..........
Epoch: 4 Train loss: 0.007295416202396154, 93.4% complete
..........
Epoch: 4 Train loss: 8.868366421665996e-06, 94.5% complete
..........
Epoch: 4 Train loss: 6.926138303242624e-05, 95.6% complete
..........
Epoch: 4 Train loss: 0.004853558726608753, 96.6% complete
..........
Epoch: 4 Train loss: 0.008861162699759007, 97.7% complete
..........
Epoch: 4 Train loss: 0.01035973709076643, 98.8% complete
..........
Epoch: 4 Train loss: 0.008189050480723381, 99.9% complete
..
Training complete
Validating epoch 4...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008077473379671574
Batch 0 Loss 0.008077473379671574
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 5.189410876482725e-05
Batch 1 Loss 5.189410876482725e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0050147040747106075
Batch 2 Loss 0.0050147040747106075
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001477348618209362
Batch 3 Loss 0.001477348618209362
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002144500846043229
Batch 4 Loss 0.002144500846043229
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012258188799023628
Batch 5 Loss 0.012258188799023628
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2174023140687495e-05
Batch 6 Loss 1.2174023140687495e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019652198534458876
Batch 7 Loss 0.0019652198534458876
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2916592822875828e-05
Batch 8 Loss 1.2916592822875828e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002325156528968364
Batch 9 Loss 0.0002325156528968364
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014374253805726767
Batch 10 Loss 0.00014374253805726767
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007355001289397478
Batch 11 Loss 0.007355001289397478
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1642044885084033e-05
Batch 12 Loss 2.1642044885084033e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4509459763066843e-05
Batch 13 Loss 2.4509459763066843e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003924344200640917
Batch 14 Loss 0.003924344200640917
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013328967615962029
Batch 15 Loss 0.013328967615962029
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007954941131174564
Batch 16 Loss 0.007954941131174564
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017496756045147777
Batch 17 Loss 0.0017496756045147777
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001047627767547965
Batch 18 Loss 0.001047627767547965
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 5.327141843736172e-05
Batch 19 Loss 5.327141843736172e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006857308559119701
Batch 20 Loss 0.006857308559119701
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017107251333072782
Batch 21 Loss 0.0017107251333072782
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 5.9909449191764e-05
Batch 22 Loss 5.9909449191764e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015365895815193653
Batch 23 Loss 0.015365895815193653
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5533070836681873e-05
Batch 24 Loss 1.5533070836681873e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010791046544909477
Batch 25 Loss 0.010791046544909477
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5990674000931904e-05
Batch 26 Loss 1.5990674000931904e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007575477939099073
Batch 27 Loss 0.007575477939099073
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 4.193259519524872e-05
Batch 28 Loss 4.193259519524872e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012181892059743404
Batch 29 Loss 0.012181892059743404
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008533958345651627
Batch 30 Loss 0.008533958345651627
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01077855471521616
Batch 31 Loss 0.01077855471521616
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 2.759177004918456e-05
Batch 32 Loss 2.759177004918456e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008296228013932705
Batch 33 Loss 0.008296228013932705
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 1.815054565668106e-05
Batch 34 Loss 1.815054565668106e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011233112774789333
Batch 35 Loss 0.011233112774789333
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0069642323069274426
Batch 36 Loss 0.0069642323069274426
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006403259467333555
Batch 37 Loss 0.006403259467333555
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 5.237981167738326e-05
Batch 38 Loss 5.237981167738326e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0076066129840910435
Batch 39 Loss 0.0076066129840910435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008811536245048046
Batch 40 Loss 0.008811536245048046
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006174555979669094
Batch 41 Loss 0.006174555979669094
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002293545287102461
Batch 42 Loss 0.002293545287102461
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004597953520715237
Batch 43 Loss 0.004597953520715237
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008891750127077103
Batch 44 Loss 0.008891750127077103
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059224083088338375
Batch 45 Loss 0.0059224083088338375
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010261966846883297
Batch 46 Loss 0.010261966846883297
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0048792860470712185
Batch 47 Loss 0.0048792860470712185
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 6.775605288567021e-05
Batch 48 Loss 6.775605288567021e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 2.525740273995325e-05
Batch 49 Loss 2.525740273995325e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005215773358941078
Batch 50 Loss 0.005215773358941078
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017486180877313018
Batch 51 Loss 0.00017486180877313018
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010928326286375523
Batch 52 Loss 0.010928326286375523
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006147103849798441
Batch 53 Loss 0.006147103849798441
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001007021521218121
Batch 54 Loss 0.001007021521218121
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 8.357351907761768e-05
Batch 55 Loss 8.357351907761768e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 4.0949984395410866e-05
Batch 56 Loss 4.0949984395410866e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 2.579626743681729e-05
Batch 57 Loss 2.579626743681729e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019396827556192875
Batch 58 Loss 0.0019396827556192875
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016343995230272412
Batch 59 Loss 0.0016343995230272412
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0043394058011472225
Batch 60 Loss 0.0043394058011472225
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008293788880109787
Batch 61 Loss 0.008293788880109787
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3486773241311312e-05
Batch 62 Loss 1.3486773241311312e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3394317647907883e-05
Batch 63 Loss 1.3394317647907883e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 2.056761150015518e-05
Batch 64 Loss 2.056761150015518e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006104530766606331
Batch 65 Loss 0.006104530766606331
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007789968978613615
Batch 66 Loss 0.007789968978613615
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008829607395455241
Batch 67 Loss 0.0008829607395455241
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005729407537728548
Batch 68 Loss 0.005729407537728548
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007741350214928389
Batch 69 Loss 0.007741350214928389
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007315807975828648
Batch 70 Loss 0.007315807975828648
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003634630935266614
Batch 71 Loss 0.003634630935266614
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 3.174278390360996e-05
Batch 72 Loss 3.174278390360996e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 6.334826321108267e-05
Batch 73 Loss 6.334826321108267e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002441445831209421
Batch 74 Loss 0.002441445831209421
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2902128219138831e-05
Batch 75 Loss 1.2902128219138831e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006820878945291042
Batch 76 Loss 0.006820878945291042
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010251115076243877
Batch 77 Loss 0.010251115076243877
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 3.7856807466596365e-05
Batch 78 Loss 3.7856807466596365e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01708940789103508
Batch 79 Loss 0.01708940789103508
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2705269909929484e-05
Batch 80 Loss 1.2705269909929484e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005232836119830608
Batch 81 Loss 0.005232836119830608
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0047381273470819
Batch 82 Loss 0.0047381273470819
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006975410506129265
Batch 83 Loss 0.006975410506129265
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021547237411141396
Batch 84 Loss 0.0021547237411141396
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007952987216413021
Batch 85 Loss 0.007952987216413021
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006510039675049484
Batch 86 Loss 0.0006510039675049484
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012392276898026466
Batch 87 Loss 0.012392276898026466
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 3.733312405529432e-05
Batch 88 Loss 3.733312405529432e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007870616391301155
Batch 89 Loss 0.007870616391301155
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010597874643281102
Batch 90 Loss 0.0010597874643281102
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 2.031575422734022e-05
Batch 91 Loss 2.031575422734022e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005684949457645416
Batch 92 Loss 0.005684949457645416
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013948673149570823
Batch 93 Loss 0.0013948673149570823
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003513899282552302
Batch 94 Loss 0.0003513899282552302
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 4.2830841266550124e-05
Batch 95 Loss 4.2830841266550124e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006541855400428176
Batch 96 Loss 0.0006541855400428176
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011195178143680096
Batch 97 Loss 0.011195178143680096
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005451876670122147
Batch 98 Loss 0.005451876670122147
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029895040206611156
Batch 99 Loss 0.0029895040206611156
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008423497900366783
Batch 100 Loss 0.008423497900366783
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00935378484427929
Batch 101 Loss 0.00935378484427929
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006062861531972885
Batch 102 Loss 0.006062861531972885
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0073540168814361095
Batch 103 Loss 0.0073540168814361095
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0039017812814563513
Batch 104 Loss 0.0039017812814563513
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005843226797878742
Batch 105 Loss 0.005843226797878742
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010667458176612854
Batch 106 Loss 0.010667458176612854
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 6.0829202993772924e-05
Batch 107 Loss 6.0829202993772924e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011400721967220306
Batch 108 Loss 0.011400721967220306
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 2.226808646810241e-05
Batch 109 Loss 2.226808646810241e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035851981956511736
Batch 110 Loss 0.0035851981956511736
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010723263025283813
Batch 111 Loss 0.010723263025283813
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007810806389898062
Batch 112 Loss 0.007810806389898062
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00801133457571268
Batch 113 Loss 0.00801133457571268
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 5...

Epoch: 5 Train loss: 1.583889388712123e-05, 0.1% complete
..........
Epoch: 5 Train loss: 0.010129882954061031, 1.2% complete
..........
Epoch: 5 Train loss: 0.004804178606718779, 2.3% complete
..........
Epoch: 5 Train loss: 0.006487753242254257, 3.4% complete
..........
Epoch: 5 Train loss: 0.0003420935827307403, 4.4% complete
..........
Epoch: 5 Train loss: 0.007162628695368767, 5.5% complete
..........
Epoch: 5 Train loss: 0.003800727194175124, 6.6% complete
..........
Epoch: 5 Train loss: 0.006197530310600996, 7.7% complete
..........
Epoch: 5 Train loss: 0.004924059379845858, 8.8% complete
..........
Epoch: 5 Train loss: 0.01008402556180954, 9.9% complete
..........
Epoch: 5 Train loss: 0.002079606521874666, 11.0% complete
..........
Epoch: 5 Train loss: 0.0018066739430651069, 12.0% complete
..........
Epoch: 5 Train loss: 0.007185697555541992, 13.1% complete
..........
Epoch: 5 Train loss: 0.003679711138829589, 14.2% complete
..........
Epoch: 5 Train loss: 0.0002109573979396373, 15.3% complete
..........
Epoch: 5 Train loss: 0.00019497430184856057, 16.4% complete
..........
Epoch: 5 Train loss: 0.00509177939966321, 17.5% complete
..........
Epoch: 5 Train loss: 0.009670435450971127, 18.5% complete
..........
Epoch: 5 Train loss: 0.005648873746395111, 19.6% complete
..........
Epoch: 5 Train loss: 0.011825984343886375, 20.7% complete
..........
Epoch: 5 Train loss: 2.1450076019391418e-05, 21.8% complete
..........
Epoch: 5 Train loss: 0.0003382186987437308, 22.9% complete
..........
Epoch: 5 Train loss: 7.637977250851691e-05, 24.0% complete
..........
Epoch: 5 Train loss: 1.2487973435781896e-05, 25.1% complete
..........
Epoch: 5 Train loss: 0.007307762745767832, 26.1% complete
..........
Epoch: 5 Train loss: 0.015910658985376358, 27.2% complete
..........
Epoch: 5 Train loss: 8.92152456799522e-06, 28.3% complete
..........
Epoch: 5 Train loss: 0.0002013179037021473, 29.4% complete
..........
Epoch: 5 Train loss: 1.0093754099216312e-05, 30.5% complete
..........
Epoch: 5 Train loss: 0.007438378408551216, 31.6% complete
..........
Epoch: 5 Train loss: 0.007331208325922489, 32.6% complete
..........
Epoch: 5 Train loss: 3.443124296609312e-05, 33.7% complete
..........
Epoch: 5 Train loss: 0.007226412184536457, 34.8% complete
..........
Epoch: 5 Train loss: 1.625001459615305e-05, 35.9% complete
..........
Epoch: 5 Train loss: 0.005284752696752548, 37.0% complete
..........
Epoch: 5 Train loss: 5.39360917173326e-06, 38.1% complete
..........
Epoch: 5 Train loss: 0.003214032854884863, 39.2% complete
..........
Epoch: 5 Train loss: 0.0016172833275049925, 40.2% complete
..........
Epoch: 5 Train loss: 5.566951585933566e-06, 41.3% complete
..........
Epoch: 5 Train loss: 0.007357160095125437, 42.4% complete
..........
Epoch: 5 Train loss: 0.007789483293890953, 43.5% complete
..........
Epoch: 5 Train loss: 0.006086793728172779, 44.6% complete
..........
Epoch: 5 Train loss: 0.004350489936769009, 45.7% complete
..........
Epoch: 5 Train loss: 0.0016568327555432916, 46.7% complete
..........
Epoch: 5 Train loss: 0.003901290474459529, 47.8% complete
..........
Epoch: 5 Train loss: 0.00925011932849884, 48.9% complete
..........
Epoch: 5 Train loss: 0.002647004323080182, 50.0% complete
..........
Epoch: 5 Train loss: 2.2290070774033666e-05, 51.1% complete
..........
Epoch: 5 Train loss: 0.005568554624915123, 52.2% complete
..........
Epoch: 5 Train loss: 0.0005534897209145129, 53.3% complete
..........
Epoch: 5 Train loss: 0.006215668749064207, 54.3% complete
..........
Epoch: 5 Train loss: 0.004318905062973499, 55.4% complete
..........
Epoch: 5 Train loss: 0.010678817518055439, 56.5% complete
..........
Epoch: 5 Train loss: 0.003841302590444684, 57.6% complete
..........
Epoch: 5 Train loss: 0.005531474947929382, 58.7% complete
..........
Epoch: 5 Train loss: 2.421530371066183e-05, 59.8% complete
..........
Epoch: 5 Train loss: 1.767501817084849e-05, 60.8% complete
..........
Epoch: 5 Train loss: 0.0003038213471882045, 61.9% complete
..........
Epoch: 5 Train loss: 0.006374721881002188, 63.0% complete
..........
Epoch: 5 Train loss: 0.008477197960019112, 64.1% complete
..........
Epoch: 5 Train loss: 1.3645571016240865e-05, 65.2% complete
..........
Epoch: 5 Train loss: 0.010641542263329029, 66.3% complete
..........
Epoch: 5 Train loss: 0.007129186764359474, 67.4% complete
..........
Epoch: 5 Train loss: 0.007396263536065817, 68.4% complete
..........
Epoch: 5 Train loss: 0.000646076921839267, 69.5% complete
..........
Epoch: 5 Train loss: 0.005412525963038206, 70.6% complete
..........
Epoch: 5 Train loss: 3.82841972168535e-06, 71.7% complete
..........
Epoch: 5 Train loss: 0.004755149595439434, 72.8% complete
..........
Epoch: 5 Train loss: 0.007599324453622103, 73.9% complete
..........
Epoch: 5 Train loss: 0.0017230829689651728, 74.9% complete
..........
Epoch: 5 Train loss: 0.004580148030072451, 76.0% complete
..........
Epoch: 5 Train loss: 8.214240369852632e-05, 77.1% complete
..........
Epoch: 5 Train loss: 1.6609381418675184e-05, 78.2% complete
..........
Epoch: 5 Train loss: 0.006621671374887228, 79.3% complete
..........
Epoch: 5 Train loss: 0.003118068678304553, 80.4% complete
..........
Epoch: 5 Train loss: 0.0030823126435279846, 81.5% complete
..........
Epoch: 5 Train loss: 4.668247129302472e-06, 82.5% complete
..........
Epoch: 5 Train loss: 0.0078123947605490685, 83.6% complete
..........
Epoch: 5 Train loss: 0.0068090856075286865, 84.7% complete
..........
Epoch: 5 Train loss: 0.008114341646432877, 85.8% complete
..........
Epoch: 5 Train loss: 0.007052905857563019, 86.9% complete
..........
Epoch: 5 Train loss: 0.004111321642994881, 88.0% complete
..........
Epoch: 5 Train loss: 0.007152989972382784, 89.0% complete
..........
Epoch: 5 Train loss: 1.5545665519312024e-06, 90.1% complete
..........
Epoch: 5 Train loss: 0.003883786266669631, 91.2% complete
..........
Epoch: 5 Train loss: 0.00016081369540188462, 92.3% complete
..........
Epoch: 5 Train loss: 0.00805422943085432, 93.4% complete
..........
Epoch: 5 Train loss: 0.00020764708460774273, 94.5% complete
..........
Epoch: 5 Train loss: 0.0018634387524798512, 95.6% complete
..........
Epoch: 5 Train loss: 0.003053196705877781, 96.6% complete
..........
Epoch: 5 Train loss: 5.339490598998964e-06, 97.7% complete
..........
Epoch: 5 Train loss: 2.3498403606936336e-05, 98.8% complete
..........
Epoch: 5 Train loss: 4.420086042955518e-06, 99.9% complete
..
Training complete
Validating epoch 5...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00022284475562628359
Batch 0 Loss 0.00022284475562628359
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0070603350177407265
Batch 1 Loss 0.0070603350177407265
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 9.710591984912753e-05
Batch 2 Loss 9.710591984912753e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 2.9979724786244333e-05
Batch 3 Loss 2.9979724786244333e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007882307982072234
Batch 4 Loss 0.0007882307982072234
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009116481989622116
Batch 5 Loss 0.009116481989622116
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009035739116370678
Batch 6 Loss 0.009035739116370678
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004914154997095466
Batch 7 Loss 0.0004914154997095466
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 5.0166883738711476e-05
Batch 8 Loss 5.0166883738711476e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 3.720251697814092e-05
Batch 9 Loss 3.720251697814092e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 9.507840877631679e-05
Batch 10 Loss 9.507840877631679e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3440825568977743e-05
Batch 11 Loss 1.3440825568977743e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00015678380441386253
Batch 12 Loss 0.00015678380441386253
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 3.840068529825658e-06
Batch 13 Loss 3.840068529825658e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026604696176946163
Batch 14 Loss 0.0026604696176946163
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0065028052777051926
Batch 15 Loss 0.0065028052777051926
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006032880395650864
Batch 16 Loss 0.006032880395650864
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7343987565254793e-05
Batch 17 Loss 1.7343987565254793e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00497606722638011
Batch 18 Loss 0.00497606722638011
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006864634808152914
Batch 19 Loss 0.006864634808152914
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009019400924444199
Batch 20 Loss 0.009019400924444199
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027293397579342127
Batch 21 Loss 0.0027293397579342127
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013246065936982632
Batch 22 Loss 0.0013246065936982632
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000905916967894882
Batch 23 Loss 0.000905916967894882
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062497882172465324
Batch 24 Loss 0.0062497882172465324
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 3.559718606993556e-06
Batch 25 Loss 3.559718606993556e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006367466878145933
Batch 26 Loss 0.006367466878145933
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005800219718366861
Batch 27 Loss 0.005800219718366861
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 9.726543794386089e-06
Batch 28 Loss 9.726543794386089e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0439027391839772e-05
Batch 29 Loss 1.0439027391839772e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032136414665728807
Batch 30 Loss 0.0032136414665728807
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006825598422437906
Batch 31 Loss 0.006825598422437906
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006406084634363651
Batch 32 Loss 0.006406084634363651
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011676161375362426
Batch 33 Loss 0.00011676161375362426
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0069029987789690495
Batch 34 Loss 0.0069029987789690495
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 5.889726526220329e-05
Batch 35 Loss 5.889726526220329e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6006924852263182e-05
Batch 36 Loss 1.6006924852263182e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034556412138044834
Batch 37 Loss 0.0034556412138044834
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00293013546615839
Batch 38 Loss 0.00293013546615839
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00012882892042398453
Batch 39 Loss 0.00012882892042398453
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7634658433962613e-05
Batch 40 Loss 1.7634658433962613e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00521421805024147
Batch 41 Loss 0.00521421805024147
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004845160059630871
Batch 42 Loss 0.004845160059630871
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 6.765520083718002e-06
Batch 43 Loss 6.765520083718002e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035330508835613728
Batch 44 Loss 0.0035330508835613728
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011013915063813329
Batch 45 Loss 0.0011013915063813329
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007058389019221067
Batch 46 Loss 0.007058389019221067
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005980118177831173
Batch 47 Loss 0.005980118177831173
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 6.573463906534016e-06
Batch 48 Loss 6.573463906534016e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009176208637654781
Batch 49 Loss 0.009176208637654781
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007147267926484346
Batch 50 Loss 0.007147267926484346
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006514484528452158
Batch 51 Loss 0.006514484528452158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005095264874398708
Batch 52 Loss 0.005095264874398708
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006164682563394308
Batch 53 Loss 0.006164682563394308
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0023716185241937637
Batch 54 Loss 0.0023716185241937637
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005664643831551075
Batch 55 Loss 0.005664643831551075
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 9.57550946623087e-06
Batch 56 Loss 9.57550946623087e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5335266652982682e-05
Batch 57 Loss 1.5335266652982682e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007191088050603867
Batch 58 Loss 0.007191088050603867
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008782665245234966
Batch 59 Loss 0.008782665245234966
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006857516244053841
Batch 60 Loss 0.006857516244053841
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005244534928351641
Batch 61 Loss 0.005244534928351641
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.015290308743715286
Batch 62 Loss 0.015290308743715286
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00021416631352622062
Batch 63 Loss 0.00021416631352622062
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00016481081547681242
Batch 64 Loss 0.00016481081547681242
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011301508493488654
Batch 65 Loss 0.00011301508493488654
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9581682863645256e-05
Batch 66 Loss 1.9581682863645256e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00706732040271163
Batch 67 Loss 0.00706732040271163
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 9.586314263287932e-06
Batch 68 Loss 9.586314263287932e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014689085946884006
Batch 69 Loss 0.00014689085946884006
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008633259683847427
Batch 70 Loss 0.008633259683847427
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007118868641555309
Batch 71 Loss 0.007118868641555309
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 1.54863009811379e-05
Batch 72 Loss 1.54863009811379e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009095252025872469
Batch 73 Loss 0.0009095252025872469
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002609715797007084
Batch 74 Loss 0.002609715797007084
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00615697680041194
Batch 75 Loss 0.00615697680041194
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 9.264629625249654e-06
Batch 76 Loss 9.264629625249654e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004353694152086973
Batch 77 Loss 0.004353694152086973
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 5.444664566311985e-06
Batch 78 Loss 5.444664566311985e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 4.269492637831718e-05
Batch 79 Loss 4.269492637831718e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00522320531308651
Batch 80 Loss 0.00522320531308651
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003203373169526458
Batch 81 Loss 0.003203373169526458
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007533521391451359
Batch 82 Loss 0.007533521391451359
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008680826053023338
Batch 83 Loss 0.008680826053023338
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 1.594324567122385e-05
Batch 84 Loss 1.594324567122385e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038755498826503754
Batch 85 Loss 0.0038755498826503754
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 4.722445737570524e-06
Batch 86 Loss 4.722445737570524e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009794656187295914
Batch 87 Loss 0.009794656187295914
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0072260634042322636
Batch 88 Loss 0.0072260634042322636
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011288977693766356
Batch 89 Loss 0.0011288977693766356
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004213419742882252
Batch 90 Loss 0.004213419742882252
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0778363503050059e-05
Batch 91 Loss 1.0778363503050059e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004324746318161488
Batch 92 Loss 0.004324746318161488
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007060799980536103
Batch 93 Loss 0.0007060799980536103
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011881018872372806
Batch 94 Loss 0.00011881018872372806
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8508224457036704e-05
Batch 95 Loss 1.8508224457036704e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 3.452347300481051e-06
Batch 96 Loss 3.452347300481051e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008499639108777046
Batch 97 Loss 0.008499639108777046
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00605428870767355
Batch 98 Loss 0.00605428870767355
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005290299188345671
Batch 99 Loss 0.005290299188345671
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003122432972304523
Batch 100 Loss 0.0003122432972304523
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021637517493218184
Batch 101 Loss 0.0021637517493218184
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017288037342950702
Batch 102 Loss 0.0017288037342950702
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007620179560035467
Batch 103 Loss 0.007620179560035467
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1224332158453763e-05
Batch 104 Loss 2.1224332158453763e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028008867520838976
Batch 105 Loss 0.0028008867520838976
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 9.39881065278314e-05
Batch 106 Loss 9.39881065278314e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059805880300700665
Batch 107 Loss 0.0059805880300700665
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032561568077653646
Batch 108 Loss 0.0032561568077653646
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008299466222524643
Batch 109 Loss 0.008299466222524643
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 6.558086897712201e-05
Batch 110 Loss 6.558086897712201e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 3.5339908208698034e-05
Batch 111 Loss 3.5339908208698034e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0053885080851614475
Batch 112 Loss 0.0053885080851614475
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 3.77632059098687e-05
Batch 113 Loss 3.77632059098687e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 6...

Epoch: 6 Train loss: 1.1995049135293812e-05, 0.1% complete
..........
Epoch: 6 Train loss: 0.005614308174699545, 1.2% complete
..........
Epoch: 6 Train loss: 0.0036674565635621548, 2.3% complete
..........
Epoch: 6 Train loss: 0.003996273968368769, 3.4% complete
..........
Epoch: 6 Train loss: 0.008703012950718403, 4.4% complete
..........
Epoch: 6 Train loss: 0.0039056965615600348, 5.5% complete
..........
Epoch: 6 Train loss: 0.0014336236054077744, 6.6% complete
..........
Epoch: 6 Train loss: 0.007999800145626068, 7.7% complete
..........
Epoch: 6 Train loss: 0.007143659982830286, 8.8% complete
..........
Epoch: 6 Train loss: 6.0416205087676644e-06, 9.9% complete
..........
Epoch: 6 Train loss: 2.983164449688047e-06, 11.0% complete
..........
Epoch: 6 Train loss: 0.009742342866957188, 12.0% complete
..........
Epoch: 6 Train loss: 1.7957354430109262e-06, 13.1% complete
..........
Epoch: 6 Train loss: 0.005920680705457926, 14.2% complete
..........
Epoch: 6 Train loss: 0.007143957540392876, 15.3% complete
..........
Epoch: 6 Train loss: 0.0007893273141235113, 16.4% complete
..........
Epoch: 6 Train loss: 0.006196475587785244, 17.5% complete
..........
Epoch: 6 Train loss: 0.0031002468895167112, 18.5% complete
..........
Epoch: 6 Train loss: 0.003123681293800473, 19.6% complete
..........
Epoch: 6 Train loss: 0.01316334493458271, 20.7% complete
..........
Epoch: 6 Train loss: 0.010413005016744137, 21.8% complete
..........
Epoch: 6 Train loss: 0.0019512245198711753, 22.9% complete
..........
Epoch: 6 Train loss: 8.370421710424125e-06, 24.0% complete
..........
Epoch: 6 Train loss: 0.006369461305439472, 25.1% complete
..........
Epoch: 6 Train loss: 0.0019642310217022896, 26.1% complete
..........
Epoch: 6 Train loss: 1.1801494110841304e-05, 27.2% complete
..........
Epoch: 6 Train loss: 7.415910658892244e-06, 28.3% complete
..........
Epoch: 6 Train loss: 0.006524455733597279, 29.4% complete
..........
Epoch: 6 Train loss: 0.008804788812994957, 30.5% complete
..........
Epoch: 6 Train loss: 0.00019968139531556517, 31.6% complete
..........
Epoch: 6 Train loss: 2.9735383577644825e-06, 32.6% complete
..........
Epoch: 6 Train loss: 0.007300719618797302, 33.7% complete
..........
Epoch: 6 Train loss: 0.004799036309123039, 34.8% complete
..........
Epoch: 6 Train loss: 3.0222581699490547e-06, 35.9% complete
..........
Epoch: 6 Train loss: 1.0273128282278776e-05, 37.0% complete
..........
Epoch: 6 Train loss: 0.0018396516097709537, 38.1% complete
..........
Epoch: 6 Train loss: 8.480521501041949e-06, 39.2% complete
..........
Epoch: 6 Train loss: 0.0034805447794497013, 40.2% complete
..........
Epoch: 6 Train loss: 0.00368996593169868, 41.3% complete
..........
Epoch: 6 Train loss: 3.899149305652827e-06, 42.4% complete
..........
Epoch: 6 Train loss: 0.007197854574769735, 43.5% complete
..........
Epoch: 6 Train loss: 0.004651930183172226, 44.6% complete
..........
Epoch: 6 Train loss: 0.004309024661779404, 45.7% complete
..........
Epoch: 6 Train loss: 0.005863616243004799, 46.7% complete
..........
Epoch: 6 Train loss: 0.00306447665207088, 47.8% complete
..........
Epoch: 6 Train loss: 0.004026791546493769, 48.9% complete
..........
Epoch: 6 Train loss: 0.007231974974274635, 50.0% complete
..........
Epoch: 6 Train loss: 0.011792358011007309, 51.1% complete
..........
Epoch: 6 Train loss: 0.006504626013338566, 52.2% complete
..........
Epoch: 6 Train loss: 4.116518539376557e-06, 53.3% complete
..........
Epoch: 6 Train loss: 6.154172297101468e-06, 54.3% complete
..........
Epoch: 6 Train loss: 0.004992549307644367, 55.4% complete
..........
Epoch: 6 Train loss: 1.807994703995064e-05, 56.5% complete
..........
Epoch: 6 Train loss: 3.145076334476471e-06, 57.6% complete
..........
Epoch: 6 Train loss: 0.0032874851021915674, 58.7% complete
..........
Epoch: 6 Train loss: 0.002945833606645465, 59.8% complete
..........
Epoch: 6 Train loss: 2.0127627067267895e-06, 60.8% complete
..........
Epoch: 6 Train loss: 0.0060281772166490555, 61.9% complete
..........
Epoch: 6 Train loss: 0.00811699964106083, 63.0% complete
..........
Epoch: 6 Train loss: 0.005132246762514114, 64.1% complete
..........
Epoch: 6 Train loss: 4.409856774145737e-05, 65.2% complete
..........
Epoch: 6 Train loss: 0.010320544242858887, 66.3% complete
..........
Epoch: 6 Train loss: 3.132437268504873e-05, 67.4% complete
..........
Epoch: 6 Train loss: 0.00018538562289904803, 68.4% complete
..........
Epoch: 6 Train loss: 1.6226527804974467e-05, 69.5% complete
..........
Epoch: 6 Train loss: 0.00013905369269195944, 70.6% complete
..........
Epoch: 6 Train loss: 0.0007163329282775521, 71.7% complete
..........
Epoch: 6 Train loss: 0.0063143540173769, 72.8% complete
..........
Epoch: 6 Train loss: 0.011266403831541538, 73.9% complete
..........
Epoch: 6 Train loss: 0.009687700308859348, 74.9% complete
..........
Epoch: 6 Train loss: 0.006666683591902256, 76.0% complete
..........
Epoch: 6 Train loss: 0.0036357289645820856, 77.1% complete
..........
Epoch: 6 Train loss: 0.002372555434703827, 78.2% complete
..........
Epoch: 6 Train loss: 8.31557554192841e-06, 79.3% complete
..........
Epoch: 6 Train loss: 0.003382490249350667, 80.4% complete
..........
Epoch: 6 Train loss: 4.6082568587735295e-06, 81.5% complete
..........
Epoch: 6 Train loss: 0.012051748111844063, 82.5% complete
..........
Epoch: 6 Train loss: 0.0035461443476378918, 83.6% complete
..........
Epoch: 6 Train loss: 0.0038959162775427103, 84.7% complete
..........
Epoch: 6 Train loss: 0.006525799166411161, 85.8% complete
..........
Epoch: 6 Train loss: 0.006939141545444727, 86.9% complete
..........
Epoch: 6 Train loss: 0.002050264971330762, 88.0% complete
..........
Epoch: 6 Train loss: 2.466847945470363e-06, 89.0% complete
..........
Epoch: 6 Train loss: 0.012481166049838066, 90.1% complete
..........
Epoch: 6 Train loss: 0.0046156300231814384, 91.2% complete
..........
Epoch: 6 Train loss: 0.006773307453840971, 92.3% complete
..........
Epoch: 6 Train loss: 0.006856866646558046, 93.4% complete
..........
Epoch: 6 Train loss: 0.0019801047164946795, 94.5% complete
..........
Epoch: 6 Train loss: 0.007679278962314129, 95.6% complete
..........
Epoch: 6 Train loss: 0.006032654084265232, 96.6% complete
..........
Epoch: 6 Train loss: 0.011125982739031315, 97.7% complete
..........
Epoch: 6 Train loss: 0.008242963813245296, 98.8% complete
..........
Epoch: 6 Train loss: 0.0024533697869628668, 99.9% complete
..
Training complete
Validating epoch 6...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000984867918305099
Batch 0 Loss 0.000984867918305099
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014965294394642115
Batch 1 Loss 0.0014965294394642115
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3423559721559286e-05
Batch 2 Loss 1.3423559721559286e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4222761567216367e-05
Batch 3 Loss 2.4222761567216367e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007278379052877426
Batch 4 Loss 0.007278379052877426
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002528561744838953
Batch 5 Loss 0.002528561744838953
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005544929765164852
Batch 6 Loss 0.005544929765164852
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006941062398254871
Batch 7 Loss 0.006941062398254871
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004359276965260506
Batch 8 Loss 0.004359276965260506
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004213064385112375
Batch 9 Loss 0.0004213064385112375
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6201447579078376e-05
Batch 10 Loss 1.6201447579078376e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4784771085251123e-05
Batch 11 Loss 2.4784771085251123e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006417777854949236
Batch 12 Loss 0.006417777854949236
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004608575254678726
Batch 13 Loss 0.004608575254678726
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007314507849514484
Batch 14 Loss 0.007314507849514484
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006107689812779427
Batch 15 Loss 0.006107689812779427
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6412537661381066e-05
Batch 16 Loss 1.6412537661381066e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0060338727198541164
Batch 17 Loss 0.0060338727198541164
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005879747215658426
Batch 18 Loss 0.005879747215658426
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0045798160135746
Batch 19 Loss 0.0045798160135746
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032528673764318228
Batch 20 Loss 0.0032528673764318228
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037601925432682037
Batch 21 Loss 0.0037601925432682037
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 6.074973498471081e-06
Batch 22 Loss 6.074973498471081e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3834436433389783e-05
Batch 23 Loss 2.3834436433389783e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004418309777975082
Batch 24 Loss 0.004418309777975082
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0040604351088404655
Batch 25 Loss 0.0040604351088404655
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005674450192600489
Batch 26 Loss 0.005674450192600489
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003916306421160698
Batch 27 Loss 0.003916306421160698
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025369522627443075
Batch 28 Loss 0.0025369522627443075
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4011009372770786e-05
Batch 29 Loss 2.4011009372770786e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00494022574275732
Batch 30 Loss 0.00494022574275732
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038995284121483564
Batch 31 Loss 0.0038995284121483564
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4362452677451074e-05
Batch 32 Loss 2.4362452677451074e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008994867093861103
Batch 33 Loss 0.008994867093861103
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028250431641936302
Batch 34 Loss 0.0028250431641936302
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009138434194028378
Batch 35 Loss 0.009138434194028378
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011087472550570965
Batch 36 Loss 0.011087472550570965
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005558907054364681
Batch 37 Loss 0.005558907054364681
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004450706765055656
Batch 38 Loss 0.004450706765055656
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 5.3904514061287045e-06
Batch 39 Loss 5.3904514061287045e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 9.075090929400176e-06
Batch 40 Loss 9.075090929400176e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 5.155314283911139e-06
Batch 41 Loss 5.155314283911139e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00012512723333202302
Batch 42 Loss 0.00012512723333202302
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 9.528215741738677e-06
Batch 43 Loss 9.528215741738677e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007376033812761307
Batch 44 Loss 0.007376033812761307
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005949471145868301
Batch 45 Loss 0.005949471145868301
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 1.733699900796637e-05
Batch 46 Loss 1.733699900796637e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004744303412735462
Batch 47 Loss 0.004744303412735462
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013998912181705236
Batch 48 Loss 0.0013998912181705236
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0054316651076078415
Batch 49 Loss 0.0054316651076078415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005668529774993658
Batch 50 Loss 0.0005668529774993658
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0825569916050881e-05
Batch 51 Loss 1.0825569916050881e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037443838082253933
Batch 52 Loss 0.0037443838082253933
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 8.657283615320921e-06
Batch 53 Loss 8.657283615320921e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035303079057484865
Batch 54 Loss 0.0035303079057484865
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007547407411038876
Batch 55 Loss 0.007547407411038876
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4591634680982679e-05
Batch 56 Loss 1.4591634680982679e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0063797906041145325
Batch 57 Loss 0.0063797906041145325
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000624063890427351
Batch 58 Loss 0.000624063890427351
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006415286101400852
Batch 59 Loss 0.006415286101400852
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005805916618555784
Batch 60 Loss 0.005805916618555784
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005000485107302666
Batch 61 Loss 0.005000485107302666
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 5.937690002610907e-05
Batch 62 Loss 5.937690002610907e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035561786498874426
Batch 63 Loss 0.0035561786498874426
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3661836166866124e-05
Batch 64 Loss 2.3661836166866124e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00753843504935503
Batch 65 Loss 0.00753843504935503
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 3.6184428608976305e-05
Batch 66 Loss 3.6184428608976305e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028319181874394417
Batch 67 Loss 0.0028319181874394417
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 6.631759606534615e-05
Batch 68 Loss 6.631759606534615e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032722270116209984
Batch 69 Loss 0.0032722270116209984
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005964796524494886
Batch 70 Loss 0.005964796524494886
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6264999178238213e-05
Batch 71 Loss 2.6264999178238213e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 9.176968887913972e-06
Batch 72 Loss 9.176968887913972e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 8.83464963408187e-06
Batch 73 Loss 8.83464963408187e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003045465564355254
Batch 74 Loss 0.003045465564355254
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003916820045560598
Batch 75 Loss 0.003916820045560598
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 6.092930561862886e-06
Batch 76 Loss 6.092930561862886e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0052518947049975395
Batch 77 Loss 0.0052518947049975395
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007526382803916931
Batch 78 Loss 0.007526382803916931
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0063049402087926865
Batch 79 Loss 0.0063049402087926865
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005373888649046421
Batch 80 Loss 0.005373888649046421
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0066972216591238976
Batch 81 Loss 0.0066972216591238976
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006774652749300003
Batch 82 Loss 0.006774652749300003
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006677443161606789
Batch 83 Loss 0.006677443161606789
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010123729705810547
Batch 84 Loss 0.010123729705810547
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003082990413531661
Batch 85 Loss 0.003082990413531661
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 9.209739801008254e-06
Batch 86 Loss 9.209739801008254e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 6.64398685330525e-05
Batch 87 Loss 6.64398685330525e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007676406763494015
Batch 88 Loss 0.007676406763494015
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003297075454611331
Batch 89 Loss 0.0003297075454611331
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004295638762414455
Batch 90 Loss 0.004295638762414455
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0077768792398273945
Batch 91 Loss 0.0077768792398273945
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0039097038097679615
Batch 92 Loss 0.0039097038097679615
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037560968194156885
Batch 93 Loss 0.0037560968194156885
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055232360027730465
Batch 94 Loss 0.0055232360027730465
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006313066463917494
Batch 95 Loss 0.006313066463917494
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00518577266484499
Batch 96 Loss 0.00518577266484499
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006895121652632952
Batch 97 Loss 0.006895121652632952
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0056581818498671055
Batch 98 Loss 0.0056581818498671055
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007667135214433074
Batch 99 Loss 0.0007667135214433074
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 5.124718154547736e-05
Batch 100 Loss 5.124718154547736e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 2.009344461839646e-05
Batch 101 Loss 2.009344461839646e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3301035980693996e-05
Batch 102 Loss 2.3301035980693996e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014119475963525474
Batch 103 Loss 0.00014119475963525474
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0045815035700798035
Batch 104 Loss 0.0045815035700798035
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055945860221982
Batch 105 Loss 0.0055945860221982
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00545892957597971
Batch 106 Loss 0.00545892957597971
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 5.296271410770714e-06
Batch 107 Loss 5.296271410770714e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001220380305312574
Batch 108 Loss 0.001220380305312574
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00034926520311273634
Batch 109 Loss 0.00034926520311273634
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006493740947917104
Batch 110 Loss 0.0006493740947917104
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1093667126260698e-05
Batch 111 Loss 1.1093667126260698e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004655466414988041
Batch 112 Loss 0.004655466414988041
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 2.9137001547496766e-05
Batch 113 Loss 2.9137001547496766e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 7...

Epoch: 7 Train loss: 3.87278814741876e-05, 0.1% complete
..........
Epoch: 7 Train loss: 0.009523795917630196, 1.2% complete
..........
Epoch: 7 Train loss: 0.007954049855470657, 2.3% complete
..........
Epoch: 7 Train loss: 6.367779860738665e-06, 3.4% complete
..........
Epoch: 7 Train loss: 0.004835212603211403, 4.4% complete
..........
Epoch: 7 Train loss: 0.007822476327419281, 5.5% complete
..........
Epoch: 7 Train loss: 7.02770848874934e-05, 6.6% complete
..........
Epoch: 7 Train loss: 0.00012470968067646027, 7.7% complete
..........
Epoch: 7 Train loss: 0.014502186328172684, 8.8% complete
..........
Epoch: 7 Train loss: 0.005606754217296839, 9.9% complete
..........
Epoch: 7 Train loss: 0.003565908409655094, 11.0% complete
..........
Epoch: 7 Train loss: 0.005243146792054176, 12.0% complete
..........
Epoch: 7 Train loss: 0.0035898045171052217, 13.1% complete
..........
Epoch: 7 Train loss: 0.006689276080578566, 14.2% complete
..........
Epoch: 7 Train loss: 6.011163350194693e-06, 15.3% complete
..........
Epoch: 7 Train loss: 0.0074919359758496284, 16.4% complete
..........
Epoch: 7 Train loss: 0.003799254074692726, 17.5% complete
..........
Epoch: 7 Train loss: 0.007089363876730204, 18.5% complete
..........
Epoch: 7 Train loss: 6.187983672134578e-06, 19.6% complete
..........
Epoch: 7 Train loss: 0.0051974160596728325, 20.7% complete
..........
Epoch: 7 Train loss: 0.005831316579133272, 21.8% complete
..........
Epoch: 7 Train loss: 0.00017376086907461286, 22.9% complete
..........
Epoch: 7 Train loss: 0.00566621171310544, 24.0% complete
..........
Epoch: 7 Train loss: 0.00630258908495307, 25.1% complete
..........
Epoch: 7 Train loss: 0.005574323702603579, 26.1% complete
..........
Epoch: 7 Train loss: 3.8130117900436744e-05, 27.2% complete
..........
Epoch: 7 Train loss: 0.0008965349406935275, 28.3% complete
..........
Epoch: 7 Train loss: 0.006271681282669306, 29.4% complete
..........
Epoch: 7 Train loss: 0.004850612487643957, 30.5% complete
..........
Epoch: 7 Train loss: 0.00012005900498479605, 31.6% complete
..........
Epoch: 7 Train loss: 0.008466299623250961, 32.6% complete
..........
Epoch: 7 Train loss: 0.0072616348043084145, 33.7% complete
..........
Epoch: 7 Train loss: 0.007285082247108221, 34.8% complete
..........
Epoch: 7 Train loss: 9.0835674200207e-06, 35.9% complete
..........
Epoch: 7 Train loss: 0.006259397137910128, 37.0% complete
..........
Epoch: 7 Train loss: 1.043895463226363e-05, 38.1% complete
..........
Epoch: 7 Train loss: 4.49167491751723e-05, 39.2% complete
..........
Epoch: 7 Train loss: 0.00886549986898899, 40.2% complete
..........
Epoch: 7 Train loss: 0.001030905987136066, 41.3% complete
..........
Epoch: 7 Train loss: 0.007730547804385424, 42.4% complete
..........
Epoch: 7 Train loss: 0.00021363636187743396, 43.5% complete
..........
Epoch: 7 Train loss: 0.00039743108209222555, 44.6% complete
..........
Epoch: 7 Train loss: 0.004549344070255756, 45.7% complete
..........
Epoch: 7 Train loss: 0.004785500932484865, 46.7% complete
..........
Epoch: 7 Train loss: 1.4429024304263294e-06, 47.8% complete
..........
Epoch: 7 Train loss: 0.007479588035494089, 48.9% complete
..........
Epoch: 7 Train loss: 5.6558026699349284e-05, 50.0% complete
..........
Epoch: 7 Train loss: 0.008559753187000751, 51.1% complete
..........
Epoch: 7 Train loss: 0.005620970390737057, 52.2% complete
..........
Epoch: 7 Train loss: 4.2956016841344535e-05, 53.3% complete
..........
Epoch: 7 Train loss: 0.00496357586234808, 54.3% complete
..........
Epoch: 7 Train loss: 0.005705529823899269, 55.4% complete
..........
Epoch: 7 Train loss: 1.0081566870212555e-06, 56.5% complete
..........
Epoch: 7 Train loss: 3.0052295187488198e-05, 57.6% complete
..........
Epoch: 7 Train loss: 4.222423740429804e-05, 58.7% complete
..........
Epoch: 7 Train loss: 0.00028790778014808893, 59.8% complete
..........
Epoch: 7 Train loss: 0.0089693833142519, 60.8% complete
..........
Epoch: 7 Train loss: 8.845672709867358e-07, 61.9% complete
..........
Epoch: 7 Train loss: 0.009691813960671425, 63.0% complete
..........
Epoch: 7 Train loss: 0.005136262159794569, 64.1% complete
..........
Epoch: 7 Train loss: 0.007949971593916416, 65.2% complete
..........
Epoch: 7 Train loss: 0.007192582357674837, 66.3% complete
..........
Epoch: 7 Train loss: 9.391127241542563e-05, 67.4% complete
..........
Epoch: 7 Train loss: 0.0035106162540614605, 68.4% complete
..........
Epoch: 7 Train loss: 2.543172740843147e-06, 69.5% complete
..........
Epoch: 7 Train loss: 0.006021579727530479, 70.6% complete
..........
Epoch: 7 Train loss: 1.6643025446683168e-06, 71.7% complete
..........
Epoch: 7 Train loss: 0.00012002921721432358, 72.8% complete
..........
Epoch: 7 Train loss: 0.008509614504873753, 73.9% complete
..........
Epoch: 7 Train loss: 2.162450982723385e-06, 74.9% complete
..........
Epoch: 7 Train loss: 0.006076550576835871, 76.0% complete
..........
Epoch: 7 Train loss: 9.14277188712731e-06, 77.1% complete
..........
Epoch: 7 Train loss: 0.005990246310830116, 78.2% complete
..........
Epoch: 7 Train loss: 0.00799040962010622, 79.3% complete
..........
Epoch: 7 Train loss: 0.006086124572902918, 80.4% complete
..........
Epoch: 7 Train loss: 0.003211288945749402, 81.5% complete
..........
Epoch: 7 Train loss: 0.0065237912349402905, 82.5% complete
..........
Epoch: 7 Train loss: 0.0008955882512964308, 83.6% complete
..........
Epoch: 7 Train loss: 0.0018805493600666523, 84.7% complete
..........
Epoch: 7 Train loss: 0.009488988667726517, 85.8% complete
..........
Epoch: 7 Train loss: 0.004300069995224476, 86.9% complete
..........
Epoch: 7 Train loss: 2.771179424598813e-06, 88.0% complete
..........
Epoch: 7 Train loss: 3.791510971495882e-05, 89.0% complete
..........
Epoch: 7 Train loss: 0.006488365586847067, 90.1% complete
..........
Epoch: 7 Train loss: 7.692047802265733e-05, 91.2% complete
..........
Epoch: 7 Train loss: 5.507026799023151e-07, 92.3% complete
..........
Epoch: 7 Train loss: 9.945797501131892e-07, 93.4% complete
..........
Epoch: 7 Train loss: 0.004173205699771643, 94.5% complete
..........
Epoch: 7 Train loss: 5.9727706684498116e-05, 95.6% complete
..........
Epoch: 7 Train loss: 0.0034191168379038572, 96.6% complete
..........
Epoch: 7 Train loss: 0.007682260125875473, 97.7% complete
..........
Epoch: 7 Train loss: 0.0011335017625242472, 98.8% complete
..........
Epoch: 7 Train loss: 0.0036949755158275366, 99.9% complete
..
Training complete
Validating epoch 7...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004837255924940109
Batch 0 Loss 0.004837255924940109
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0148069122806191e-06
Batch 1 Loss 1.0148069122806191e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1110608940944076e-06
Batch 2 Loss 2.1110608940944076e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006135588977485895
Batch 3 Loss 0.006135588977485895
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004646828398108482
Batch 4 Loss 0.004646828398108482
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006332991179078817
Batch 5 Loss 0.006332991179078817
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004441705998033285
Batch 6 Loss 0.004441705998033285
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007172379642724991
Batch 7 Loss 0.007172379642724991
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005483875283971429
Batch 8 Loss 0.0005483875283971429
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006055120378732681
Batch 9 Loss 0.006055120378732681
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021686938125640154
Batch 10 Loss 0.0021686938125640154
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030190821271389723
Batch 11 Loss 0.0030190821271389723
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 3.827641558018513e-05
Batch 12 Loss 3.827641558018513e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9878207240253687e-06
Batch 13 Loss 1.9878207240253687e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0068494114093482494
Batch 14 Loss 0.0068494114093482494
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00830844696611166
Batch 15 Loss 0.00830844696611166
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000936774886213243
Batch 16 Loss 0.000936774886213243
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 8.093108044704422e-05
Batch 17 Loss 8.093108044704422e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005199612583965063
Batch 18 Loss 0.005199612583965063
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 9.16323479032144e-05
Batch 19 Loss 9.16323479032144e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005406372714787722
Batch 20 Loss 0.005406372714787722
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004405495710670948
Batch 21 Loss 0.004405495710670948
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 3.137945896014571e-06
Batch 22 Loss 3.137945896014571e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004638115409761667
Batch 23 Loss 0.004638115409761667
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055617038160562515
Batch 24 Loss 0.0055617038160562515
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004160859156399965
Batch 25 Loss 0.004160859156399965
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027539816219359636
Batch 26 Loss 0.0027539816219359636
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007142925169318914
Batch 27 Loss 0.007142925169318914
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 5.492702257470228e-05
Batch 28 Loss 5.492702257470228e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005066582467406988
Batch 29 Loss 0.005066582467406988
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0022892432752996683
Batch 30 Loss 0.0022892432752996683
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003993644379079342
Batch 31 Loss 0.003993644379079342
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005494771990925074
Batch 32 Loss 0.005494771990925074
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00654313201084733
Batch 33 Loss 0.00654313201084733
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007656753994524479
Batch 34 Loss 0.007656753994524479
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 1.146129216067493e-05
Batch 35 Loss 1.146129216067493e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 3.8021375075913966e-05
Batch 36 Loss 3.8021375075913966e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006296230014413595
Batch 37 Loss 0.006296230014413595
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005330377724021673
Batch 38 Loss 0.005330377724021673
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00582717452198267
Batch 39 Loss 0.00582717452198267
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029905035626143217
Batch 40 Loss 0.0029905035626143217
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059215836226940155
Batch 41 Loss 0.0059215836226940155
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1716437408467755e-05
Batch 42 Loss 4.1716437408467755e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016430113464593887
Batch 43 Loss 0.0016430113464593887
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0057279206812381744
Batch 44 Loss 0.0057279206812381744
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 7.826238288544118e-06
Batch 45 Loss 7.826238288544118e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018690185388550162
Batch 46 Loss 0.0018690185388550162
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004750552121549845
Batch 47 Loss 0.004750552121549845
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 4.6841378207318485e-06
Batch 48 Loss 4.6841378207318485e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007329372689127922
Batch 49 Loss 0.007329372689127922
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00482386676594615
Batch 50 Loss 0.00482386676594615
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004076123295817524
Batch 51 Loss 0.0004076123295817524
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007691581733524799
Batch 52 Loss 0.007691581733524799
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007797513157129288
Batch 53 Loss 0.007797513157129288
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009302255930379033
Batch 54 Loss 0.0009302255930379033
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004024549387395382
Batch 55 Loss 0.004024549387395382
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 7.531194569310173e-05
Batch 56 Loss 7.531194569310173e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4869227015879005e-05
Batch 57 Loss 1.4869227015879005e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059408340603113174
Batch 58 Loss 0.0059408340603113174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00010655762889655307
Batch 59 Loss 0.00010655762889655307
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021162936463952065
Batch 60 Loss 0.0021162936463952065
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002855574246495962
Batch 61 Loss 0.002855574246495962
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 3.328575985506177e-06
Batch 62 Loss 3.328575985506177e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 4.4701526348944753e-05
Batch 63 Loss 4.4701526348944753e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0043715620413422585
Batch 64 Loss 0.0043715620413422585
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007604589685797691
Batch 65 Loss 0.007604589685797691
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00631871959194541
Batch 66 Loss 0.00631871959194541
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006007875315845013
Batch 67 Loss 0.006007875315845013
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004787283018231392
Batch 68 Loss 0.004787283018231392
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004475475288927555
Batch 69 Loss 0.004475475288927555
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008560310117900372
Batch 70 Loss 0.008560310117900372
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010400747880339622
Batch 71 Loss 0.010400747880339622
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 1.613138010725379e-06
Batch 72 Loss 1.613138010725379e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004479920491576195
Batch 73 Loss 0.004479920491576195
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007821949198842049
Batch 74 Loss 0.007821949198842049
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00016524759121239185
Batch 75 Loss 0.00016524759121239185
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 1.279321440961212e-05
Batch 76 Loss 1.279321440961212e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0039197481237351894
Batch 77 Loss 0.0039197481237351894
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 8.13683436717838e-06
Batch 78 Loss 8.13683436717838e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005314679816365242
Batch 79 Loss 0.005314679816365242
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3362099707592279e-05
Batch 80 Loss 1.3362099707592279e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001633764710277319
Batch 81 Loss 0.001633764710277319
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 7.5369025580585e-06
Batch 82 Loss 7.5369025580585e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006088394206017256
Batch 83 Loss 0.006088394206017256
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0057537248358130455
Batch 84 Loss 0.0057537248358130455
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006470808759331703
Batch 85 Loss 0.006470808759331703
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006384832318872213
Batch 86 Loss 0.006384832318872213
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0075573306530714035
Batch 87 Loss 0.0075573306530714035
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010185722261667252
Batch 88 Loss 0.010185722261667252
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009800470434129238
Batch 89 Loss 0.009800470434129238
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004224690608680248
Batch 90 Loss 0.004224690608680248
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006158627104014158
Batch 91 Loss 0.006158627104014158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0060794539749622345
Batch 92 Loss 0.0060794539749622345
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 6.315116479527205e-06
Batch 93 Loss 6.315116479527205e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007141335401684046
Batch 94 Loss 0.007141335401684046
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006376627832651138
Batch 95 Loss 0.006376627832651138
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 3.5398261388763785e-06
Batch 96 Loss 3.5398261388763785e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00745648005977273
Batch 97 Loss 0.00745648005977273
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 5.6358694564551115e-06
Batch 98 Loss 5.6358694564551115e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005721948109567165
Batch 99 Loss 0.005721948109567165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00244231428951025
Batch 100 Loss 0.00244231428951025
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005762099754065275
Batch 101 Loss 0.005762099754065275
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004574972670525312
Batch 102 Loss 0.004574972670525312
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006574200466275215
Batch 103 Loss 0.006574200466275215
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 8.622737368568778e-07
Batch 104 Loss 8.622737368568778e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001235246192663908
Batch 105 Loss 0.001235246192663908
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5482368073426187e-06
Batch 106 Loss 2.5482368073426187e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011240161955356598
Batch 107 Loss 0.011240161955356598
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 7.821916369721293e-06
Batch 108 Loss 7.821916369721293e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005120613146573305
Batch 109 Loss 0.005120613146573305
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00048749556299299
Batch 110 Loss 0.00048749556299299
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005002135410904884
Batch 111 Loss 0.005002135410904884
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00796579197049141
Batch 112 Loss 0.00796579197049141
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004360071383416653
Batch 113 Loss 0.004360071383416653
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 8...

Epoch: 8 Train loss: 0.005743626970797777, 0.1% complete
..........
Epoch: 8 Train loss: 0.005508397240191698, 1.2% complete
..........
Epoch: 8 Train loss: 0.006677836645394564, 2.3% complete
..........
Epoch: 8 Train loss: 1.6782039892859757e-05, 3.4% complete
..........
Epoch: 8 Train loss: 0.005251806695014238, 4.4% complete
..........
Epoch: 8 Train loss: 0.0003015163238160312, 5.5% complete
..........
Epoch: 8 Train loss: 0.004811996128410101, 6.6% complete
..........
Epoch: 8 Train loss: 0.010903595015406609, 7.7% complete
..........
Epoch: 8 Train loss: 0.00018836742674466223, 8.8% complete
..........
Epoch: 8 Train loss: 9.866489563137293e-06, 9.9% complete
..........
Epoch: 8 Train loss: 1.4689576346427202e-06, 11.0% complete
..........
Epoch: 8 Train loss: 0.007549317087978125, 12.0% complete
..........
Epoch: 8 Train loss: 0.006015051156282425, 13.1% complete
..........
Epoch: 8 Train loss: 0.006731310859322548, 14.2% complete
..........
Epoch: 8 Train loss: 2.6712270482676104e-05, 15.3% complete
..........
Epoch: 8 Train loss: 0.0018357904627919197, 16.4% complete
..........
Epoch: 8 Train loss: 0.0031292501371353865, 17.5% complete
..........
Epoch: 8 Train loss: 0.005628904327750206, 18.5% complete
..........
Epoch: 8 Train loss: 0.002866367343813181, 19.6% complete
..........
Epoch: 8 Train loss: 0.004786064848303795, 20.7% complete
..........
Epoch: 8 Train loss: 0.006096644327044487, 21.8% complete
..........
Epoch: 8 Train loss: 9.736620995681733e-06, 22.9% complete
..........
Epoch: 8 Train loss: 0.003907854203134775, 24.0% complete
..........
Epoch: 8 Train loss: 0.0046477895230054855, 25.1% complete
..........
Epoch: 8 Train loss: 3.9246369851753116e-06, 26.1% complete
..........
Epoch: 8 Train loss: 1.1265365174040198e-06, 27.2% complete
..........
Epoch: 8 Train loss: 0.0006576038431376219, 28.3% complete
..........
Epoch: 8 Train loss: 0.008988549932837486, 29.4% complete
..........
Epoch: 8 Train loss: 0.0061301966197788715, 30.5% complete
..........
Epoch: 8 Train loss: 0.005095267202705145, 31.6% complete
..........
Epoch: 8 Train loss: 0.005844695959240198, 32.6% complete
..........
Epoch: 8 Train loss: 0.005566692445427179, 33.7% complete
..........
Epoch: 8 Train loss: 0.0021833356004208326, 34.8% complete
..........
Epoch: 8 Train loss: 0.010741650126874447, 35.9% complete
..........
Epoch: 8 Train loss: 0.00640506949275732, 37.0% complete
..........
Epoch: 8 Train loss: 0.012309038080275059, 38.1% complete
..........
Epoch: 8 Train loss: 0.003849310101941228, 39.2% complete
..........
Epoch: 8 Train loss: 0.007212653290480375, 40.2% complete
..........
Epoch: 8 Train loss: 3.3923712180694565e-05, 41.3% complete
..........
Epoch: 8 Train loss: 0.004624687600880861, 42.4% complete
..........
Epoch: 8 Train loss: 0.0034721950069069862, 43.5% complete
..........
Epoch: 8 Train loss: 0.004206629935652018, 44.6% complete
..........
Epoch: 8 Train loss: 6.5532076405361295e-06, 45.7% complete
..........
Epoch: 8 Train loss: 7.6556701969821e-05, 46.7% complete
..........
Epoch: 8 Train loss: 0.004126803018152714, 47.8% complete
..........
Epoch: 8 Train loss: 0.004757503047585487, 48.9% complete
..........
Epoch: 8 Train loss: 1.9300277926959097e-06, 50.0% complete
..........
Epoch: 8 Train loss: 7.965965778566897e-06, 51.1% complete
..........
Epoch: 8 Train loss: 0.00737778190523386, 52.2% complete
..........
Epoch: 8 Train loss: 0.0007190231117419899, 53.3% complete
..........
Epoch: 8 Train loss: 0.0009153333958238363, 54.3% complete
..........
Epoch: 8 Train loss: 0.0009475979604758322, 55.4% complete
..........
Epoch: 8 Train loss: 0.0049450150690972805, 56.5% complete
..........
Epoch: 8 Train loss: 0.007102765142917633, 57.6% complete
..........
Epoch: 8 Train loss: 0.0018541308818385005, 58.7% complete
..........
Epoch: 8 Train loss: 0.010621585883200169, 59.8% complete
..........
Epoch: 8 Train loss: 3.73198781744577e-05, 60.8% complete
..........
Epoch: 8 Train loss: 4.81082679471001e-06, 61.9% complete
..........
Epoch: 8 Train loss: 0.003098746994510293, 63.0% complete
..........
Epoch: 8 Train loss: 0.004548375029116869, 64.1% complete
..........
Epoch: 8 Train loss: 0.002999925520271063, 65.2% complete
..........
Epoch: 8 Train loss: 0.0068389722146093845, 66.3% complete
..........
Epoch: 8 Train loss: 3.77126743842382e-05, 67.4% complete
..........
Epoch: 8 Train loss: 3.8111102185212076e-06, 68.4% complete
..........
Epoch: 8 Train loss: 0.012679781764745712, 69.5% complete
..........
Epoch: 8 Train loss: 7.529743015766144e-07, 70.6% complete
..........
Epoch: 8 Train loss: 0.0034428148064762354, 71.7% complete
..........
Epoch: 8 Train loss: 1.8228747649118304e-06, 72.8% complete
..........
Epoch: 8 Train loss: 0.009228588081896305, 73.9% complete
..........
Epoch: 8 Train loss: 0.007587892469018698, 74.9% complete
..........
Epoch: 8 Train loss: 0.00805841013789177, 76.0% complete
..........
Epoch: 8 Train loss: 0.0017414732137694955, 77.1% complete
..........
Epoch: 8 Train loss: 0.007005228661000729, 78.2% complete
..........
Epoch: 8 Train loss: 0.004095852840691805, 79.3% complete
..........
Epoch: 8 Train loss: 0.0070936912670731544, 80.4% complete
..........
Epoch: 8 Train loss: 0.0007064210949465632, 81.5% complete
..........
Epoch: 8 Train loss: 6.755799404345453e-06, 82.5% complete
..........
Epoch: 8 Train loss: 0.008626182563602924, 83.6% complete
..........
Epoch: 8 Train loss: 0.004588474053889513, 84.7% complete
..........
Epoch: 8 Train loss: 0.007050294894725084, 85.8% complete
..........
Epoch: 8 Train loss: 0.0021992020774632692, 86.9% complete
..........
Epoch: 8 Train loss: 0.005041095893830061, 88.0% complete
..........
Epoch: 8 Train loss: 1.259788405150175e-06, 89.0% complete
..........
Epoch: 8 Train loss: 0.0006068805232644081, 90.1% complete
..........
Epoch: 8 Train loss: 0.00428151385858655, 91.2% complete
..........
Epoch: 8 Train loss: 1.491280272603035e-06, 92.3% complete
..........
Epoch: 8 Train loss: 0.0022914495784789324, 93.4% complete
..........
Epoch: 8 Train loss: 0.0028294511139392853, 94.5% complete
..........
Epoch: 8 Train loss: 2.940321428468451e-05, 95.6% complete
..........
Epoch: 8 Train loss: 0.006970065180212259, 96.6% complete
..........
Epoch: 8 Train loss: 3.4025360946543515e-06, 97.7% complete
..........
Epoch: 8 Train loss: 0.010163535363972187, 98.8% complete
..........
Epoch: 8 Train loss: 1.3557342754211277e-05, 99.9% complete
..
Training complete
Validating epoch 8...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004500411916524172
Batch 0 Loss 0.004500411916524172
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006546125281602144
Batch 1 Loss 0.006546125281602144
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062324488535523415
Batch 2 Loss 0.0062324488535523415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 3.3552059903740883e-06
Batch 3 Loss 3.3552059903740883e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005925983190536499
Batch 4 Loss 0.005925983190536499
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9902290659956634e-06
Batch 5 Loss 1.9902290659956634e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00044472640729509294
Batch 6 Loss 0.00044472640729509294
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005025957943871617
Batch 7 Loss 0.0005025957943871617
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008113312534987926
Batch 8 Loss 0.008113312534987926
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00641191890463233
Batch 9 Loss 0.00641191890463233
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 2.521176793379709e-05
Batch 10 Loss 2.521176793379709e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001812223345041275
Batch 11 Loss 0.001812223345041275
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004560871981084347
Batch 12 Loss 0.004560871981084347
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 3.912835381925106e-06
Batch 13 Loss 3.912835381925106e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0054547348991036415
Batch 14 Loss 0.0054547348991036415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0068953922018408775
Batch 15 Loss 0.0068953922018408775
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005143518093973398
Batch 16 Loss 0.005143518093973398
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 2.263794158352539e-05
Batch 17 Loss 2.263794158352539e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 4.90332386107184e-05
Batch 18 Loss 4.90332386107184e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 8.771603461354971e-07
Batch 19 Loss 8.771603461354971e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 5.077185051050037e-05
Batch 20 Loss 5.077185051050037e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 9.383365977555513e-07
Batch 21 Loss 9.383365977555513e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004658678080886602
Batch 22 Loss 0.004658678080886602
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034571026917546988
Batch 23 Loss 0.0034571026917546988
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00528904190286994
Batch 24 Loss 0.00528904190286994
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004981654696166515
Batch 25 Loss 0.004981654696166515
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 3.705579729285091e-06
Batch 26 Loss 3.705579729285091e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001646808086661622
Batch 27 Loss 0.0001646808086661622
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005933617707341909
Batch 28 Loss 0.005933617707341909
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008884529583156109
Batch 29 Loss 0.008884529583156109
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062673562206327915
Batch 30 Loss 0.0062673562206327915
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005059781018644571
Batch 31 Loss 0.005059781018644571
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 4.937762423651293e-05
Batch 32 Loss 4.937762423651293e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011123870499432087
Batch 33 Loss 0.0011123870499432087
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00625565554946661
Batch 34 Loss 0.00625565554946661
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0190728062298149e-05
Batch 35 Loss 1.0190728062298149e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0069450861774384975
Batch 36 Loss 0.0069450861774384975
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027679982595145702
Batch 37 Loss 0.0027679982595145702
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00026714600971899927
Batch 38 Loss 0.00026714600971899927
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001687735435552895
Batch 39 Loss 0.001687735435552895
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007413752842694521
Batch 40 Loss 0.007413752842694521
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003122523659840226
Batch 41 Loss 0.003122523659840226
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016588965663686395
Batch 42 Loss 0.0016588965663686395
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2243988749105483e-05
Batch 43 Loss 1.2243988749105483e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009900093078613281
Batch 44 Loss 0.009900093078613281
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 2.116590621881187e-05
Batch 45 Loss 2.116590621881187e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007903204532340169
Batch 46 Loss 0.0007903204532340169
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004294115584343672
Batch 47 Loss 0.004294115584343672
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028336516115814447
Batch 48 Loss 0.0028336516115814447
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0162388207390904e-05
Batch 49 Loss 1.0162388207390904e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8701837689150125e-05
Batch 50 Loss 1.8701837689150125e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028676283545792103
Batch 51 Loss 0.0028676283545792103
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034438332077115774
Batch 52 Loss 0.0034438332077115774
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008063131012022495
Batch 53 Loss 0.008063131012022495
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008667338406667113
Batch 54 Loss 0.0008667338406667113
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 8.744252409087494e-05
Batch 55 Loss 8.744252409087494e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004541413858532906
Batch 56 Loss 0.004541413858532906
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011423650430515409
Batch 57 Loss 0.0011423650430515409
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00735330767929554
Batch 58 Loss 0.00735330767929554
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00018230642308481038
Batch 59 Loss 0.00018230642308481038
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 9.878436685539782e-06
Batch 60 Loss 9.878436685539782e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00519378250464797
Batch 61 Loss 0.00519378250464797
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036218012683093548
Batch 62 Loss 0.0036218012683093548
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0016343521419912577
Batch 63 Loss 0.0016343521419912577
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5278422981500626e-06
Batch 64 Loss 2.5278422981500626e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003312749322503805
Batch 65 Loss 0.003312749322503805
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006957776378840208
Batch 66 Loss 0.006957776378840208
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005795017350465059
Batch 67 Loss 0.005795017350465059
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005827334243804216
Batch 68 Loss 0.005827334243804216
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006857047323137522
Batch 69 Loss 0.006857047323137522
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0056058503687381744
Batch 70 Loss 0.0056058503687381744
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 5.7555444072932005e-06
Batch 71 Loss 5.7555444072932005e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 7.085487595759332e-06
Batch 72 Loss 7.085487595759332e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027640233747661114
Batch 73 Loss 0.0027640233747661114
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6092973712366074e-05
Batch 74 Loss 2.6092973712366074e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007453368045389652
Batch 75 Loss 0.007453368045389652
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1465788702480495e-05
Batch 76 Loss 1.1465788702480495e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004908428061753511
Batch 77 Loss 0.004908428061753511
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0023237806744873524
Batch 78 Loss 0.0023237806744873524
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007204339373856783
Batch 79 Loss 0.007204339373856783
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0050696684047579765
Batch 80 Loss 0.0050696684047579765
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005736374296247959
Batch 81 Loss 0.005736374296247959
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002899511717259884
Batch 82 Loss 0.002899511717259884
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0053962054662406445
Batch 83 Loss 0.0053962054662406445
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004858020227402449
Batch 84 Loss 0.004858020227402449
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00012780680845025927
Batch 85 Loss 0.00012780680845025927
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004422225523740053
Batch 86 Loss 0.004422225523740053
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9210114260204136e-05
Batch 87 Loss 1.9210114260204136e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002294764621183276
Batch 88 Loss 0.002294764621183276
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004792973631992936
Batch 89 Loss 0.0004792973631992936
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00859417300671339
Batch 90 Loss 0.00859417300671339
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002481210685800761
Batch 91 Loss 0.0002481210685800761
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034792013466358185
Batch 92 Loss 0.0034792013466358185
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000737774302251637
Batch 93 Loss 0.000737774302251637
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006906544789671898
Batch 94 Loss 0.006906544789671898
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037898123264312744
Batch 95 Loss 0.0037898123264312744
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001530034060124308
Batch 96 Loss 0.0001530034060124308
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005094320513308048
Batch 97 Loss 0.0005094320513308048
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006360155530273914
Batch 98 Loss 0.006360155530273914
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026571792550385
Batch 99 Loss 0.0026571792550385
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 1.124312257161364e-05
Batch 100 Loss 1.124312257161364e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 5.949709884589538e-05
Batch 101 Loss 5.949709884589538e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0050405231304466724
Batch 102 Loss 0.0050405231304466724
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006173640955239534
Batch 103 Loss 0.006173640955239534
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5302470021415502e-05
Batch 104 Loss 2.5302470021415502e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004648643080145121
Batch 105 Loss 0.004648643080145121
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 2.944514562841505e-06
Batch 106 Loss 2.944514562841505e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005509587004780769
Batch 107 Loss 0.005509587004780769
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004732844885438681
Batch 108 Loss 0.004732844885438681
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 6.061538806534372e-05
Batch 109 Loss 6.061538806534372e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6192527255043387e-05
Batch 110 Loss 1.6192527255043387e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005655888933688402
Batch 111 Loss 0.005655888933688402
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 1.614449138287455e-05
Batch 112 Loss 1.614449138287455e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004728676751255989
Batch 113 Loss 0.004728676751255989
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 9...

Epoch: 9 Train loss: 5.316658644005656e-06, 0.1% complete
..........
Epoch: 9 Train loss: 0.0030385199934244156, 1.2% complete
..........
Epoch: 9 Train loss: 3.595089219743386e-05, 2.3% complete
..........
Epoch: 9 Train loss: 0.001555716386064887, 3.4% complete
..........
Epoch: 9 Train loss: 0.0033216197043657303, 4.4% complete
..........
Epoch: 9 Train loss: 0.003865920938551426, 5.5% complete
..........
Epoch: 9 Train loss: 2.155022229999304e-06, 6.6% complete
..........
Epoch: 9 Train loss: 0.0008302938658744097, 7.7% complete
..........
Epoch: 9 Train loss: 0.005490267649292946, 8.8% complete
..........
Epoch: 9 Train loss: 0.00614605937153101, 9.9% complete
..........
Epoch: 9 Train loss: 0.0013284682063385844, 11.0% complete
..........
Epoch: 9 Train loss: 0.0039007305167615414, 12.0% complete
..........
Epoch: 9 Train loss: 0.010688135400414467, 13.1% complete
..........
Epoch: 9 Train loss: 0.008155614137649536, 14.2% complete
..........
Epoch: 9 Train loss: 2.7881360438186675e-05, 15.3% complete
..........
Epoch: 9 Train loss: 0.0009915080154314637, 16.4% complete
..........
Epoch: 9 Train loss: 3.980450856033713e-06, 17.5% complete
..........
Epoch: 9 Train loss: 0.006535533349961042, 18.5% complete
..........
Epoch: 9 Train loss: 0.0036884641740471125, 19.6% complete
..........
Epoch: 9 Train loss: 0.0003155526937916875, 20.7% complete
..........
Epoch: 9 Train loss: 0.003811040660366416, 21.8% complete
..........
Epoch: 9 Train loss: 0.004966484382748604, 22.9% complete
..........
Epoch: 9 Train loss: 0.00020087069424334913, 24.0% complete
..........
Epoch: 9 Train loss: 0.007570772897452116, 25.1% complete
..........
Epoch: 9 Train loss: 0.007056117989122868, 26.1% complete
..........
Epoch: 9 Train loss: 0.004912443924695253, 27.2% complete
..........
Epoch: 9 Train loss: 0.0030547413043677807, 28.3% complete
..........
Epoch: 9 Train loss: 0.001699614804238081, 29.4% complete
..........
Epoch: 9 Train loss: 0.002021952997893095, 30.5% complete
..........
Epoch: 9 Train loss: 0.0014072582125663757, 31.6% complete
..........
Epoch: 9 Train loss: 0.002358880126848817, 32.6% complete
..........
Epoch: 9 Train loss: 0.00544473621994257, 33.7% complete
..........
Epoch: 9 Train loss: 1.4819132047705352e-05, 34.8% complete
..........
Epoch: 9 Train loss: 9.710682206787169e-06, 35.9% complete
..........
Epoch: 9 Train loss: 0.008729162625968456, 37.0% complete
..........
Epoch: 9 Train loss: 0.006121369078755379, 38.1% complete
..........
Epoch: 9 Train loss: 0.004304330330342054, 39.2% complete
..........
Epoch: 9 Train loss: 0.007058793678879738, 40.2% complete
..........
Epoch: 9 Train loss: 9.142830822383985e-05, 41.3% complete
..........
Epoch: 9 Train loss: 0.0008192714885808527, 42.4% complete
..........
Epoch: 9 Train loss: 0.005663025658577681, 43.5% complete
..........
Epoch: 9 Train loss: 2.219050657004118e-06, 44.6% complete
..........
Epoch: 9 Train loss: 0.004512837156653404, 45.7% complete
..........
Epoch: 9 Train loss: 0.00489949993789196, 46.7% complete
..........
Epoch: 9 Train loss: 1.3221928384155035e-05, 47.8% complete
..........
Epoch: 9 Train loss: 2.0043444237671793e-06, 48.9% complete
..........
Epoch: 9 Train loss: 9.214418241754174e-07, 50.0% complete
..........
Epoch: 9 Train loss: 0.0064172642305493355, 51.1% complete
..........
Epoch: 9 Train loss: 0.007443886250257492, 52.2% complete
..........
Epoch: 9 Train loss: 0.0067981849424541, 53.3% complete
..........
Epoch: 9 Train loss: 0.00533079169690609, 54.3% complete
..........
Epoch: 9 Train loss: 8.06956086307764e-05, 55.4% complete
..........
Epoch: 9 Train loss: 9.509465598966926e-06, 56.5% complete
..........
Epoch: 9 Train loss: 0.0035553753841668367, 57.6% complete
..........
Epoch: 9 Train loss: 0.0006077707512304187, 58.7% complete
..........
Epoch: 9 Train loss: 1.2409291230142117e-06, 59.8% complete
..........
Epoch: 9 Train loss: 0.0038438281044363976, 60.8% complete
..........
Epoch: 9 Train loss: 0.006682980339974165, 61.9% complete
..........
Epoch: 9 Train loss: 1.267946936422959e-05, 63.0% complete
..........
Epoch: 9 Train loss: 0.005979783833026886, 64.1% complete
..........
Epoch: 9 Train loss: 0.0010046963579952717, 65.2% complete
..........
Epoch: 9 Train loss: 0.008424941450357437, 66.3% complete
..........
Epoch: 9 Train loss: 0.010555200278759003, 67.4% complete
..........
Epoch: 9 Train loss: 0.0021362584084272385, 68.4% complete
..........
Epoch: 9 Train loss: 0.009529469534754753, 69.5% complete
..........
Epoch: 9 Train loss: 0.008601037785410881, 70.6% complete
..........
Epoch: 9 Train loss: 0.00704876659438014, 71.7% complete
..........
Epoch: 9 Train loss: 6.754125934094191e-07, 72.8% complete
..........
Epoch: 9 Train loss: 0.008341463282704353, 73.9% complete
..........
Epoch: 9 Train loss: 0.00170053169131279, 74.9% complete
..........
Epoch: 9 Train loss: 0.005278387572616339, 76.0% complete
..........
Epoch: 9 Train loss: 0.005638962145894766, 77.1% complete
..........
Epoch: 9 Train loss: 0.006442841142416, 78.2% complete
..........
Epoch: 9 Train loss: 0.004401915706694126, 79.3% complete
..........
Epoch: 9 Train loss: 0.007260122802108526, 80.4% complete
..........
Epoch: 9 Train loss: 5.099776899442077e-05, 81.5% complete
..........
Epoch: 9 Train loss: 0.005875049624592066, 82.5% complete
..........
Epoch: 9 Train loss: 0.002159238327294588, 83.6% complete
..........
Epoch: 9 Train loss: 4.2295490857213736e-05, 84.7% complete
..........
Epoch: 9 Train loss: 9.894865797832608e-07, 85.8% complete
..........
Epoch: 9 Train loss: 0.007036293391138315, 86.9% complete
..........
Epoch: 9 Train loss: 0.005141349509358406, 88.0% complete
..........
Epoch: 9 Train loss: 0.0022568777203559875, 89.0% complete
..........
Epoch: 9 Train loss: 0.003898946801200509, 90.1% complete
..........
Epoch: 9 Train loss: 5.0655537052080035e-05, 91.2% complete
..........
Epoch: 9 Train loss: 0.0008276285952888429, 92.3% complete
..........
Epoch: 9 Train loss: 0.007666584104299545, 93.4% complete
..........
Epoch: 9 Train loss: 0.005308634601533413, 94.5% complete
..........
Epoch: 9 Train loss: 1.1986230674665421e-05, 95.6% complete
..........
Epoch: 9 Train loss: 0.008979992009699345, 96.6% complete
..........
Epoch: 9 Train loss: 0.003824841696768999, 97.7% complete
..........
Epoch: 9 Train loss: 0.00923171080648899, 98.8% complete
..........
Epoch: 9 Train loss: 8.289061952382326e-07, 99.9% complete
..
Training complete
Validating epoch 9...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 6.735353963449597e-07
Batch 0 Loss 6.735353963449597e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005662495270371437
Batch 1 Loss 0.005662495270371437
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004748431965708733
Batch 2 Loss 0.004748431965708733
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006599821615964174
Batch 3 Loss 0.006599821615964174
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004225151613354683
Batch 4 Loss 0.004225151613354683
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007989318110048771
Batch 5 Loss 0.007989318110048771
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007009732071310282
Batch 6 Loss 0.0007009732071310282
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0072280713357031345
Batch 7 Loss 0.0072280713357031345
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005866261664777994
Batch 8 Loss 0.005866261664777994
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005395770072937012
Batch 9 Loss 0.005395770072937012
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033952423837035894
Batch 10 Loss 0.0033952423837035894
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005333584267646074
Batch 11 Loss 0.005333584267646074
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 5.248148227110505e-07
Batch 12 Loss 5.248148227110505e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3633689377456903e-06
Batch 13 Loss 1.3633689377456903e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0049560838378965855
Batch 14 Loss 0.0049560838378965855
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 4.0358296246267855e-05
Batch 15 Loss 4.0358296246267855e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0780204320326447e-06
Batch 16 Loss 1.0780204320326447e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004715999588370323
Batch 17 Loss 0.004715999588370323
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006462784018367529
Batch 18 Loss 0.006462784018367529
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038917427882552147
Batch 19 Loss 0.0038917427882552147
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004339306615293026
Batch 20 Loss 0.004339306615293026
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005446219351142645
Batch 21 Loss 0.005446219351142645
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003545045852661133
Batch 22 Loss 0.003545045852661133
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 6.378052057698369e-05
Batch 23 Loss 6.378052057698369e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0076898192055523396
Batch 24 Loss 0.0076898192055523396
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 5.028945452068001e-06
Batch 25 Loss 5.028945452068001e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036338178906589746
Batch 26 Loss 0.0036338178906589746
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1506854207254946e-06
Batch 27 Loss 1.1506854207254946e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004282002802938223
Batch 28 Loss 0.004282002802938223
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006669878028333187
Batch 29 Loss 0.006669878028333187
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008577349595725536
Batch 30 Loss 0.008577349595725536
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031215425115078688
Batch 31 Loss 0.0031215425115078688
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0051101273857057095
Batch 32 Loss 0.0051101273857057095
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008697204291820526
Batch 33 Loss 0.008697204291820526
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 7.738126441836357e-07
Batch 34 Loss 7.738126441836357e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 7.625402213307098e-05
Batch 35 Loss 7.625402213307098e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004397096578031778
Batch 36 Loss 0.004397096578031778
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002452738583087921
Batch 37 Loss 0.002452738583087921
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00432088365778327
Batch 38 Loss 0.00432088365778327
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 1.210202754009515e-06
Batch 39 Loss 1.210202754009515e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00048223367775790393
Batch 40 Loss 0.00048223367775790393
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004102332517504692
Batch 41 Loss 0.004102332517504692
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004277849569916725
Batch 42 Loss 0.004277849569916725
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 2.615323319332674e-05
Batch 43 Loss 2.615323319332674e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003009046660736203
Batch 44 Loss 0.003009046660736203
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019175880588591099
Batch 45 Loss 0.0019175880588591099
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004790938924998045
Batch 46 Loss 0.004790938924998045
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 7.902599463704973e-06
Batch 47 Loss 7.902599463704973e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0046109408140182495
Batch 48 Loss 0.0046109408140182495
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 6.689224392175674e-07
Batch 49 Loss 6.689224392175674e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00488122645765543
Batch 50 Loss 0.00488122645765543
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0861418559215963e-05
Batch 51 Loss 1.0861418559215963e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 5.778267222922295e-06
Batch 52 Loss 5.778267222922295e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025648779701441526
Batch 53 Loss 0.0025648779701441526
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00534249609336257
Batch 54 Loss 0.00534249609336257
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007937203161418438
Batch 55 Loss 0.007937203161418438
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 6.866466719657183e-07
Batch 56 Loss 6.866466719657183e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0046534244902431965
Batch 57 Loss 0.0046534244902431965
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006853061728179455
Batch 58 Loss 0.006853061728179455
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9348124624229968e-05
Batch 59 Loss 1.9348124624229968e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00025491180713288486
Batch 60 Loss 0.00025491180713288486
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007422381080687046
Batch 61 Loss 0.007422381080687046
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 4.101835656911135e-06
Batch 62 Loss 4.101835656911135e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035136262886226177
Batch 63 Loss 0.0035136262886226177
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00513446144759655
Batch 64 Loss 0.00513446144759655
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035240536089986563
Batch 65 Loss 0.0035240536089986563
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001063611707650125
Batch 66 Loss 0.001063611707650125
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2850134226027876e-05
Batch 67 Loss 1.2850134226027876e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009041277691721916
Batch 68 Loss 0.009041277691721916
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00488212238997221
Batch 69 Loss 0.00488212238997221
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005886819679290056
Batch 70 Loss 0.005886819679290056
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8975715395063162e-05
Batch 71 Loss 2.8975715395063162e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003432482946664095
Batch 72 Loss 0.003432482946664095
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004991252208128572
Batch 73 Loss 0.0004991252208128572
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 3.915476554539055e-06
Batch 74 Loss 3.915476554539055e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 5.323381628841162e-07
Batch 75 Loss 5.323381628841162e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019877436570823193
Batch 76 Loss 0.0019877436570823193
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 7.735863619018346e-06
Batch 77 Loss 7.735863619018346e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8608774300664663e-06
Batch 78 Loss 2.8608774300664663e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 1.793446426745504e-05
Batch 79 Loss 1.793446426745504e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009540576720610261
Batch 80 Loss 0.0009540576720610261
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01217614859342575
Batch 81 Loss 0.01217614859342575
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005947170779109001
Batch 82 Loss 0.005947170779109001
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038816649466753006
Batch 83 Loss 0.0038816649466753006
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055160908959805965
Batch 84 Loss 0.0055160908959805965
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 1.212581992149353e-06
Batch 85 Loss 1.212581992149353e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005150689277797937
Batch 86 Loss 0.005150689277797937
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010010306723415852
Batch 87 Loss 0.0010010306723415852
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004981898237019777
Batch 88 Loss 0.004981898237019777
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006528030149638653
Batch 89 Loss 0.006528030149638653
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 8.870034798746929e-05
Batch 90 Loss 8.870034798746929e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004620707593858242
Batch 91 Loss 0.004620707593858242
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011763377115130424
Batch 92 Loss 0.0011763377115130424
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2128682758193463e-05
Batch 93 Loss 2.2128682758193463e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 9.559444151818752e-07
Batch 94 Loss 9.559444151818752e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 4.816436558030546e-06
Batch 95 Loss 4.816436558030546e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029100377578288317
Batch 96 Loss 0.0029100377578288317
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006408987566828728
Batch 97 Loss 0.006408987566828728
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 9.143441275227815e-06
Batch 98 Loss 9.143441275227815e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006381416693329811
Batch 99 Loss 0.006381416693329811
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 4.30070940637961e-06
Batch 100 Loss 4.30070940637961e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 2.856907667592168e-05
Batch 101 Loss 2.856907667592168e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003956887871026993
Batch 102 Loss 0.003956887871026993
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 3.4215845516882837e-06
Batch 103 Loss 3.4215845516882837e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 6.2599501688964665e-06
Batch 104 Loss 6.2599501688964665e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 8.684255590196699e-06
Batch 105 Loss 8.684255590196699e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004021250177174807
Batch 106 Loss 0.004021250177174807
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7663187463767827e-06
Batch 107 Loss 1.7663187463767827e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0039989035576581955
Batch 108 Loss 0.0039989035576581955
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6503327060490847e-05
Batch 109 Loss 1.6503327060490847e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008431022986769676
Batch 110 Loss 0.008431022986769676
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025585321709513664
Batch 111 Loss 0.0025585321709513664
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008912578923627734
Batch 112 Loss 0.0008912578923627734
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005824349820613861
Batch 113 Loss 0.005824349820613861
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 10...

Epoch: 10 Train loss: 0.002931749913841486, 0.1% complete
..........
Epoch: 10 Train loss: 0.006291699130088091, 1.2% complete
..........
Epoch: 10 Train loss: 5.2449890063144267e-05, 2.3% complete
..........
Epoch: 10 Train loss: 0.001574832247570157, 3.4% complete
..........
Epoch: 10 Train loss: 1.288150087930262e-06, 4.4% complete
..........
Epoch: 10 Train loss: 2.9082541004754603e-05, 5.5% complete
..........
Epoch: 10 Train loss: 0.0059605748392641544, 6.6% complete
..........
Epoch: 10 Train loss: 1.930638973135501e-06, 7.7% complete
..........
Epoch: 10 Train loss: 0.007154783699661493, 8.8% complete
..........
Epoch: 10 Train loss: 0.003034038469195366, 9.9% complete
..........
Epoch: 10 Train loss: 0.0050562298856675625, 11.0% complete
..........
Epoch: 10 Train loss: 2.6263718609698117e-05, 12.0% complete
..........
Epoch: 10 Train loss: 7.048074621707201e-07, 13.1% complete
..........
Epoch: 10 Train loss: 0.0019424294587224722, 14.2% complete
..........
Epoch: 10 Train loss: 0.004126863554120064, 15.3% complete
..........
Epoch: 10 Train loss: 0.008627498522400856, 16.4% complete
..........
Epoch: 10 Train loss: 0.005248892121016979, 17.5% complete
..........
Epoch: 10 Train loss: 5.663154297508299e-06, 18.5% complete
..........
Epoch: 10 Train loss: 0.00749985920265317, 19.6% complete
..........
Epoch: 10 Train loss: 1.5616824384778738e-06, 20.7% complete
..........
Epoch: 10 Train loss: 0.00016435103316325694, 21.8% complete
..........
Epoch: 10 Train loss: 0.002531057223677635, 22.9% complete
..........
Epoch: 10 Train loss: 0.008406191132962704, 24.0% complete
..........
Epoch: 10 Train loss: 0.00839981622993946, 25.1% complete
..........
Epoch: 10 Train loss: 1.5596160665154457e-06, 26.1% complete
..........
Epoch: 10 Train loss: 1.362230977974832e-05, 27.2% complete
..........
Epoch: 10 Train loss: 0.00515367928892374, 28.3% complete
..........
Epoch: 10 Train loss: 0.005969380494207144, 29.4% complete
..........
Epoch: 10 Train loss: 7.847279630368575e-05, 30.5% complete
..........
Epoch: 10 Train loss: 0.0016203525010496378, 31.6% complete
..........
Epoch: 10 Train loss: 0.0018547301879152656, 32.6% complete
..........
Epoch: 10 Train loss: 2.979359123855829e-07, 33.7% complete
..........
Epoch: 10 Train loss: 8.925764996092767e-05, 34.8% complete
..........
Epoch: 10 Train loss: 4.2120638681808487e-05, 35.9% complete
..........
Epoch: 10 Train loss: 0.008490405045449734, 37.0% complete
..........
Epoch: 10 Train loss: 0.00645452318713069, 38.1% complete
..........
Epoch: 10 Train loss: 0.003790427464991808, 39.2% complete
..........
Epoch: 10 Train loss: 0.00026101592811755836, 40.2% complete
..........
Epoch: 10 Train loss: 5.137844709679484e-07, 41.3% complete
..........
Epoch: 10 Train loss: 0.004686778876930475, 42.4% complete
..........
Epoch: 10 Train loss: 2.082007995340973e-06, 43.5% complete
..........
Epoch: 10 Train loss: 1.8826336599886417e-05, 44.6% complete
..........
Epoch: 10 Train loss: 4.018074832856655e-07, 45.7% complete
..........
Epoch: 10 Train loss: 0.006116412580013275, 46.7% complete
..........
Epoch: 10 Train loss: 5.078266985947266e-05, 47.8% complete
..........
Epoch: 10 Train loss: 4.140783858019859e-06, 48.9% complete
..........
Epoch: 10 Train loss: 4.094560426892713e-05, 50.0% complete
..........
Epoch: 10 Train loss: 0.007218269165605307, 51.1% complete
..........
Epoch: 10 Train loss: 0.004702382255345583, 52.2% complete
..........
Epoch: 10 Train loss: 0.00044060806976631284, 53.3% complete
..........
Epoch: 10 Train loss: 0.0009510915842838585, 54.3% complete
..........
Epoch: 10 Train loss: 0.006059536710381508, 55.4% complete
..........
Epoch: 10 Train loss: 1.2219148629810661e-05, 56.5% complete
..........
Epoch: 10 Train loss: 0.005418888293206692, 57.6% complete
..........
Epoch: 10 Train loss: 0.004239138215780258, 58.7% complete
..........
Epoch: 10 Train loss: 0.0016239059623330832, 59.8% complete
..........
Epoch: 10 Train loss: 0.003563244827091694, 60.8% complete
..........
Epoch: 10 Train loss: 0.0029344381764531136, 61.9% complete
..........
Epoch: 10 Train loss: 5.931215127930045e-06, 63.0% complete
..........
Epoch: 10 Train loss: 7.756341074127704e-05, 64.1% complete
..........
Epoch: 10 Train loss: 0.004927156958729029, 65.2% complete
..........
Epoch: 10 Train loss: 0.0041588060557842255, 66.3% complete
..........
Epoch: 10 Train loss: 0.0047757504507899284, 67.4% complete
..........
Epoch: 10 Train loss: 1.2124604836571962e-05, 68.4% complete
..........
Epoch: 10 Train loss: 9.72296402323991e-06, 69.5% complete
..........
Epoch: 10 Train loss: 3.0625960789620876e-07, 70.6% complete
..........
Epoch: 10 Train loss: 3.461819142103195e-05, 71.7% complete
..........
Epoch: 10 Train loss: 0.003092230763286352, 72.8% complete
..........
Epoch: 10 Train loss: 1.002030330710113e-05, 73.9% complete
..........
Epoch: 10 Train loss: 1.7621168808545917e-05, 74.9% complete
..........
Epoch: 10 Train loss: 0.005746734794229269, 76.0% complete
..........
Epoch: 10 Train loss: 0.0020353510044515133, 77.1% complete
..........
Epoch: 10 Train loss: 3.0618684832006693e-07, 78.2% complete
..........
Epoch: 10 Train loss: 0.006234092637896538, 79.3% complete
..........
Epoch: 10 Train loss: 0.0004738018033094704, 80.4% complete
..........
Epoch: 10 Train loss: 0.006805725861340761, 81.5% complete
..........
Epoch: 10 Train loss: 2.730957930907607e-06, 82.5% complete
..........
Epoch: 10 Train loss: 0.0017383744707331061, 83.6% complete
..........
Epoch: 10 Train loss: 0.005522516090422869, 84.7% complete
..........
Epoch: 10 Train loss: 0.0017136489041149616, 85.8% complete
..........
Epoch: 10 Train loss: 0.004880009684711695, 86.9% complete
..........
Epoch: 10 Train loss: 0.004174171946942806, 88.0% complete
..........
Epoch: 10 Train loss: 0.001255082432180643, 89.0% complete
..........
Epoch: 10 Train loss: 0.005650169216096401, 90.1% complete
..........
Epoch: 10 Train loss: 0.005697176326066256, 91.2% complete
..........
Epoch: 10 Train loss: 1.5216784959193319e-05, 92.3% complete
..........
Epoch: 10 Train loss: 0.005075580440461636, 93.4% complete
..........
Epoch: 10 Train loss: 0.0035355938598513603, 94.5% complete
..........
Epoch: 10 Train loss: 0.0059914179146289825, 95.6% complete
..........
Epoch: 10 Train loss: 0.003951700869947672, 96.6% complete
..........
Epoch: 10 Train loss: 0.003045495133846998, 97.7% complete
..........
Epoch: 10 Train loss: 0.0024102693423628807, 98.8% complete
..........
Epoch: 10 Train loss: 3.984918294008821e-06, 99.9% complete
..
Training complete
Validating epoch 10...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006234111730009317
Batch 0 Loss 0.006234111730009317
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067923348397016525
Batch 1 Loss 0.0067923348397016525
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010377582162618637
Batch 2 Loss 0.010377582162618637
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 4.048765549669042e-05
Batch 3 Loss 4.048765549669042e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002758439222816378
Batch 4 Loss 0.0002758439222816378
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005684487521648407
Batch 5 Loss 0.005684487521648407
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006673138588666916
Batch 6 Loss 0.006673138588666916
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005893551278859377
Batch 7 Loss 0.005893551278859377
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 2.836502972058952e-06
Batch 8 Loss 2.836502972058952e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001718251034617424
Batch 9 Loss 0.001718251034617424
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005217107478529215
Batch 10 Loss 0.005217107478529215
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006431773770600557
Batch 11 Loss 0.006431773770600557
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00320973782800138
Batch 12 Loss 0.00320973782800138
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012822344433516264
Batch 13 Loss 0.0012822344433516264
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006641570944339037
Batch 14 Loss 0.006641570944339037
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006232268642634153
Batch 15 Loss 0.006232268642634153
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003119755128864199
Batch 16 Loss 0.0003119755128864199
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6699341358616948e-06
Batch 17 Loss 1.6699341358616948e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 3.588210529414937e-05
Batch 18 Loss 3.588210529414937e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3428001441061497e-06
Batch 19 Loss 2.3428001441061497e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 4.830209945794195e-06
Batch 20 Loss 4.830209945794195e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 5.208508810028434e-06
Batch 21 Loss 5.208508810028434e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004782475996762514
Batch 22 Loss 0.004782475996762514
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007133819162845612
Batch 23 Loss 0.007133819162845612
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026789663825184107
Batch 24 Loss 0.0026789663825184107
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 3.39256803272292e-06
Batch 25 Loss 3.39256803272292e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 6.9316738517954946e-06
Batch 26 Loss 6.9316738517954946e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008478626608848572
Batch 27 Loss 0.008478626608848572
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007310963235795498
Batch 28 Loss 0.007310963235795498
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6405247151851654e-06
Batch 29 Loss 1.6405247151851654e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021220259368419647
Batch 30 Loss 0.0021220259368419647
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005584972910583019
Batch 31 Loss 0.005584972910583019
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009126550517976284
Batch 32 Loss 0.009126550517976284
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6806483219843358e-05
Batch 33 Loss 2.6806483219843358e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005030751693993807
Batch 34 Loss 0.005030751693993807
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003987874835729599
Batch 35 Loss 0.003987874835729599
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011094594083260745
Batch 36 Loss 0.00011094594083260745
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011262203333899379
Batch 37 Loss 0.0011262203333899379
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 3.4763070289045572e-06
Batch 38 Loss 3.4763070289045572e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 7.348107465077192e-05
Batch 39 Loss 7.348107465077192e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004669929388910532
Batch 40 Loss 0.004669929388910532
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005041975062340498
Batch 41 Loss 0.005041975062340498
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 6.674563337583095e-06
Batch 42 Loss 6.674563337583095e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005268879234790802
Batch 43 Loss 0.005268879234790802
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005434550344944
Batch 44 Loss 0.005434550344944
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 2.957734977826476e-06
Batch 45 Loss 2.957734977826476e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002849843353033066
Batch 46 Loss 0.002849843353033066
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 4.001431807409972e-05
Batch 47 Loss 4.001431807409972e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 1.340718154096976e-05
Batch 48 Loss 1.340718154096976e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0024933198001235723
Batch 49 Loss 0.0024933198001235723
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010506108403205872
Batch 50 Loss 0.010506108403205872
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00013416168803814799
Batch 51 Loss 0.00013416168803814799
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 6.1313301557675e-05
Batch 52 Loss 6.1313301557675e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031929458491504192
Batch 53 Loss 0.0031929458491504192
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00656004948541522
Batch 54 Loss 0.00656004948541522
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00544916745275259
Batch 55 Loss 0.00544916745275259
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005841151811182499
Batch 56 Loss 0.005841151811182499
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4888762962073088e-05
Batch 57 Loss 1.4888762962073088e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007271885871887207
Batch 58 Loss 0.007271885871887207
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 6.350746843963861e-07
Batch 59 Loss 6.350746843963861e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 3.585300873965025e-06
Batch 60 Loss 3.585300873965025e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019116526236757636
Batch 61 Loss 0.0019116526236757636
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 3.0294562748167664e-05
Batch 62 Loss 3.0294562748167664e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005096915178000927
Batch 63 Loss 0.005096915178000927
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00241445517167449
Batch 64 Loss 0.00241445517167449
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006837337277829647
Batch 65 Loss 0.006837337277829647
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067194788716733456
Batch 66 Loss 0.0067194788716733456
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1996307875961065e-06
Batch 67 Loss 1.1996307875961065e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 2.7809146558865905e-05
Batch 68 Loss 2.7809146558865905e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004484807141125202
Batch 69 Loss 0.004484807141125202
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004472709260880947
Batch 70 Loss 0.004472709260880947
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0066543519496917725
Batch 71 Loss 0.0066543519496917725
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001273641100851819
Batch 72 Loss 0.0001273641100851819
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 4.124063707422465e-06
Batch 73 Loss 4.124063707422465e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002048100810497999
Batch 74 Loss 0.002048100810497999
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00441106129437685
Batch 75 Loss 0.00441106129437685
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004885408096015453
Batch 76 Loss 0.004885408096015453
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004753015469759703
Batch 77 Loss 0.004753015469759703
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006342013832181692
Batch 78 Loss 0.006342013832181692
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 6.0045713325962424e-06
Batch 79 Loss 6.0045713325962424e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004704305436462164
Batch 80 Loss 0.004704305436462164
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004731439519673586
Batch 81 Loss 0.004731439519673586
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007657207082957029
Batch 82 Loss 0.007657207082957029
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00599051034078002
Batch 83 Loss 0.00599051034078002
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004954904317855835
Batch 84 Loss 0.004954904317855835
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006295622792094946
Batch 85 Loss 0.006295622792094946
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004973517265170813
Batch 86 Loss 0.004973517265170813
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 7.077469490468502e-07
Batch 87 Loss 7.077469490468502e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00639032106846571
Batch 88 Loss 0.00639032106846571
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 3.9244230720214546e-05
Batch 89 Loss 3.9244230720214546e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003481514984741807
Batch 90 Loss 0.003481514984741807
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 8.768154657445848e-06
Batch 91 Loss 8.768154657445848e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033280947245657444
Batch 92 Loss 0.0033280947245657444
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 5.4818097851239145e-05
Batch 93 Loss 5.4818097851239145e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8736467356793582e-06
Batch 94 Loss 2.8736467356793582e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007775185164064169
Batch 95 Loss 0.007775185164064169
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013970951549708843
Batch 96 Loss 0.0013970951549708843
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003810174996033311
Batch 97 Loss 0.003810174996033311
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8368187486194074e-05
Batch 98 Loss 2.8368187486194074e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 1.425672962795943e-06
Batch 99 Loss 1.425672962795943e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004869935568422079
Batch 100 Loss 0.004869935568422079
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 4.5220847823657095e-05
Batch 101 Loss 4.5220847823657095e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0047836932353675365
Batch 102 Loss 0.0047836932353675365
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007235810160636902
Batch 103 Loss 0.007235810160636902
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01092332974076271
Batch 104 Loss 0.01092332974076271
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00024343194672837853
Batch 105 Loss 0.00024343194672837853
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007267541717737913
Batch 106 Loss 0.007267541717737913
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008775150403380394
Batch 107 Loss 0.008775150403380394
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 6.177352042868733e-05
Batch 108 Loss 6.177352042868733e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005206319969147444
Batch 109 Loss 0.005206319969147444
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 6.066678906790912e-06
Batch 110 Loss 6.066678906790912e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005476515274494886
Batch 111 Loss 0.005476515274494886
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 3.6421988625079393e-07
Batch 112 Loss 3.6421988625079393e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001104055845644325
Batch 113 Loss 0.0001104055845644325
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 11...

Epoch: 11 Train loss: 0.00028353690868243575, 0.1% complete
..........
Epoch: 11 Train loss: 0.007022871170192957, 1.2% complete
..........
Epoch: 11 Train loss: 0.006301683373749256, 2.3% complete
..........
Epoch: 11 Train loss: 6.042522727511823e-06, 3.4% complete
..........
Epoch: 11 Train loss: 5.546316970139742e-07, 4.4% complete
..........
Epoch: 11 Train loss: 4.6021614252822474e-05, 5.5% complete
..........
Epoch: 11 Train loss: 0.010246746242046356, 6.6% complete
..........
Epoch: 11 Train loss: 0.004677197430282831, 7.7% complete
..........
Epoch: 11 Train loss: 0.0046279155649244785, 8.8% complete
..........
Epoch: 11 Train loss: 0.007038322277367115, 9.9% complete
..........
Epoch: 11 Train loss: 0.005985979922115803, 11.0% complete
..........
Epoch: 11 Train loss: 0.009270966053009033, 12.0% complete
..........
Epoch: 11 Train loss: 8.151764632202685e-06, 13.1% complete
..........
Epoch: 11 Train loss: 0.0032210422214120626, 14.2% complete
..........
Epoch: 11 Train loss: 0.0031372373923659325, 15.3% complete
..........
Epoch: 11 Train loss: 6.4651467255316675e-06, 16.4% complete
..........
Epoch: 11 Train loss: 0.01577659323811531, 17.5% complete
..........
Epoch: 11 Train loss: 0.0011319697368890047, 18.5% complete
..........
Epoch: 11 Train loss: 0.0057931095361709595, 19.6% complete
..........
Epoch: 11 Train loss: 0.004064440727233887, 20.7% complete
..........
Epoch: 11 Train loss: 6.793015927542001e-06, 21.8% complete
..........
Epoch: 11 Train loss: 0.003988501150161028, 22.9% complete
..........
Epoch: 11 Train loss: 5.730813427362591e-05, 24.0% complete
..........
Epoch: 11 Train loss: 0.005011470522731543, 25.1% complete
..........
Epoch: 11 Train loss: 0.0038208719342947006, 26.1% complete
..........
Epoch: 11 Train loss: 0.0003674246254377067, 27.2% complete
..........
Epoch: 11 Train loss: 0.003900831565260887, 28.3% complete
..........
Epoch: 11 Train loss: 2.222386683570221e-05, 29.4% complete
..........
Epoch: 11 Train loss: 1.9742823496926576e-05, 30.5% complete
..........
Epoch: 11 Train loss: 0.005207871086895466, 31.6% complete
..........
Epoch: 11 Train loss: 3.624969394877553e-06, 32.6% complete
..........
Epoch: 11 Train loss: 0.00921175628900528, 33.7% complete
..........
Epoch: 11 Train loss: 0.002676581498235464, 34.8% complete
..........
Epoch: 11 Train loss: 0.0028509499970823526, 35.9% complete
..........
Epoch: 11 Train loss: 0.00029973717755638063, 37.0% complete
..........
Epoch: 11 Train loss: 0.001310122199356556, 38.1% complete
..........
Epoch: 11 Train loss: 0.0008742916397750378, 39.2% complete
..........
Epoch: 11 Train loss: 0.007273796945810318, 40.2% complete
..........
Epoch: 11 Train loss: 0.0072592259384691715, 41.3% complete
..........
Epoch: 11 Train loss: 0.005829615518450737, 42.4% complete
..........
Epoch: 11 Train loss: 0.00016901727940421551, 43.5% complete
..........
Epoch: 11 Train loss: 0.00990941934287548, 44.6% complete
..........
Epoch: 11 Train loss: 0.0039055899251252413, 45.7% complete
..........
Epoch: 11 Train loss: 0.005134815350174904, 46.7% complete
..........
Epoch: 11 Train loss: 0.00474441098049283, 47.8% complete
..........
Epoch: 11 Train loss: 2.6900452212430537e-06, 48.9% complete
..........
Epoch: 11 Train loss: 0.0026205824688076973, 50.0% complete
..........
Epoch: 11 Train loss: 2.9419381462503225e-05, 51.1% complete
..........
Epoch: 11 Train loss: 0.005482801236212254, 52.2% complete
..........
Epoch: 11 Train loss: 0.004537357948720455, 53.3% complete
..........
Epoch: 11 Train loss: 0.005618169903755188, 54.3% complete
..........
Epoch: 11 Train loss: 0.0014619984431192279, 55.4% complete
..........
Epoch: 11 Train loss: 1.7558777472004294e-06, 56.5% complete
..........
Epoch: 11 Train loss: 0.00011582201113924384, 57.6% complete
..........
Epoch: 11 Train loss: 0.004261896014213562, 58.7% complete
..........
Epoch: 11 Train loss: 3.860739525407553e-06, 59.8% complete
..........
Epoch: 11 Train loss: 0.00746170012280345, 60.8% complete
..........
Epoch: 11 Train loss: 0.0019528401317074895, 61.9% complete
..........
Epoch: 11 Train loss: 0.0016766723711043596, 63.0% complete
..........
Epoch: 11 Train loss: 0.003954403568059206, 64.1% complete
..........
Epoch: 11 Train loss: 0.004919739905744791, 65.2% complete
..........
Epoch: 11 Train loss: 0.005087488796561956, 66.3% complete
..........
Epoch: 11 Train loss: 0.0041368016973137856, 67.4% complete
..........
Epoch: 11 Train loss: 0.005797344259917736, 68.4% complete
..........
Epoch: 11 Train loss: 0.0033520166762173176, 69.5% complete
..........
Epoch: 11 Train loss: 0.00417129322886467, 70.6% complete
..........
Epoch: 11 Train loss: 0.001315200119279325, 71.7% complete
..........
Epoch: 11 Train loss: 5.713423888664693e-06, 72.8% complete
..........
Epoch: 11 Train loss: 3.364708391018212e-06, 73.9% complete
..........
Epoch: 11 Train loss: 0.0031812728848308325, 74.9% complete
..........
Epoch: 11 Train loss: 0.005374566651880741, 76.0% complete
..........
Epoch: 11 Train loss: 0.004740578122437, 77.1% complete
..........
Epoch: 11 Train loss: 0.00014371656288858503, 78.2% complete
..........
Epoch: 11 Train loss: 0.0049651917070150375, 79.3% complete
..........
Epoch: 11 Train loss: 5.701440386474133e-07, 80.4% complete
..........
Epoch: 11 Train loss: 4.977773642167449e-07, 81.5% complete
..........
Epoch: 11 Train loss: 0.007838993333280087, 82.5% complete
..........
Epoch: 11 Train loss: 0.007571789436042309, 83.6% complete
..........
Epoch: 11 Train loss: 0.005022313911467791, 84.7% complete
..........
Epoch: 11 Train loss: 0.0024908531922847033, 85.8% complete
..........
Epoch: 11 Train loss: 0.005956186912953854, 86.9% complete
..........
Epoch: 11 Train loss: 0.0005642546457238495, 88.0% complete
..........
Epoch: 11 Train loss: 0.004304700996726751, 89.0% complete
..........
Epoch: 11 Train loss: 0.00025959376944229007, 90.1% complete
..........
Epoch: 11 Train loss: 0.004010136239230633, 91.2% complete
..........
Epoch: 11 Train loss: 0.009000997059047222, 92.3% complete
..........
Epoch: 11 Train loss: 0.004214208573102951, 93.4% complete
..........
Epoch: 11 Train loss: 0.006042944733053446, 94.5% complete
..........
Epoch: 11 Train loss: 0.003876769682392478, 95.6% complete
..........
Epoch: 11 Train loss: 2.2090061975177377e-05, 96.6% complete
..........
Epoch: 11 Train loss: 1.233602233696729e-06, 97.7% complete
..........
Epoch: 11 Train loss: 8.75089899636805e-05, 98.8% complete
..........
Epoch: 11 Train loss: 4.6052497054915875e-05, 99.9% complete
..
Training complete
Validating epoch 11...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006318396423012018
Batch 0 Loss 0.006318396423012018
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033164573833346367
Batch 1 Loss 0.0033164573833346367
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006072860676795244
Batch 2 Loss 0.006072860676795244
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6611818864475936e-05
Batch 3 Loss 1.6611818864475936e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059922742657363415
Batch 4 Loss 0.0059922742657363415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 9.05991328181699e-06
Batch 5 Loss 9.05991328181699e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006568851415067911
Batch 6 Loss 0.006568851415067911
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006549745798110962
Batch 7 Loss 0.006549745798110962
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 4.3172462028451264e-05
Batch 8 Loss 4.3172462028451264e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007664526347070932
Batch 9 Loss 0.007664526347070932
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 7.929818821139634e-06
Batch 10 Loss 7.929818821139634e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 2.649561793077737e-06
Batch 11 Loss 2.649561793077737e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004753835499286652
Batch 12 Loss 0.004753835499286652
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2692908057942986e-06
Batch 13 Loss 1.2692908057942986e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013318350538611412
Batch 14 Loss 0.0013318350538611412
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005996081978082657
Batch 15 Loss 0.005996081978082657
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036504636518657207
Batch 16 Loss 0.0036504636518657207
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6915000742301345e-06
Batch 17 Loss 1.6915000742301345e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005555988289415836
Batch 18 Loss 0.005555988289415836
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0080031643155962e-05
Batch 19 Loss 1.0080031643155962e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004187051206827164
Batch 20 Loss 0.004187051206827164
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003634474240243435
Batch 21 Loss 0.003634474240243435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 9.802370914258063e-05
Batch 22 Loss 9.802370914258063e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002847470808774233
Batch 23 Loss 0.002847470808774233
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 4.334578989073634e-07
Batch 24 Loss 4.334578989073634e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002470875158905983
Batch 25 Loss 0.002470875158905983
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 6.776885129511356e-06
Batch 26 Loss 6.776885129511356e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8085785743314773e-05
Batch 27 Loss 2.8085785743314773e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 3.367007593624294e-06
Batch 28 Loss 3.367007593624294e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004466994199901819
Batch 29 Loss 0.004466994199901819
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 9.152427082881331e-07
Batch 30 Loss 9.152427082881331e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004488806705921888
Batch 31 Loss 0.004488806705921888
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000245211529545486
Batch 32 Loss 0.000245211529545486
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029357171151787043
Batch 33 Loss 0.0029357171151787043
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01021305751055479
Batch 34 Loss 0.01021305751055479
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0052122394554317
Batch 35 Loss 0.0052122394554317
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0787007340695709e-05
Batch 36 Loss 1.0787007340695709e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2709646145813167e-05
Batch 37 Loss 2.2709646145813167e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006776735186576843
Batch 38 Loss 0.006776735186576843
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 1.503140083514154e-06
Batch 39 Loss 1.503140083514154e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011099616531282663
Batch 40 Loss 0.0011099616531282663
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005036441143602133
Batch 41 Loss 0.0005036441143602133
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 4.205904406262562e-05
Batch 42 Loss 4.205904406262562e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00670553557574749
Batch 43 Loss 0.00670553557574749
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 3.795976226683706e-06
Batch 44 Loss 3.795976226683706e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0510848369449377e-06
Batch 45 Loss 1.0510848369449377e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0046094567514956
Batch 46 Loss 0.0046094567514956
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002145166043192148
Batch 47 Loss 0.002145166043192148
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007533449679613113
Batch 48 Loss 0.007533449679613113
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005522763356566429
Batch 49 Loss 0.005522763356566429
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007317481096833944
Batch 50 Loss 0.007317481096833944
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001444443129003048
Batch 51 Loss 0.001444443129003048
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004197423812001944
Batch 52 Loss 0.004197423812001944
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003117011161521077
Batch 53 Loss 0.003117011161521077
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014002594398334622
Batch 54 Loss 0.0014002594398334622
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0052275871858000755
Batch 55 Loss 0.0052275871858000755
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006885535549372435
Batch 56 Loss 0.006885535549372435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005139428190886974
Batch 57 Loss 0.005139428190886974
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005379491951316595
Batch 58 Loss 0.005379491951316595
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7684869817458093e-06
Batch 59 Loss 1.7684869817458093e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 2.391563612036407e-06
Batch 60 Loss 2.391563612036407e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029482985846698284
Batch 61 Loss 0.0029482985846698284
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028306026943027973
Batch 62 Loss 0.0028306026943027973
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004416156094521284
Batch 63 Loss 0.004416156094521284
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 7.744580216240138e-06
Batch 64 Loss 7.744580216240138e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007545535918325186
Batch 65 Loss 0.007545535918325186
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008368993178009987
Batch 66 Loss 0.008368993178009987
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0022782725282013416
Batch 67 Loss 0.0022782725282013416
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006763750221580267
Batch 68 Loss 0.006763750221580267
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1438033804297447e-07
Batch 69 Loss 4.1438033804297447e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004524302203208208
Batch 70 Loss 0.004524302203208208
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006305480841547251
Batch 71 Loss 0.006305480841547251
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031183967366814613
Batch 72 Loss 0.0031183967366814613
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 2.0814259187318385e-06
Batch 73 Loss 2.0814259187318385e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005475470679812133
Batch 74 Loss 0.0005475470679812133
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 3.7515710573643446e-06
Batch 75 Loss 3.7515710573643446e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026414296589791775
Batch 76 Loss 0.0026414296589791775
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019971062429249287
Batch 77 Loss 0.0019971062429249287
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 1.431384589523077e-06
Batch 78 Loss 1.431384589523077e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002667646389454603
Batch 79 Loss 0.002667646389454603
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030286747496575117
Batch 80 Loss 0.0030286747496575117
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032310672104358673
Batch 81 Loss 0.0032310672104358673
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017485543503426015
Batch 82 Loss 0.00017485543503426015
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005392954684793949
Batch 83 Loss 0.005392954684793949
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005481033585965633
Batch 84 Loss 0.005481033585965633
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 8.007482392713428e-07
Batch 85 Loss 8.007482392713428e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005861625075340271
Batch 86 Loss 0.005861625075340271
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004712705500423908
Batch 87 Loss 0.004712705500423908
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00043790199561044574
Batch 88 Loss 0.00043790199561044574
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00015650034765712917
Batch 89 Loss 0.00015650034765712917
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5521582099609077e-06
Batch 90 Loss 1.5521582099609077e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 6.39852078165859e-06
Batch 91 Loss 6.39852078165859e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4197285054251552e-06
Batch 92 Loss 1.4197285054251552e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 1.6404308553319424e-05
Batch 93 Loss 1.6404308553319424e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006703296210616827
Batch 94 Loss 0.006703296210616827
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6615089154802263e-05
Batch 95 Loss 2.6615089154802263e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00010936200123978779
Batch 96 Loss 0.00010936200123978779
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006038304418325424
Batch 97 Loss 0.006038304418325424
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003907090052962303
Batch 98 Loss 0.003907090052962303
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 3.1184390536509454e-05
Batch 99 Loss 3.1184390536509454e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004473107401281595
Batch 100 Loss 0.004473107401281595
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005181984044611454
Batch 101 Loss 0.005181984044611454
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003388212062418461
Batch 102 Loss 0.003388212062418461
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005150713492184877
Batch 103 Loss 0.005150713492184877
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 2.109336492139846e-06
Batch 104 Loss 2.109336492139846e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006638265214860439
Batch 105 Loss 0.006638265214860439
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 5.802969099022448e-06
Batch 106 Loss 5.802969099022448e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002566091949120164
Batch 107 Loss 0.002566091949120164
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 2.00835129362531e-05
Batch 108 Loss 2.00835129362531e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 8.885253919288516e-07
Batch 109 Loss 8.885253919288516e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0077823856845498085
Batch 110 Loss 0.0077823856845498085
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006419776938855648
Batch 111 Loss 0.006419776938855648
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004194979090243578
Batch 112 Loss 0.004194979090243578
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 6.474147085100412e-07
Batch 113 Loss 6.474147085100412e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 12...

Epoch: 12 Train loss: 0.0027304429095238447, 0.1% complete
..........
Epoch: 12 Train loss: 0.0029555996879935265, 1.2% complete
..........
Epoch: 12 Train loss: 0.0036740940995514393, 2.3% complete
..........
Epoch: 12 Train loss: 0.0030087174382060766, 3.4% complete
..........
Epoch: 12 Train loss: 0.005961022339761257, 4.4% complete
..........
Epoch: 12 Train loss: 0.0042036790400743484, 5.5% complete
..........
Epoch: 12 Train loss: 9.890674846246839e-06, 6.6% complete
..........
Epoch: 12 Train loss: 0.0044895075261592865, 7.7% complete
..........
Epoch: 12 Train loss: 0.0034515857696533203, 8.8% complete
..........
Epoch: 12 Train loss: 0.0032032558228820562, 9.9% complete
..........
Epoch: 12 Train loss: 0.003479455132037401, 11.0% complete
..........
Epoch: 12 Train loss: 3.3281474316027015e-05, 12.0% complete
..........
Epoch: 12 Train loss: 0.007907003164291382, 13.1% complete
..........
Epoch: 12 Train loss: 0.007719298359006643, 14.2% complete
..........
Epoch: 12 Train loss: 0.0005000110249966383, 15.3% complete
..........
Epoch: 12 Train loss: 0.006208150647580624, 16.4% complete
..........
Epoch: 12 Train loss: 0.005853485316038132, 17.5% complete
..........
Epoch: 12 Train loss: 1.0045674571301788e-05, 18.5% complete
..........
Epoch: 12 Train loss: 0.0040209852159023285, 19.6% complete
..........
Epoch: 12 Train loss: 0.006295021157711744, 20.7% complete
..........
Epoch: 12 Train loss: 0.0008128987974487245, 21.8% complete
..........
Epoch: 12 Train loss: 5.611647793557495e-06, 22.9% complete
..........
Epoch: 12 Train loss: 7.093671592883766e-05, 24.0% complete
..........
Epoch: 12 Train loss: 1.078587956726551e-06, 25.1% complete
..........
Epoch: 12 Train loss: 0.00970749743282795, 26.1% complete
..........
Epoch: 12 Train loss: 0.0008835360640659928, 27.2% complete
..........
Epoch: 12 Train loss: 0.007317204028367996, 28.3% complete
..........
Epoch: 12 Train loss: 0.006403080653399229, 29.4% complete
..........
Epoch: 12 Train loss: 0.001549832639284432, 30.5% complete
..........
Epoch: 12 Train loss: 0.005304987542331219, 31.6% complete
..........
Epoch: 12 Train loss: 0.002191311214119196, 32.6% complete
..........
Epoch: 12 Train loss: 0.0001740537118166685, 33.7% complete
..........
Epoch: 12 Train loss: 0.005455023609101772, 34.8% complete
..........
Epoch: 12 Train loss: 0.004751425236463547, 35.9% complete
..........
Epoch: 12 Train loss: 0.0005089272744953632, 37.0% complete
..........
Epoch: 12 Train loss: 0.0015837533865123987, 38.1% complete
..........
Epoch: 12 Train loss: 1.3138196663931012e-06, 39.2% complete
..........
Epoch: 12 Train loss: 2.1978557924740016e-06, 40.2% complete
..........
Epoch: 12 Train loss: 0.005570379551500082, 41.3% complete
..........
Epoch: 12 Train loss: 0.00041211998905055225, 42.4% complete
..........
Epoch: 12 Train loss: 0.0015208750264719129, 43.5% complete
..........
Epoch: 12 Train loss: 8.25489405542612e-06, 44.6% complete
..........
Epoch: 12 Train loss: 0.0007053571753203869, 45.7% complete
..........
Epoch: 12 Train loss: 0.002192771527916193, 46.7% complete
..........
Epoch: 12 Train loss: 1.7370901332469657e-05, 47.8% complete
..........
Epoch: 12 Train loss: 0.005246343556791544, 48.9% complete
..........
Epoch: 12 Train loss: 0.00805057492107153, 50.0% complete
..........
Epoch: 12 Train loss: 6.999882316449657e-05, 51.1% complete
..........
Epoch: 12 Train loss: 5.354617314878851e-06, 52.2% complete
..........
Epoch: 12 Train loss: 9.42964106798172e-08, 53.3% complete
..........
Epoch: 12 Train loss: 0.0038203769363462925, 54.3% complete
..........
Epoch: 12 Train loss: 0.0029107010923326015, 55.4% complete
..........
Epoch: 12 Train loss: 0.003248949535191059, 56.5% complete
..........
Epoch: 12 Train loss: 0.00506091071292758, 57.6% complete
..........
Epoch: 12 Train loss: 0.0024101652670651674, 58.7% complete
..........
Epoch: 12 Train loss: 0.005952805280685425, 59.8% complete
..........
Epoch: 12 Train loss: 0.0009901957819238305, 60.8% complete
..........
Epoch: 12 Train loss: 0.004261522553861141, 61.9% complete
..........
Epoch: 12 Train loss: 0.0008923603454604745, 63.0% complete
..........
Epoch: 12 Train loss: 0.004351651296019554, 64.1% complete
..........
Epoch: 12 Train loss: 3.1914576538838446e-05, 65.2% complete
..........
Epoch: 12 Train loss: 0.005698961205780506, 66.3% complete
..........
Epoch: 12 Train loss: 0.0056825787760317326, 67.4% complete
..........
Epoch: 12 Train loss: 0.0036909948103129864, 68.4% complete
..........
Epoch: 12 Train loss: 0.005504737142473459, 69.5% complete
..........
Epoch: 12 Train loss: 0.0036433227360248566, 70.6% complete
..........
Epoch: 12 Train loss: 0.004279120359569788, 71.7% complete
..........
Epoch: 12 Train loss: 3.967797965742648e-06, 72.8% complete
..........
Epoch: 12 Train loss: 5.9287802287144586e-05, 73.9% complete
..........
Epoch: 12 Train loss: 1.3164099073037505e-06, 74.9% complete
..........
Epoch: 12 Train loss: 0.007513186428695917, 76.0% complete
..........
Epoch: 12 Train loss: 9.096402209252119e-08, 77.1% complete
..........
Epoch: 12 Train loss: 0.006553865037858486, 78.2% complete
..........
Epoch: 12 Train loss: 0.0022982126101851463, 79.3% complete
..........
Epoch: 12 Train loss: 0.0060248118825256824, 80.4% complete
..........
Epoch: 12 Train loss: 0.004523829557001591, 81.5% complete
..........
Epoch: 12 Train loss: 2.6130874175578356e-07, 82.5% complete
..........
Epoch: 12 Train loss: 0.005456520710140467, 83.6% complete
..........
Epoch: 12 Train loss: 6.6004940890707076e-06, 84.7% complete
..........
Epoch: 12 Train loss: 9.262294042855501e-08, 85.8% complete
..........
Epoch: 12 Train loss: 0.004177639726549387, 86.9% complete
..........
Epoch: 12 Train loss: 8.328403055202216e-06, 88.0% complete
..........
Epoch: 12 Train loss: 0.00395340146496892, 89.0% complete
..........
Epoch: 12 Train loss: 0.0033744939137250185, 90.1% complete
..........
Epoch: 12 Train loss: 0.002095836913213134, 91.2% complete
..........
Epoch: 12 Train loss: 0.0004631058545783162, 92.3% complete
..........
Epoch: 12 Train loss: 0.0020706295035779476, 93.4% complete
..........
Epoch: 12 Train loss: 0.005467682145535946, 94.5% complete
..........
Epoch: 12 Train loss: 7.962850213516504e-05, 95.6% complete
..........
Epoch: 12 Train loss: 0.004289380274713039, 96.6% complete
..........
Epoch: 12 Train loss: 1.9300205167382956e-07, 97.7% complete
..........
Epoch: 12 Train loss: 7.079711940605193e-05, 98.8% complete
..........
Epoch: 12 Train loss: 0.0008079103427007794, 99.9% complete
..
Training complete
Validating epoch 12...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 7.17341317795217e-05
Batch 0 Loss 7.17341317795217e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004311932250857353
Batch 1 Loss 0.004311932250857353
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004540164023637772
Batch 2 Loss 0.004540164023637772
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004339295439422131
Batch 3 Loss 0.004339295439422131
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 9.130657417699695e-06
Batch 4 Loss 9.130657417699695e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004130413755774498
Batch 5 Loss 0.004130413755774498
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005674591287970543
Batch 6 Loss 0.005674591287970543
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014339741319417953
Batch 7 Loss 0.0014339741319417953
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 6.872694939374924e-06
Batch 8 Loss 6.872694939374924e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005304827354848385
Batch 9 Loss 0.005304827354848385
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 1.353633706457913e-06
Batch 10 Loss 1.353633706457913e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 4.127374631934799e-05
Batch 11 Loss 4.127374631934799e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 5.675523425452411e-06
Batch 12 Loss 5.675523425452411e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 8.239294402301311e-07
Batch 13 Loss 8.239294402301311e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0043321046978235245
Batch 14 Loss 0.0043321046978235245
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0051282960921525955
Batch 15 Loss 0.0051282960921525955
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014211753150448203
Batch 16 Loss 0.0014211753150448203
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004755026660859585
Batch 17 Loss 0.004755026660859585
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029510462190955877
Batch 18 Loss 0.0029510462190955877
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032451136503368616
Batch 19 Loss 0.0032451136503368616
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006037836894392967
Batch 20 Loss 0.006037836894392967
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002281435066834092
Batch 21 Loss 0.002281435066834092
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005904575809836388
Batch 22 Loss 0.005904575809836388
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006559063331224024
Batch 23 Loss 0.0006559063331224024
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017643810133449733
Batch 24 Loss 0.00017643810133449733
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012314626947045326
Batch 25 Loss 0.0012314626947045326
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 4.17351839132607e-06
Batch 26 Loss 4.17351839132607e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9120561773888767e-06
Batch 27 Loss 1.9120561773888767e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004770729690790176
Batch 28 Loss 0.004770729690790176
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004325151443481445
Batch 29 Loss 0.004325151443481445
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0807380022015423e-05
Batch 30 Loss 1.0807380022015423e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006001995876431465
Batch 31 Loss 0.006001995876431465
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005422644317150116
Batch 32 Loss 0.005422644317150116
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062896450981497765
Batch 33 Loss 0.0062896450981497765
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012795737711712718
Batch 34 Loss 0.0012795737711712718
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036157334689050913
Batch 35 Loss 0.0036157334689050913
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001524044491816312
Batch 36 Loss 0.0001524044491816312
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005520663107745349
Batch 37 Loss 0.0005520663107745349
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00423051044344902
Batch 38 Loss 0.00423051044344902
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004683941602706909
Batch 39 Loss 0.004683941602706909
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018638805486261845
Batch 40 Loss 0.0018638805486261845
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003010480897501111
Batch 41 Loss 0.003010480897501111
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003633372485637665
Batch 42 Loss 0.003633372485637665
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002596138045191765
Batch 43 Loss 0.002596138045191765
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 4.091853043064475e-07
Batch 44 Loss 4.091853043064475e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00808690395206213
Batch 45 Loss 0.00808690395206213
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002881657099351287
Batch 46 Loss 0.002881657099351287
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038137680385261774
Batch 47 Loss 0.0038137680385261774
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012481212615966797
Batch 48 Loss 0.0012481212615966797
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6889130822382867e-05
Batch 49 Loss 2.6889130822382867e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5526067474856973e-05
Batch 50 Loss 2.5526067474856973e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006680953316390514
Batch 51 Loss 0.006680953316390514
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034830730874091387
Batch 52 Loss 0.0034830730874091387
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 2.0274746930226684e-06
Batch 53 Loss 2.0274746930226684e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001957537839189172
Batch 54 Loss 0.001957537839189172
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004807463847100735
Batch 55 Loss 0.004807463847100735
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 6.044712790753692e-06
Batch 56 Loss 6.044712790753692e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007738902233541012
Batch 57 Loss 0.007738902233541012
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003050132654607296
Batch 58 Loss 0.003050132654607296
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00023686717031523585
Batch 59 Loss 0.00023686717031523585
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008150679059326649
Batch 60 Loss 0.008150679059326649
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001451231655664742
Batch 61 Loss 0.001451231655664742
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 2.638844307512045e-07
Batch 62 Loss 2.638844307512045e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031471748370677233
Batch 63 Loss 0.0031471748370677233
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031128497794270515
Batch 64 Loss 0.0031128497794270515
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000330702168866992
Batch 65 Loss 0.000330702168866992
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004313093377277255
Batch 66 Loss 0.0004313093377277255
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005971708334982395
Batch 67 Loss 0.005971708334982395
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030250782147049904
Batch 68 Loss 0.0030250782147049904
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 1.681088178884238e-05
Batch 69 Loss 1.681088178884238e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006845202296972275
Batch 70 Loss 0.006845202296972275
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006909523159265518
Batch 71 Loss 0.006909523159265518
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009632513392716646
Batch 72 Loss 0.0009632513392716646
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002990486565977335
Batch 73 Loss 0.002990486565977335
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004686754196882248
Batch 74 Loss 0.004686754196882248
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1430757846683264e-07
Batch 75 Loss 4.1430757846683264e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021379340905696154
Batch 76 Loss 0.0021379340905696154
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 8.03397415438667e-06
Batch 77 Loss 8.03397415438667e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006058995611965656
Batch 78 Loss 0.006058995611965656
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0015620922204107046
Batch 79 Loss 0.0015620922204107046
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002901257248595357
Batch 80 Loss 0.002901257248595357
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029813179280608892
Batch 81 Loss 0.0029813179280608892
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 2.932505594799295e-05
Batch 82 Loss 2.932505594799295e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002845829352736473
Batch 83 Loss 0.002845829352736473
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 2.065826265607029e-06
Batch 84 Loss 2.065826265607029e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007238517515361309
Batch 85 Loss 0.007238517515361309
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005471406504511833
Batch 86 Loss 0.005471406504511833
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034337621182203293
Batch 87 Loss 0.0034337621182203293
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006021889857947826
Batch 88 Loss 0.006021889857947826
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004405781626701355
Batch 89 Loss 0.004405781626701355
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006884836591780186
Batch 90 Loss 0.006884836591780186
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 2.430315362289548e-07
Batch 91 Loss 2.430315362289548e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013618702068924904
Batch 92 Loss 0.0013618702068924904
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0020891206804662943
Batch 93 Loss 0.0020891206804662943
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8906575860455632e-06
Batch 94 Loss 1.8906575860455632e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 4.138062649872154e-06
Batch 95 Loss 4.138062649872154e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025820781011134386
Batch 96 Loss 0.0025820781011134386
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005240398459136486
Batch 97 Loss 0.0005240398459136486
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3683456927537918e-06
Batch 98 Loss 1.3683456927537918e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 9.442497685085982e-06
Batch 99 Loss 9.442497685085982e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5759800337255e-07
Batch 100 Loss 2.5759800337255e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005140840075910091
Batch 101 Loss 0.005140840075910091
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008746848441660404
Batch 102 Loss 0.008746848441660404
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030827668961137533
Batch 103 Loss 0.0030827668961137533
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036415576469153166
Batch 104 Loss 0.0036415576469153166
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059708282351493835
Batch 105 Loss 0.0059708282351493835
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006261470727622509
Batch 106 Loss 0.006261470727622509
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 5.702335329260677e-05
Batch 107 Loss 5.702335329260677e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014982689754106104
Batch 108 Loss 0.00014982689754106104
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038813163992017508
Batch 109 Loss 0.0038813163992017508
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003907585516571999
Batch 110 Loss 0.003907585516571999
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005772297270596027
Batch 111 Loss 0.005772297270596027
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 9.508612129138783e-05
Batch 112 Loss 9.508612129138783e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00021655469026882201
Batch 113 Loss 0.00021655469026882201
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 13...

Epoch: 13 Train loss: 0.004697304219007492, 0.1% complete
..........
Epoch: 13 Train loss: 0.005652186926454306, 1.2% complete
..........
Epoch: 13 Train loss: 0.0037768820766359568, 2.3% complete
..........
Epoch: 13 Train loss: 0.003402577480301261, 3.4% complete
..........
Epoch: 13 Train loss: 0.005644879769533873, 4.4% complete
..........
Epoch: 13 Train loss: 0.009076551534235477, 5.5% complete
..........
Epoch: 13 Train loss: 0.00503438338637352, 6.6% complete
..........
Epoch: 13 Train loss: 0.0016110845608636737, 7.7% complete
..........
Epoch: 13 Train loss: 0.004762590397149324, 8.8% complete
..........
Epoch: 13 Train loss: 5.259588942863047e-05, 9.9% complete
..........
Epoch: 13 Train loss: 0.002005363116040826, 11.0% complete
..........
Epoch: 13 Train loss: 0.004230844788253307, 12.0% complete
..........
Epoch: 13 Train loss: 0.004013720899820328, 13.1% complete
..........
Epoch: 13 Train loss: 4.162595723755658e-05, 14.2% complete
..........
Epoch: 13 Train loss: 0.0037408187054097652, 15.3% complete
..........
Epoch: 13 Train loss: 0.004755769390612841, 16.4% complete
..........
Epoch: 13 Train loss: 2.4178494641091675e-05, 17.5% complete
..........
Epoch: 13 Train loss: 4.5669206883758307e-05, 18.5% complete
..........
Epoch: 13 Train loss: 0.005784078501164913, 19.6% complete
..........
Epoch: 13 Train loss: 1.3877666788175702e-05, 20.7% complete
..........
Epoch: 13 Train loss: 6.781748379580677e-05, 21.8% complete
..........
Epoch: 13 Train loss: 0.005333904176950455, 22.9% complete
..........
Epoch: 13 Train loss: 0.008245733566582203, 24.0% complete
..........
Epoch: 13 Train loss: 1.5025143511593342e-06, 25.1% complete
..........
Epoch: 13 Train loss: 9.272189345210791e-07, 26.1% complete
..........
Epoch: 13 Train loss: 0.004067751578986645, 27.2% complete
..........
Epoch: 13 Train loss: 0.011704659089446068, 28.3% complete
..........
Epoch: 13 Train loss: 1.8026184989139438e-06, 29.4% complete
..........
Epoch: 13 Train loss: 0.0031009651720523834, 30.5% complete
..........
Epoch: 13 Train loss: 0.005014263559132814, 31.6% complete
..........
Epoch: 13 Train loss: 0.004785008262842894, 32.6% complete
..........
Epoch: 13 Train loss: 0.005716162733733654, 33.7% complete
..........
Epoch: 13 Train loss: 0.0002728156978264451, 34.8% complete
..........
Epoch: 13 Train loss: 0.007650219835340977, 35.9% complete
..........
Epoch: 13 Train loss: 0.004308375529944897, 37.0% complete
..........
Epoch: 13 Train loss: 0.004906185902655125, 38.1% complete
..........
Epoch: 13 Train loss: 0.006518415175378323, 39.2% complete
..........
Epoch: 13 Train loss: 0.0007364822085946798, 40.2% complete
..........
Epoch: 13 Train loss: 0.009244200773537159, 41.3% complete
..........
Epoch: 13 Train loss: 3.663959796540439e-05, 42.4% complete
..........
Epoch: 13 Train loss: 0.00013682461576536298, 43.5% complete
..........
Epoch: 13 Train loss: 0.0029208105988800526, 44.6% complete
..........
Epoch: 13 Train loss: 0.0010184856364503503, 45.7% complete
..........
Epoch: 13 Train loss: 1.8274091416969895e-05, 46.7% complete
..........
Epoch: 13 Train loss: 0.0030555480625480413, 47.8% complete
..........
Epoch: 13 Train loss: 0.0034281436819583178, 48.9% complete
..........
Epoch: 13 Train loss: 0.0031427175272256136, 50.0% complete
..........
Epoch: 13 Train loss: 0.003949692007154226, 51.1% complete
..........
Epoch: 13 Train loss: 4.429261025507003e-05, 52.2% complete
..........
Epoch: 13 Train loss: 0.0078376280143857, 53.3% complete
..........
Epoch: 13 Train loss: 0.009349169209599495, 54.3% complete
..........
Epoch: 13 Train loss: 0.002656970638781786, 55.4% complete
..........
Epoch: 13 Train loss: 0.0002710426051635295, 56.5% complete
..........
Epoch: 13 Train loss: 0.006431881338357925, 57.6% complete
..........
Epoch: 13 Train loss: 0.004151500761508942, 58.7% complete
..........
Epoch: 13 Train loss: 0.007130095269531012, 59.8% complete
..........
Epoch: 13 Train loss: 0.002012680983170867, 60.8% complete
..........
Epoch: 13 Train loss: 6.071932148188353e-07, 61.9% complete
..........
Epoch: 13 Train loss: 0.0004570011515170336, 63.0% complete
..........
Epoch: 13 Train loss: 0.006862590555101633, 64.1% complete
..........
Epoch: 13 Train loss: 0.004156206734478474, 65.2% complete
..........
Epoch: 13 Train loss: 0.005682948045432568, 66.3% complete
..........
Epoch: 13 Train loss: 2.883825800381601e-06, 67.4% complete
..........
Epoch: 13 Train loss: 7.749137148493901e-05, 68.4% complete
..........
Epoch: 13 Train loss: 0.0026262160390615463, 69.5% complete
..........
Epoch: 13 Train loss: 0.004897016566246748, 70.6% complete
..........
Epoch: 13 Train loss: 0.0041519878432154655, 71.7% complete
..........
Epoch: 13 Train loss: 0.004124436993151903, 72.8% complete
..........
Epoch: 13 Train loss: 0.0034899157471954823, 73.9% complete
..........
Epoch: 13 Train loss: 1.727457856759429e-07, 74.9% complete
..........
Epoch: 13 Train loss: 0.007270428817719221, 76.0% complete
..........
Epoch: 13 Train loss: 1.3732642401009798e-07, 77.1% complete
..........
Epoch: 13 Train loss: 2.2721796995028853e-06, 78.2% complete
..........
Epoch: 13 Train loss: 2.240078174509108e-06, 79.3% complete
..........
Epoch: 13 Train loss: 6.14711461821571e-06, 80.4% complete
..........
Epoch: 13 Train loss: 0.006647405680269003, 81.5% complete
..........
Epoch: 13 Train loss: 0.005708634853363037, 82.5% complete
..........
Epoch: 13 Train loss: 0.006208730395883322, 83.6% complete
..........
Epoch: 13 Train loss: 0.007576415315270424, 84.7% complete
..........
Epoch: 13 Train loss: 0.002813020022585988, 85.8% complete
..........
Epoch: 13 Train loss: 0.0015782020054757595, 86.9% complete
..........
Epoch: 13 Train loss: 0.005151348188519478, 88.0% complete
..........
Epoch: 13 Train loss: 4.131386958761141e-05, 89.0% complete
..........
Epoch: 13 Train loss: 0.007104427088052034, 90.1% complete
..........
Epoch: 13 Train loss: 0.0002183641481678933, 91.2% complete
..........
Epoch: 13 Train loss: 5.538306140806526e-06, 92.3% complete
..........
Epoch: 13 Train loss: 0.005513360258191824, 93.4% complete
..........
Epoch: 13 Train loss: 0.005135389976203442, 94.5% complete
..........
Epoch: 13 Train loss: 0.0022777116391807795, 95.6% complete
..........
Epoch: 13 Train loss: 8.936774975154549e-06, 96.6% complete
..........
Epoch: 13 Train loss: 0.004363091196864843, 97.7% complete
..........
Epoch: 13 Train loss: 2.1811836631968617e-05, 98.8% complete
..........
Epoch: 13 Train loss: 0.005683636758476496, 99.9% complete
..
Training complete
Validating epoch 13...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033134270925074816
Batch 0 Loss 0.0033134270925074816
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004756978712975979
Batch 1 Loss 0.004756978712975979
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0065224082209169865
Batch 2 Loss 0.0065224082209169865
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00011773026926675811
Batch 3 Loss 0.00011773026926675811
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3290282115340233e-05
Batch 4 Loss 2.3290282115340233e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3136497475206852e-05
Batch 5 Loss 2.3136497475206852e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4618439308833331e-05
Batch 6 Loss 1.4618439308833331e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004689001478254795
Batch 7 Loss 0.004689001478254795
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005383661482483149
Batch 8 Loss 0.005383661482483149
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006024714559316635
Batch 9 Loss 0.006024714559316635
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006563427159562707
Batch 10 Loss 0.0006563427159562707
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035376122687011957
Batch 11 Loss 0.0035376122687011957
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031528642866760492
Batch 12 Loss 0.0031528642866760492
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007315929979085922
Batch 13 Loss 0.007315929979085922
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007002756930887699
Batch 14 Loss 0.007002756930887699
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 7.062044460326433e-08
Batch 15 Loss 7.062044460326433e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002757443580776453
Batch 16 Loss 0.002757443580776453
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003962565679103136
Batch 17 Loss 0.003962565679103136
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 4.145840648561716e-08
Batch 18 Loss 4.145840648561716e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004683447070419788
Batch 19 Loss 0.004683447070419788
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059403181076049805
Batch 20 Loss 0.0059403181076049805
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008690646849572659
Batch 21 Loss 0.008690646849572659
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006166123319417238
Batch 22 Loss 0.006166123319417238
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 7.445792289217934e-05
Batch 23 Loss 7.445792289217934e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 2.0034771296195686e-05
Batch 24 Loss 2.0034771296195686e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007229011505842209
Batch 25 Loss 0.007229011505842209
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4119723346084356e-07
Batch 26 Loss 1.4119723346084356e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 6.120244506746531e-07
Batch 27 Loss 6.120244506746531e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005171945318579674
Batch 28 Loss 0.005171945318579674
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0057269795797765255
Batch 29 Loss 0.0057269795797765255
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006027522962540388
Batch 30 Loss 0.006027522962540388
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006195911555550992
Batch 31 Loss 0.0006195911555550992
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006696308963000774
Batch 32 Loss 0.006696308963000774
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 6.494679837487638e-06
Batch 33 Loss 6.494679837487638e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004144627600908279
Batch 34 Loss 0.004144627600908279
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7420097719877958e-07
Batch 35 Loss 1.7420097719877958e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7957718227989972e-06
Batch 36 Loss 1.7957718227989972e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0015127413207665086
Batch 37 Loss 0.0015127413207665086
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006255757063627243
Batch 38 Loss 0.006255757063627243
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4333854778669775e-06
Batch 39 Loss 1.4333854778669775e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 4.308822099119425e-08
Batch 40 Loss 4.308822099119425e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 2.9870716389268637e-07
Batch 41 Loss 2.9870716389268637e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002214105101302266
Batch 42 Loss 0.002214105101302266
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 8.249298844020814e-06
Batch 43 Loss 8.249298844020814e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029943312983959913
Batch 44 Loss 0.0029943312983959913
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006623092573136091
Batch 45 Loss 0.006623092573136091
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004288847092539072
Batch 46 Loss 0.004288847092539072
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 1.66997779160738e-07
Batch 47 Loss 1.66997779160738e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008912504650652409
Batch 48 Loss 0.008912504650652409
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0022954060696065426
Batch 49 Loss 0.0022954060696065426
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0011717677116394e-07
Batch 50 Loss 1.0011717677116394e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004212993662804365
Batch 51 Loss 0.004212993662804365
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004617712460458279
Batch 52 Loss 0.004617712460458279
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008347563445568085
Batch 53 Loss 0.008347563445568085
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 4.812311090063304e-06
Batch 54 Loss 4.812311090063304e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 6.02740328758955e-08
Batch 55 Loss 6.02740328758955e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00391407823190093
Batch 56 Loss 0.00391407823190093
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0045686159282922745
Batch 57 Loss 0.0045686159282922745
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032261349260807037
Batch 58 Loss 0.0032261349260807037
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00010513724555494264
Batch 59 Loss 0.00010513724555494264
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062243021093308926
Batch 60 Loss 0.0062243021093308926
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00565230892971158
Batch 61 Loss 0.00565230892971158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00662989029660821
Batch 62 Loss 0.00662989029660821
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0013477886095643044
Batch 63 Loss 0.0013477886095643044
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5756813809275627e-07
Batch 64 Loss 1.5756813809275627e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007198491599410772
Batch 65 Loss 0.007198491599410772
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014901479880791157
Batch 66 Loss 0.00014901479880791157
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0133371688425541e-05
Batch 67 Loss 1.0133371688425541e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00844535231590271
Batch 68 Loss 0.00844535231590271
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002929752692580223
Batch 69 Loss 0.002929752692580223
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005078094080090523
Batch 70 Loss 0.005078094080090523
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0047795698046684265
Batch 71 Loss 0.0047795698046684265
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 2.6293419068679214e-06
Batch 72 Loss 2.6293419068679214e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002178704831749201
Batch 73 Loss 0.002178704831749201
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003659093054011464
Batch 74 Loss 0.0003659093054011464
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004571079276502132
Batch 75 Loss 0.004571079276502132
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008289516903460026
Batch 76 Loss 0.008289516903460026
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 4.244793672114611e-08
Batch 77 Loss 4.244793672114611e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8706487026065588e-07
Batch 78 Loss 1.8706487026065588e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007436342071741819
Batch 79 Loss 0.007436342071741819
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1919720893492922e-05
Batch 80 Loss 2.1919720893492922e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3767923519480973e-05
Batch 81 Loss 1.3767923519480973e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00021457117691170424
Batch 82 Loss 0.00021457117691170424
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 9.690120350569487e-08
Batch 83 Loss 9.690120350569487e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008479255251586437
Batch 84 Loss 0.008479255251586437
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5606347005814314e-06
Batch 85 Loss 1.5606347005814314e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017312057025264949
Batch 86 Loss 0.00017312057025264949
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0033046926837414503
Batch 87 Loss 0.0033046926837414503
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005056076683104038
Batch 88 Loss 0.005056076683104038
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0024532389361411333
Batch 89 Loss 0.0024532389361411333
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009643227676860988
Batch 90 Loss 0.0009643227676860988
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 1.5176919987425208e-06
Batch 91 Loss 1.5176919987425208e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002201049355790019
Batch 92 Loss 0.002201049355790019
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004941408056765795
Batch 93 Loss 0.004941408056765795
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 7.994822226464748e-08
Batch 94 Loss 7.994822226464748e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 8.947972673922777e-08
Batch 95 Loss 8.947972673922777e-08
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003594681154936552
Batch 96 Loss 0.003594681154936552
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 5.698093445971608e-07
Batch 97 Loss 5.698093445971608e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 9.117371519096196e-06
Batch 98 Loss 9.117371519096196e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028236201032996178
Batch 99 Loss 0.0028236201032996178
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 1.3554163160733879e-06
Batch 100 Loss 1.3554163160733879e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8054739232175052e-05
Batch 101 Loss 2.8054739232175052e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 1.1587690096348524e-07
Batch 102 Loss 1.1587690096348524e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008013990707695484
Batch 103 Loss 0.008013990707695484
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005788456182926893
Batch 104 Loss 0.005788456182926893
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009495559148490429
Batch 105 Loss 0.009495559148490429
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 9.646173566579819e-07
Batch 106 Loss 9.646173566579819e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003677224274724722
Batch 107 Loss 0.003677224274724722
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019891688134521246
Batch 108 Loss 0.0019891688134521246
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002655558637343347
Batch 109 Loss 0.0002655558637343347
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 8.541827264707536e-05
Batch 110 Loss 8.541827264707536e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00042366847628727555
Batch 111 Loss 0.00042366847628727555
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 1.415853330399841e-05
Batch 112 Loss 1.415853330399841e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 3.7491554394364357e-07
Batch 113 Loss 3.7491554394364357e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Validation complete
Training epoch 14...

Epoch: 14 Train loss: 5.572386726271361e-06, 0.1% complete
..........
Epoch: 14 Train loss: 8.830465958453715e-07, 1.2% complete
..........
Epoch: 14 Train loss: 0.005964531097561121, 2.3% complete
..........
Epoch: 14 Train loss: 2.780592330964282e-05, 3.4% complete
..........
Epoch: 14 Train loss: 0.0016248711617663503, 4.4% complete
..........
Epoch: 14 Train loss: 1.0774238035082817e-07, 5.5% complete
..........
Epoch: 14 Train loss: 5.164765752851963e-07, 6.6% complete
..........
Epoch: 14 Train loss: 0.001614636741578579, 7.7% complete
..........
Epoch: 14 Train loss: 0.0049817063845694065, 8.8% complete
..........
Epoch: 14 Train loss: 8.437495853286237e-05, 9.9% complete
..........
Epoch: 14 Train loss: 0.003646932076662779, 11.0% complete
..........
Epoch: 14 Train loss: 0.004353825002908707, 12.0% complete
..........
Epoch: 14 Train loss: 2.6254565455019474e-07, 13.1% complete
..........
Epoch: 14 Train loss: 0.0034200805239379406, 14.2% complete
..........
Epoch: 14 Train loss: 0.003170635551214218, 15.3% complete
..........
Epoch: 14 Train loss: 0.0023402036167681217, 16.4% complete
..........
Epoch: 14 Train loss: 1.2705480912700295e-05, 17.5% complete
..........
Epoch: 14 Train loss: 3.77932155970484e-06, 18.5% complete
..........
Epoch: 14 Train loss: 6.694455805700272e-06, 19.6% complete
..........
Epoch: 14 Train loss: 0.002551525365561247, 20.7% complete
..........
Epoch: 14 Train loss: 0.006267774384468794, 21.8% complete
..........
Epoch: 14 Train loss: 0.0027070653159171343, 22.9% complete
..........
Epoch: 14 Train loss: 0.0021692770533263683, 24.0% complete
..........
Epoch: 14 Train loss: 8.342129876837134e-05, 25.1% complete
..........
Epoch: 14 Train loss: 0.0024139664601534605, 26.1% complete
..........
Epoch: 14 Train loss: 0.002609986113384366, 27.2% complete
..........
Epoch: 14 Train loss: 0.004318980500102043, 28.3% complete
..........
Epoch: 14 Train loss: 0.005293154623359442, 29.4% complete
..........
Epoch: 14 Train loss: 0.0025423839688301086, 30.5% complete
..........
Epoch: 14 Train loss: 0.0006756657385267317, 31.6% complete
..........
Epoch: 14 Train loss: 0.004925878718495369, 32.6% complete
..........
Epoch: 14 Train loss: 0.0017352876020595431, 33.7% complete
..........
Epoch: 14 Train loss: 0.00014106208982411772, 34.8% complete
..........
Epoch: 14 Train loss: 0.004241420421749353, 35.9% complete
..........
Epoch: 14 Train loss: 0.007810680661350489, 37.0% complete
..........
Epoch: 14 Train loss: 0.00429152138531208, 38.1% complete
..........
Epoch: 14 Train loss: 2.6507674192544073e-05, 39.2% complete
..........
Epoch: 14 Train loss: 0.005502038169652224, 40.2% complete
..........
Epoch: 14 Train loss: 5.307854735292494e-06, 41.3% complete
..........
Epoch: 14 Train loss: 3.459621802903712e-05, 42.4% complete
..........
Epoch: 14 Train loss: 0.004854490049183369, 43.5% complete
..........
Epoch: 14 Train loss: 0.0018542129546403885, 44.6% complete
..........
Epoch: 14 Train loss: 0.004813149105757475, 45.7% complete
..........
Epoch: 14 Train loss: 2.8297436074353755e-05, 46.7% complete
..........
Epoch: 14 Train loss: 0.010249455459415913, 47.8% complete
..........
Epoch: 14 Train loss: 2.3421962396241724e-05, 48.9% complete
..........
Epoch: 14 Train loss: 0.003606838174164295, 50.0% complete
..........
Epoch: 14 Train loss: 0.005693035200238228, 51.1% complete
..........
Epoch: 14 Train loss: 0.00014999654376879334, 52.2% complete
..........
Epoch: 14 Train loss: 0.0038683973252773285, 53.3% complete
..........
Epoch: 14 Train loss: 0.007838230580091476, 54.3% complete
..........
Epoch: 14 Train loss: 0.003407566575333476, 55.4% complete
..........
Epoch: 14 Train loss: 0.0038512214086949825, 56.5% complete
..........
Epoch: 14 Train loss: 7.92085484135896e-06, 57.6% complete
..........
Epoch: 14 Train loss: 0.00035520741948857903, 58.7% complete
..........
Epoch: 14 Train loss: 0.005855831317603588, 59.8% complete
..........
Epoch: 14 Train loss: 0.003814938012510538, 60.8% complete
..........
Epoch: 14 Train loss: 0.00505263963714242, 61.9% complete
..........
Epoch: 14 Train loss: 0.0027608200907707214, 63.0% complete
..........
Epoch: 14 Train loss: 0.006944504100829363, 64.1% complete
..........
Epoch: 14 Train loss: 5.871625035069883e-06, 65.2% complete
..........
Epoch: 14 Train loss: 0.005535278003662825, 66.3% complete
..........
Epoch: 14 Train loss: 5.153357778908685e-05, 67.4% complete
..........
Epoch: 14 Train loss: 0.00020602978474926203, 68.4% complete
..........
Epoch: 14 Train loss: 0.0015754339983686805, 69.5% complete
..........
Epoch: 14 Train loss: 2.9237999115139246e-05, 70.6% complete
..........
Epoch: 14 Train loss: 0.007940839976072311, 71.7% complete
..........
Epoch: 14 Train loss: 4.688954504672438e-06, 72.8% complete
..........
Epoch: 14 Train loss: 0.0038932599127292633, 73.9% complete
..........
Epoch: 14 Train loss: 0.0049495031125843525, 74.9% complete
..........
Epoch: 14 Train loss: 1.0390598617959768e-05, 76.0% complete
..........
Epoch: 14 Train loss: 0.0013119375798851252, 77.1% complete
..........
Epoch: 14 Train loss: 0.003255276009440422, 78.2% complete
..........
Epoch: 14 Train loss: 0.003969014622271061, 79.3% complete
..........
Epoch: 14 Train loss: 4.824229108635336e-05, 80.4% complete
..........
Epoch: 14 Train loss: 0.005178345832973719, 81.5% complete
..........
Epoch: 14 Train loss: 0.005933801643550396, 82.5% complete
..........
Epoch: 14 Train loss: 0.002001919550821185, 83.6% complete
..........
Epoch: 14 Train loss: 0.0011953397188335657, 84.7% complete
..........
Epoch: 14 Train loss: 1.091633748728782e-05, 85.8% complete
..........
Epoch: 14 Train loss: 0.00041033729212358594, 86.9% complete
..........
Epoch: 14 Train loss: 2.9672373784705997e-06, 88.0% complete
..........
Epoch: 14 Train loss: 0.00304613565094769, 89.0% complete
..........
Epoch: 14 Train loss: 0.007040480617433786, 90.1% complete
..........
Epoch: 14 Train loss: 0.0042304834350943565, 91.2% complete
..........
Epoch: 14 Train loss: 0.008557053282856941, 92.3% complete
..........
Epoch: 14 Train loss: 2.0917272195219994e-05, 93.4% complete
..........
Epoch: 14 Train loss: 1.3850840332452208e-05, 94.5% complete
..........
Epoch: 14 Train loss: 5.8815858210437e-06, 95.6% complete
..........
Epoch: 14 Train loss: 0.001053659594617784, 96.6% complete
..........
Epoch: 14 Train loss: 0.006817779969424009, 97.7% complete
..........
Epoch: 14 Train loss: 0.005502166226506233, 98.8% complete
..........
Epoch: 14 Train loss: 2.0731939002871513e-05, 99.9% complete
..
Training complete
Validating epoch 14...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007868376560509205
Batch 0 Loss 0.007868376560509205
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004224664531648159
Batch 1 Loss 0.004224664531648159
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004974424839019775
Batch 2 Loss 0.004974424839019775
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00264847744256258
Batch 3 Loss 0.00264847744256258
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004947415553033352
Batch 4 Loss 0.004947415553033352
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027706208638846874
Batch 5 Loss 0.0027706208638846874
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8739352526608855e-05
Batch 6 Loss 1.8739352526608855e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004758443683385849
Batch 7 Loss 0.004758443683385849
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018208574037998915
Batch 8 Loss 0.0018208574037998915
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018891911022365093
Batch 9 Loss 0.0018891911022365093
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0050840978510677814
Batch 10 Loss 0.0050840978510677814
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001807462191209197
Batch 11 Loss 0.001807462191209197
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004960394464433193
Batch 12 Loss 0.004960394464433193
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00039799773367121816
Batch 13 Loss 0.00039799773367121816
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002492420026101172
Batch 14 Loss 0.0002492420026101172
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 3.786262823268771e-07
Batch 15 Loss 3.786262823268771e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004407399799674749
Batch 16 Loss 0.004407399799674749
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 9.896612027660012e-07
Batch 17 Loss 9.896612027660012e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004796147346496582
Batch 18 Loss 0.004796147346496582
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 7.243673462653533e-05
Batch 19 Loss 7.243673462653533e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001971411402337253
Batch 20 Loss 0.0001971411402337253
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 5.526827953872271e-05
Batch 21 Loss 5.526827953872271e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037083900533616543
Batch 22 Loss 0.0037083900533616543
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005439397878944874
Batch 23 Loss 0.005439397878944874
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003052826039493084
Batch 24 Loss 0.003052826039493084
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00149821606464684
Batch 25 Loss 0.00149821606464684
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005663653835654259
Batch 26 Loss 0.005663653835654259
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 4.082598024979234e-05
Batch 27 Loss 4.082598024979234e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0015064147301018238
Batch 28 Loss 0.0015064147301018238
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004250293131917715
Batch 29 Loss 0.004250293131917715
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010463546495884657
Batch 30 Loss 0.0010463546495884657
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 4.214016371406615e-06
Batch 31 Loss 4.214016371406615e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0018260888755321503
Batch 32 Loss 0.0018260888755321503
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0068673426285386086
Batch 33 Loss 0.0068673426285386086
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 1.0843141353689134e-06
Batch 34 Loss 1.0843141353689134e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003988021053373814
Batch 35 Loss 0.003988021053373814
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 1.214648364111781e-07
Batch 36 Loss 1.214648364111781e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 2.367843990214169e-05
Batch 37 Loss 2.367843990214169e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034500258043408394
Batch 38 Loss 0.0034500258043408394
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008133085793815553
Batch 39 Loss 0.0008133085793815553
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006168459076434374
Batch 40 Loss 0.006168459076434374
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 1.4170313079375774e-05
Batch 41 Loss 1.4170313079375774e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002224944473709911
Batch 42 Loss 0.0002224944473709911
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0041344561614096165
Batch 43 Loss 0.0041344561614096165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004412386566400528
Batch 44 Loss 0.004412386566400528
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 5.220499588176608e-07
Batch 45 Loss 5.220499588176608e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029597480315715075
Batch 46 Loss 0.0029597480315715075
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006781762465834618
Batch 47 Loss 0.006781762465834618
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 3.035405825357884e-06
Batch 48 Loss 3.035405825357884e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004948806948959827
Batch 49 Loss 0.004948806948959827
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0049003795720636845
Batch 50 Loss 0.0049003795720636845
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005768063943833113
Batch 51 Loss 0.005768063943833113
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005511168856173754
Batch 52 Loss 0.005511168856173754
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005072595551609993
Batch 53 Loss 0.005072595551609993
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 9.80886397883296e-06
Batch 54 Loss 9.80886397883296e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 9.413051884621382e-07
Batch 55 Loss 9.413051884621382e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003595530753955245
Batch 56 Loss 0.003595530753955245
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0015509012155234814
Batch 57 Loss 0.0015509012155234814
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0060378145426511765
Batch 58 Loss 0.0060378145426511765
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 3.639725036919117e-07
Batch 59 Loss 3.639725036919117e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005688439588993788
Batch 60 Loss 0.005688439588993788
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004729021806269884
Batch 61 Loss 0.004729021806269884
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008232657797634602
Batch 62 Loss 0.008232657797634602
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005927123129367828
Batch 63 Loss 0.005927123129367828
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 9.549570677336305e-06
Batch 64 Loss 9.549570677336305e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 1.8846913008019328e-06
Batch 65 Loss 1.8846913008019328e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0039415378123521805
Batch 66 Loss 0.0039415378123521805
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006207573693245649
Batch 67 Loss 0.006207573693245649
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035367999225854874
Batch 68 Loss 0.0035367999225854874
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 2.7296693588141352e-05
Batch 69 Loss 2.7296693588141352e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003227375913411379
Batch 70 Loss 0.003227375913411379
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004423642065376043
Batch 71 Loss 0.004423642065376043
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 5.203764885663986e-05
Batch 72 Loss 5.203764885663986e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004385823383927345
Batch 73 Loss 0.004385823383927345
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 1.2158125173300505e-07
Batch 74 Loss 1.2158125173300505e-07
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021648183465003967
Batch 75 Loss 0.0021648183465003967
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000722956785466522
Batch 76 Loss 0.000722956785466522
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007546615321189165
Batch 77 Loss 0.007546615321189165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8870927053503692e-06
Batch 78 Loss 2.8870927053503692e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009445118717849255
Batch 79 Loss 0.009445118717849255
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004275563172996044
Batch 80 Loss 0.004275563172996044
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0050354828126728535
Batch 81 Loss 0.0050354828126728535
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007611463777720928
Batch 82 Loss 0.007611463777720928
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 4.633853677660227e-05
Batch 83 Loss 4.633853677660227e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003157245460897684
Batch 84 Loss 0.003157245460897684
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004486639052629471
Batch 85 Loss 0.004486639052629471
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000689343607518822
Batch 86 Loss 0.000689343607518822
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004052609205245972
Batch 87 Loss 0.004052609205245972
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004412021487951279
Batch 88 Loss 0.004412021487951279
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 6.776609370717779e-05
Batch 89 Loss 6.776609370717779e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029866008553653955
Batch 90 Loss 0.0029866008553653955
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002774052321910858
Batch 91 Loss 0.002774052321910858
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004714321810752153
Batch 92 Loss 0.004714321810752153
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0041289557702839375
Batch 93 Loss 0.0041289557702839375
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004967883694916964
Batch 94 Loss 0.004967883694916964
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009107756428420544
Batch 95 Loss 0.009107756428420544
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00020201173902023584
Batch 96 Loss 0.00020201173902023584
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0055794198997318745
Batch 97 Loss 0.0055794198997318745
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004238463006913662
Batch 98 Loss 0.004238463006913662
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0026762497145682573
Batch 99 Loss 0.0026762497145682573
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006270578014664352
Batch 100 Loss 0.0006270578014664352
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00653156079351902
Batch 101 Loss 0.00653156079351902
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006297260522842407
Batch 102 Loss 0.006297260522842407
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 5.536960088647902e-06
Batch 103 Loss 5.536960088647902e-06
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0037887145299464464
Batch 104 Loss 0.0037887145299464464
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006683654151856899
Batch 105 Loss 0.006683654151856899
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031356788240373135
Batch 106 Loss 0.0031356788240373135
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006603679619729519
Batch 107 Loss 0.006603679619729519
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 1.7197802662849426e-05
Batch 108 Loss 1.7197802662849426e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003186308778822422
Batch 109 Loss 0.003186308778822422
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006271334830671549
Batch 110 Loss 0.006271334830671549
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035553481429815292
Batch 111 Loss 0.0035553481429815292
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0004353495896793902
Batch 112 Loss 0.0004353495896793902
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004184712655842304
Batch 113 Loss 0.004184712655842304
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Run complete. Total time: 00:40:00
Testing...
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 0: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_197.nii.gz Dice 0.8200. 3.85% complete
Jaccard: 0.6948512045347189 sensitivity : 0.8711874444773468 specificity :0.9943718764571061
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (41, 64, 64)
🛠 Test Sample 1: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_164.nii.gz Dice 0.7071. 7.69% complete
Jaccard: 0.546907574704656 sensitivity : 0.6102352028948048 specificity :0.9972694082295648
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (32, 64, 64)
🛠 Test Sample 2: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_226.nii.gz Dice 0.8614. 11.54% complete
Jaccard: 0.756544502617801 sensitivity : 0.9080911233307148 specificity :0.9960319312823864
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (32, 64, 64)
🛠 Test Sample 3: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_123.nii.gz Dice 0.8210. 15.38% complete
Jaccard: 0.6964032181732135 sensitivity : 0.9114276865902756 specificity :0.992201371995338
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (34, 64, 64)
🛠 Test Sample 4: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_279.nii.gz Dice 0.7948. 19.23% complete
Jaccard: 0.6594611528822055 sensitivity : 0.8837111670864819 specificity :0.9940824944112447
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 5: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_276.nii.gz Dice 0.8929. 23.08% complete
Jaccard: 0.8065978342986653 sensitivity : 0.8792204227285204 specificity :0.9976523973460638
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 6: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_385.nii.gz Dice 0.8613. 26.92% complete
Jaccard: 0.7564632885211996 sensitivity : 0.8613482484545187 specificity :0.996634824918014
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 7: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_042.nii.gz Dice 0.7852. 30.77% complete
Jaccard: 0.6463903743315508 sensitivity : 0.7540940992981544 specificity :0.9956602687789852
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 8: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_141.nii.gz Dice 0.7749. 34.62% complete
Jaccard: 0.632514817950889 sensitivity : 0.8257184966838614 specificity :0.9937412233681127
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 9: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_108.nii.gz Dice 0.8765. 38.46% complete
Jaccard: 0.7801336713528463 sensitivity : 0.8591370558375635 specificity :0.9972198221801054
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (31, 64, 64)
🛠 Test Sample 10: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_265.nii.gz Dice 0.8142. 42.31% complete
Jaccard: 0.6865859808371155 sensitivity : 0.8758443229334191 specificity :0.9930812888016985
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 11: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_034.nii.gz Dice 0.8370. 46.15% complete
Jaccard: 0.7197371746272428 sensitivity : 0.8438518518518519 specificity :0.9959606054927437
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 12: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_308.nii.gz Dice 0.8915. 50.00% complete
Jaccard: 0.8042908224076282 sensitivity : 0.9233716475095786 specificity :0.9964406489729858
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 13: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_017.nii.gz Dice 0.8271. 53.85% complete
Jaccard: 0.7051855751513556 sensitivity : 0.7702702702702703 specificity :0.997705208675884
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (39, 64, 64)
🛠 Test Sample 14: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_126.nii.gz Dice 0.8045. 57.69% complete
Jaccard: 0.6729341909945572 sensitivity : 0.8648648648648649 specificity :0.9942719940740362
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (32, 64, 64)
🛠 Test Sample 15: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_299.nii.gz Dice 0.7797. 61.54% complete
Jaccard: 0.6390060947022972 sensitivity : 0.8513429106808245 specificity :0.9916790490341754
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 16: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_259.nii.gz Dice 0.7590. 65.38% complete
Jaccard: 0.6115724618269488 sensitivity : 0.7818493150684932 specificity :0.9938524590163934
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 17: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_227.nii.gz Dice 0.8483. 69.23% complete
Jaccard: 0.7365618661257607 sensitivity : 0.8215497737556561 specificity :0.9971650917176209
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 18: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_189.nii.gz Dice 0.8798. 73.08% complete
Jaccard: 0.7853390042531899 sensitivity : 0.9085383502170767 specificity :0.9961259426039097
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 19: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_322.nii.gz Dice 0.7743. 76.92% complete
Jaccard: 0.6317663817663818 sensitivity : 0.7713043478260869 specificity :0.9949933639075415
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (34, 64, 64)
🛠 Test Sample 20: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_329.nii.gz Dice 0.8003. 80.77% complete
Jaccard: 0.6670558020079383 sensitivity : 0.7774149659863946 specificity :0.9955158604311559
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 21: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_109.nii.gz Dice 0.8421. 84.62% complete
Jaccard: 0.7272967265047519 sensitivity : 0.8558558558558559 specificity :0.9960551037528512
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 22: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_219.nii.gz Dice 0.6785. 88.46% complete
Jaccard: 0.5134870034330554 sensitivity : 0.7399293286219081 specificity :0.991608504457982
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 23: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_337.nii.gz Dice 0.8049. 92.31% complete
Jaccard: 0.6735205616850551 sensitivity : 0.8256993544420534 specificity :0.9944282302998143
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 24: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_057.nii.gz Dice 0.8399. 96.15% complete
Jaccard: 0.7240174672489083 sensitivity : 0.8968626036783267 specificity :0.9952911720144821
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 25: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_381.nii.gz Dice 0.8305. 100.00% complete
Jaccard: 0.7101449275362319 sensitivity : 0.8394670050761421 specificity :0.9956520421767059

Testing complete.
(medai) root@85308d459df9725d6ae1cfd5cbadd4e98c4702dc-d98cddf88-wsspd:/home/workspace/src#                  