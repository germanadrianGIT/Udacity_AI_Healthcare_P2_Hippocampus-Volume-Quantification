Student commands can be added to /home/workspace/.student_bashrc
(medai) root@85308d459df9725d6ae1cfd5cbadd4e98c4702dc-d98cddf88-wsspd:/home/workspace# cd src
(medai) root@85308d459df9725d6ae1cfd5cbadd4e98c4702dc-d98cddf88-wsspd:/home/workspace/src# python run_ml_pipeline.py
Loading data...
Processed 260 files, total 9198 slices
✅ hippocampus_161.nii.gz: img max=1.0000, lbl sum=5659
✅ hippocampus_360.nii.gz: img max=1.0000, lbl sum=4564
✅ hippocampus_229.nii.gz: img max=1.0000, lbl sum=4896
✅ hippocampus_102.nii.gz: img max=1.0000, lbl sum=5621
✅ hippocampus_372.nii.gz: img max=1.0000, lbl sum=5215
✅ hippocampus_097.nii.gz: img max=1.0000, lbl sum=4152
✅ hippocampus_083.nii.gz: img max=1.0000, lbl sum=5092
✅ hippocampus_370.nii.gz: img max=1.0000, lbl sum=5199
✅ hippocampus_049.nii.gz: img max=1.0000, lbl sum=5548
✅ hippocampus_157.nii.gz: img max=1.0000, lbl sum=5260
✅ hippocampus_340.nii.gz: img max=1.0000, lbl sum=4828
✅ hippocampus_136.nii.gz: img max=1.0000, lbl sum=4181
✅ hippocampus_251.nii.gz: img max=1.0000, lbl sum=4929
✅ hippocampus_004.nii.gz: img max=1.0000, lbl sum=5564
✅ hippocampus_035.nii.gz: img max=1.0000, lbl sum=5033
✅ hippocampus_327.nii.gz: img max=1.0000, lbl sum=5194
✅ hippocampus_074.nii.gz: img max=1.0000, lbl sum=4562
✅ hippocampus_037.nii.gz: img max=1.0000, lbl sum=4812
✅ hippocampus_175.nii.gz: img max=1.0000, lbl sum=3903
✅ hippocampus_163.nii.gz: img max=1.0000, lbl sum=5437
✅ hippocampus_349.nii.gz: img max=1.0000, lbl sum=3963
✅ hippocampus_104.nii.gz: img max=1.0000, lbl sum=5413
✅ hippocampus_180.nii.gz: img max=1.0000, lbl sum=3959
✅ hippocampus_068.nii.gz: img max=1.0000, lbl sum=4267
✅ hippocampus_228.nii.gz: img max=1.0000, lbl sum=5309
✅ hippocampus_091.nii.gz: img max=1.0000, lbl sum=4733
✅ hippocampus_350.nii.gz: img max=1.0000, lbl sum=4409
✅ hippocampus_210.nii.gz: img max=1.0000, lbl sum=4349
✅ hippocampus_171.nii.gz: img max=1.0000, lbl sum=5499
✅ hippocampus_165.nii.gz: img max=1.0000, lbl sum=4502
✅ hippocampus_105.nii.gz: img max=1.0000, lbl sum=4488
✅ hippocampus_152.nii.gz: img max=1.0000, lbl sum=5867
✅ hippocampus_231.nii.gz: img max=1.0000, lbl sum=4115
✅ hippocampus_176.nii.gz: img max=1.0000, lbl sum=4049
✅ hippocampus_184.nii.gz: img max=1.0000, lbl sum=5262
✅ hippocampus_352.nii.gz: img max=1.0000, lbl sum=5392
✅ hippocampus_245.nii.gz: img max=1.0000, lbl sum=5020
✅ hippocampus_221.nii.gz: img max=1.0000, lbl sum=3703
✅ hippocampus_248.nii.gz: img max=1.0000, lbl sum=4989
✅ hippocampus_300.nii.gz: img max=1.0000, lbl sum=4962
✅ hippocampus_359.nii.gz: img max=1.0000, lbl sum=4006
✅ hippocampus_050.nii.gz: img max=1.0000, lbl sum=5639
✅ hippocampus_314.nii.gz: img max=1.0000, lbl sum=4189
✅ hippocampus_368.nii.gz: img max=1.0000, lbl sum=6209
✅ hippocampus_160.nii.gz: img max=1.0000, lbl sum=4610
✅ hippocampus_343.nii.gz: img max=1.0000, lbl sum=4169
✅ hippocampus_052.nii.gz: img max=1.0000, lbl sum=4875
✅ hippocampus_205.nii.gz: img max=1.0000, lbl sum=4094
✅ hippocampus_333.nii.gz: img max=1.0000, lbl sum=4078
✅ hippocampus_138.nii.gz: img max=1.0000, lbl sum=3897
✅ hippocampus_263.nii.gz: img max=1.0000, lbl sum=5421
✅ hippocampus_249.nii.gz: img max=1.0000, lbl sum=4914
✅ hippocampus_195.nii.gz: img max=1.0000, lbl sum=5491
✅ hippocampus_132.nii.gz: img max=1.0000, lbl sum=4930
✅ hippocampus_014.nii.gz: img max=1.0000, lbl sum=5227
✅ hippocampus_320.nii.gz: img max=1.0000, lbl sum=3848
✅ hippocampus_181.nii.gz: img max=1.0000, lbl sum=5421
✅ hippocampus_166.nii.gz: img max=1.0000, lbl sum=4652
✅ hippocampus_193.nii.gz: img max=1.0000, lbl sum=4065
✅ hippocampus_008.nii.gz: img max=1.0000, lbl sum=4771
✅ hippocampus_070.nii.gz: img max=1.0000, lbl sum=5005
✅ hippocampus_367.nii.gz: img max=1.0000, lbl sum=6090
✅ hippocampus_294.nii.gz: img max=1.0000, lbl sum=4750
✅ hippocampus_389.nii.gz: img max=1.0000, lbl sum=4131
✅ hippocampus_334.nii.gz: img max=1.0000, lbl sum=4114
✅ hippocampus_345.nii.gz: img max=1.0000, lbl sum=4275
✅ hippocampus_178.nii.gz: img max=1.0000, lbl sum=4204
✅ hippocampus_025.nii.gz: img max=1.0000, lbl sum=4756
✅ hippocampus_301.nii.gz: img max=1.0000, lbl sum=5325
✅ hippocampus_303.nii.gz: img max=1.0000, lbl sum=4830
✅ hippocampus_277.nii.gz: img max=1.0000, lbl sum=4913
✅ hippocampus_335.nii.gz: img max=1.0000, lbl sum=3801
✅ hippocampus_046.nii.gz: img max=1.0000, lbl sum=4973
✅ hippocampus_378.nii.gz: img max=1.0000, lbl sum=4334
✅ hippocampus_244.nii.gz: img max=1.0000, lbl sum=4719
✅ hippocampus_172.nii.gz: img max=1.0000, lbl sum=5865
✅ hippocampus_387.nii.gz: img max=1.0000, lbl sum=4651
✅ hippocampus_290.nii.gz: img max=1.0000, lbl sum=4875
✅ hippocampus_023.nii.gz: img max=1.0000, lbl sum=5388
✅ hippocampus_099.nii.gz: img max=1.0000, lbl sum=3818
✅ hippocampus_274.nii.gz: img max=1.0000, lbl sum=3839
✅ hippocampus_144.nii.gz: img max=1.0000, lbl sum=3715
✅ hippocampus_149.nii.gz: img max=1.0000, lbl sum=4666
✅ hippocampus_386.nii.gz: img max=1.0000, lbl sum=4457
✅ hippocampus_230.nii.gz: img max=1.0000, lbl sum=4774
✅ hippocampus_289.nii.gz: img max=1.0000, lbl sum=4194
✅ hippocampus_207.nii.gz: img max=1.0000, lbl sum=6161
✅ hippocampus_296.nii.gz: img max=1.0000, lbl sum=5738
✅ hippocampus_040.nii.gz: img max=1.0000, lbl sum=4984
✅ hippocampus_060.nii.gz: img max=1.0000, lbl sum=5092
✅ hippocampus_058.nii.gz: img max=1.0000, lbl sum=4920
✅ hippocampus_297.nii.gz: img max=1.0000, lbl sum=4021
✅ hippocampus_053.nii.gz: img max=1.0000, lbl sum=5388
✅ hippocampus_114.nii.gz: img max=1.0000, lbl sum=4949
✅ hippocampus_088.nii.gz: img max=1.0000, lbl sum=5646
✅ hippocampus_338.nii.gz: img max=1.0000, lbl sum=5239
✅ hippocampus_326.nii.gz: img max=1.0000, lbl sum=5831
✅ hippocampus_217.nii.gz: img max=1.0000, lbl sum=4184
✅ hippocampus_033.nii.gz: img max=1.0000, lbl sum=4991
✅ hippocampus_325.nii.gz: img max=1.0000, lbl sum=5770
✅ hippocampus_019.nii.gz: img max=1.0000, lbl sum=4824
✅ hippocampus_150.nii.gz: img max=1.0000, lbl sum=4571
✅ hippocampus_373.nii.gz: img max=1.0000, lbl sum=5496
✅ hippocampus_048.nii.gz: img max=1.0000, lbl sum=4587
✅ hippocampus_295.nii.gz: img max=1.0000, lbl sum=5349
✅ hippocampus_375.nii.gz: img max=1.0000, lbl sum=4758
✅ hippocampus_065.nii.gz: img max=1.0000, lbl sum=5570
✅ hippocampus_133.nii.gz: img max=1.0000, lbl sum=4919
✅ hippocampus_363.nii.gz: img max=1.0000, lbl sum=5174
✅ hippocampus_328.nii.gz: img max=1.0000, lbl sum=5543
✅ hippocampus_355.nii.gz: img max=1.0000, lbl sum=4851
✅ hippocampus_093.nii.gz: img max=1.0000, lbl sum=5722
✅ hippocampus_298.nii.gz: img max=1.0000, lbl sum=3916
✅ hippocampus_268.nii.gz: img max=1.0000, lbl sum=4671
✅ hippocampus_199.nii.gz: img max=1.0000, lbl sum=3982
✅ hippocampus_155.nii.gz: img max=1.0000, lbl sum=5064
✅ hippocampus_038.nii.gz: img max=1.0000, lbl sum=5279
✅ hippocampus_185.nii.gz: img max=1.0000, lbl sum=4385
✅ hippocampus_162.nii.gz: img max=1.0000, lbl sum=5040
✅ hippocampus_064.nii.gz: img max=1.0000, lbl sum=5276
✅ hippocampus_174.nii.gz: img max=1.0000, lbl sum=6100
✅ hippocampus_374.nii.gz: img max=1.0000, lbl sum=5631
✅ hippocampus_318.nii.gz: img max=1.0000, lbl sum=5003
✅ hippocampus_330.nii.gz: img max=1.0000, lbl sum=5762
✅ hippocampus_197.nii.gz: img max=1.0000, lbl sum=4970
✅ hippocampus_286.nii.gz: img max=1.0000, lbl sum=4782
✅ hippocampus_156.nii.gz: img max=1.0000, lbl sum=5098
✅ hippocampus_164.nii.gz: img max=1.0000, lbl sum=6038
✅ hippocampus_233.nii.gz: img max=1.0000, lbl sum=4741
✅ hippocampus_044.nii.gz: img max=1.0000, lbl sum=4748
✅ hippocampus_321.nii.gz: img max=1.0000, lbl sum=4299
✅ hippocampus_261.nii.gz: img max=1.0000, lbl sum=5743
✅ hippocampus_020.nii.gz: img max=1.0000, lbl sum=5076
✅ hippocampus_089.nii.gz: img max=1.0000, lbl sum=5647
✅ hippocampus_226.nii.gz: img max=1.0000, lbl sum=3728
✅ hippocampus_123.nii.gz: img max=1.0000, lbl sum=4746
✅ hippocampus_146.nii.gz: img max=1.0000, lbl sum=5027
✅ hippocampus_216.nii.gz: img max=1.0000, lbl sum=5002
✅ hippocampus_356.nii.gz: img max=1.0000, lbl sum=5129
✅ hippocampus_235.nii.gz: img max=1.0000, lbl sum=4601
✅ hippocampus_015.nii.gz: img max=1.0000, lbl sum=4127
✅ hippocampus_051.nii.gz: img max=1.0000, lbl sum=4697
✅ hippocampus_331.nii.gz: img max=1.0000, lbl sum=4827
✅ hippocampus_390.nii.gz: img max=1.0000, lbl sum=4618
✅ hippocampus_125.nii.gz: img max=1.0000, lbl sum=3795
✅ hippocampus_173.nii.gz: img max=1.0000, lbl sum=5575
✅ hippocampus_279.nii.gz: img max=1.0000, lbl sum=3503
✅ hippocampus_224.nii.gz: img max=1.0000, lbl sum=5607
✅ hippocampus_148.nii.gz: img max=1.0000, lbl sum=4201
✅ hippocampus_234.nii.gz: img max=1.0000, lbl sum=4883
✅ hippocampus_024.nii.gz: img max=1.0000, lbl sum=6049
✅ hippocampus_101.nii.gz: img max=1.0000, lbl sum=5300
✅ hippocampus_264.nii.gz: img max=1.0000, lbl sum=5750
✅ hippocampus_135.nii.gz: img max=1.0000, lbl sum=3895
✅ hippocampus_084.nii.gz: img max=1.0000, lbl sum=4910
✅ hippocampus_232.nii.gz: img max=1.0000, lbl sum=4212
✅ hippocampus_194.nii.gz: img max=1.0000, lbl sum=4427
✅ hippocampus_257.nii.gz: img max=1.0000, lbl sum=4538
✅ hippocampus_309.nii.gz: img max=1.0000, lbl sum=5442
✅ hippocampus_276.nii.gz: img max=1.0000, lbl sum=5230
✅ hippocampus_127.nii.gz: img max=1.0000, lbl sum=5404
✅ hippocampus_039.nii.gz: img max=1.0000, lbl sum=5278
✅ hippocampus_385.nii.gz: img max=1.0000, lbl sum=4681
✅ hippocampus_287.nii.gz: img max=1.0000, lbl sum=5634
✅ hippocampus_336.nii.gz: img max=1.0000, lbl sum=3888
✅ hippocampus_177.nii.gz: img max=1.0000, lbl sum=4132
✅ hippocampus_260.nii.gz: img max=1.0000, lbl sum=4815
✅ hippocampus_305.nii.gz: img max=1.0000, lbl sum=4409
✅ hippocampus_042.nii.gz: img max=1.0000, lbl sum=5823
✅ hippocampus_282.nii.gz: img max=1.0000, lbl sum=3699
✅ hippocampus_141.nii.gz: img max=1.0000, lbl sum=4193
✅ hippocampus_358.nii.gz: img max=1.0000, lbl sum=4255
✅ hippocampus_383.nii.gz: img max=1.0000, lbl sum=5063
✅ hippocampus_242.nii.gz: img max=1.0000, lbl sum=6289
✅ hippocampus_075.nii.gz: img max=1.0000, lbl sum=4874
✅ hippocampus_090.nii.gz: img max=1.0000, lbl sum=5958
✅ hippocampus_204.nii.gz: img max=1.0000, lbl sum=4104
✅ hippocampus_311.nii.gz: img max=1.0000, lbl sum=4837
✅ hippocampus_095.nii.gz: img max=1.0000, lbl sum=5379
✅ hippocampus_092.nii.gz: img max=1.0000, lbl sum=4796
✅ hippocampus_366.nii.gz: img max=1.0000, lbl sum=5476
✅ hippocampus_108.nii.gz: img max=1.0000, lbl sum=5821
✅ hippocampus_265.nii.gz: img max=1.0000, lbl sum=4763
✅ hippocampus_034.nii.gz: img max=1.0000, lbl sum=4920
✅ hippocampus_252.nii.gz: img max=1.0000, lbl sum=5090
✅ hippocampus_011.nii.gz: img max=1.0000, lbl sum=5000
✅ hippocampus_292.nii.gz: img max=1.0000, lbl sum=5047
✅ hippocampus_376.nii.gz: img max=1.0000, lbl sum=5537
✅ hippocampus_332.nii.gz: img max=1.0000, lbl sum=4851
✅ hippocampus_124.nii.gz: img max=1.0000, lbl sum=4668
✅ hippocampus_308.nii.gz: img max=1.0000, lbl sum=5297
✅ hippocampus_094.nii.gz: img max=1.0000, lbl sum=5696
✅ hippocampus_077.nii.gz: img max=1.0000, lbl sum=5452
✅ hippocampus_017.nii.gz: img max=1.0000, lbl sum=4821
✅ hippocampus_106.nii.gz: img max=1.0000, lbl sum=4472
✅ hippocampus_001.nii.gz: img max=1.0000, lbl sum=4572
✅ hippocampus_236.nii.gz: img max=1.0000, lbl sum=4583
✅ hippocampus_170.nii.gz: img max=1.0000, lbl sum=4429
✅ hippocampus_319.nii.gz: img max=1.0000, lbl sum=3464
✅ hippocampus_154.nii.gz: img max=1.0000, lbl sum=4915
✅ hippocampus_142.nii.gz: img max=1.0000, lbl sum=4072
✅ hippocampus_188.nii.gz: img max=1.0000, lbl sum=5302
✅ hippocampus_253.nii.gz: img max=1.0000, lbl sum=5303
✅ hippocampus_304.nii.gz: img max=1.0000, lbl sum=5056
✅ hippocampus_145.nii.gz: img max=1.0000, lbl sum=4998
✅ hippocampus_190.nii.gz: img max=1.0000, lbl sum=4953
✅ hippocampus_126.nii.gz: img max=1.0000, lbl sum=4640
✅ hippocampus_299.nii.gz: img max=1.0000, lbl sum=4887
✅ hippocampus_036.nii.gz: img max=1.0000, lbl sum=5169
✅ hippocampus_220.nii.gz: img max=1.0000, lbl sum=4010
✅ hippocampus_316.nii.gz: img max=1.0000, lbl sum=4610
✅ hippocampus_215.nii.gz: img max=1.0000, lbl sum=5066
✅ hippocampus_169.nii.gz: img max=1.0000, lbl sum=4355
✅ hippocampus_041.nii.gz: img max=1.0000, lbl sum=5749
✅ hippocampus_259.nii.gz: img max=1.0000, lbl sum=4502
✅ hippocampus_393.nii.gz: img max=1.0000, lbl sum=5500
✅ hippocampus_227.nii.gz: img max=1.0000, lbl sum=5093
✅ hippocampus_361.nii.gz: img max=1.0000, lbl sum=4477
✅ hippocampus_087.nii.gz: img max=1.0000, lbl sum=5481
✅ hippocampus_269.nii.gz: img max=1.0000, lbl sum=5153
✅ hippocampus_189.nii.gz: img max=1.0000, lbl sum=5129
✅ hippocampus_341.nii.gz: img max=1.0000, lbl sum=3845
✅ hippocampus_243.nii.gz: img max=1.0000, lbl sum=4491
✅ hippocampus_212.nii.gz: img max=1.0000, lbl sum=5418
✅ hippocampus_098.nii.gz: img max=1.0000, lbl sum=4265
✅ hippocampus_225.nii.gz: img max=1.0000, lbl sum=3831
✅ hippocampus_322.nii.gz: img max=1.0000, lbl sum=4921
✅ hippocampus_351.nii.gz: img max=1.0000, lbl sum=5626
✅ hippocampus_158.nii.gz: img max=1.0000, lbl sum=4950
✅ hippocampus_203.nii.gz: img max=1.0000, lbl sum=4128
✅ hippocampus_045.nii.gz: img max=1.0000, lbl sum=4490
✅ hippocampus_288.nii.gz: img max=1.0000, lbl sum=5840
✅ hippocampus_107.nii.gz: img max=1.0000, lbl sum=5981
✅ hippocampus_302.nii.gz: img max=1.0000, lbl sum=5503
✅ hippocampus_310.nii.gz: img max=1.0000, lbl sum=5723
✅ hippocampus_280.nii.gz: img max=1.0000, lbl sum=3859
✅ hippocampus_329.nii.gz: img max=1.0000, lbl sum=5413
✅ hippocampus_007.nii.gz: img max=1.0000, lbl sum=4902
✅ hippocampus_096.nii.gz: img max=1.0000, lbl sum=4790
✅ hippocampus_109.nii.gz: img max=1.0000, lbl sum=4849
✅ hippocampus_222.nii.gz: img max=1.0000, lbl sum=3677
✅ hippocampus_003.nii.gz: img max=1.0000, lbl sum=5156
✅ hippocampus_026.nii.gz: img max=1.0000, lbl sum=5393
✅ hippocampus_219.nii.gz: img max=1.0000, lbl sum=4122
✅ hippocampus_337.nii.gz: img max=1.0000, lbl sum=4955
✅ hippocampus_067.nii.gz: img max=1.0000, lbl sum=4101
✅ hippocampus_143.nii.gz: img max=1.0000, lbl sum=3601
✅ hippocampus_057.nii.gz: img max=1.0000, lbl sum=4156
✅ hippocampus_380.nii.gz: img max=1.0000, lbl sum=5140
✅ hippocampus_238.nii.gz: img max=1.0000, lbl sum=5672
✅ hippocampus_381.nii.gz: img max=1.0000, lbl sum=4754
✅ hippocampus_353.nii.gz: img max=1.0000, lbl sum=4095
✅ hippocampus_130.nii.gz: img max=1.0000, lbl sum=4865
✅ hippocampus_317.nii.gz: img max=1.0000, lbl sum=4914
✅ hippocampus_056.nii.gz: img max=1.0000, lbl sum=5501
✅ hippocampus_250.nii.gz: img max=1.0000, lbl sum=4963
✅ hippocampus_354.nii.gz: img max=1.0000, lbl sum=4295
✅ hippocampus_223.nii.gz: img max=1.0000, lbl sum=5427
✅ hippocampus_394.nii.gz: img max=1.0000, lbl sum=5641
✅ hippocampus_006.nii.gz: img max=1.0000, lbl sum=6212
Number of files in train, validation and test set are :   208 26 26
SPLIT STRUCTURE:
train: type=<class 'numpy.ndarray'>, len=208
val: type=<class 'list'>, len=26
test: type=<class 'list'>, len=26
Experiment started.
Training epoch 0...
/opt/conda/conda-bld/pytorch_1587428094786/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.

Epoch: 0 Train loss: 0.9840248227119446, 0.0% complete
..........
Epoch: 0 Train loss: 0.9663288593292236, 0.3% complete
..........
Epoch: 0 Train loss: 0.9281108379364014, 0.6% complete
..........
Epoch: 0 Train loss: 0.8835903406143188, 0.8% complete
..........
Epoch: 0 Train loss: 0.817402720451355, 1.1% complete
..........
Epoch: 0 Train loss: 0.6644402742385864, 1.4% complete
..........
Epoch: 0 Train loss: 0.6358108520507812, 1.7% complete
..........
Epoch: 0 Train loss: 0.5525268912315369, 1.9% complete
..........
Epoch: 0 Train loss: 0.589169979095459, 2.2% complete
..........
Epoch: 0 Train loss: 0.4432227909564972, 2.5% complete
..........
Epoch: 0 Train loss: 0.42968618869781494, 2.7% complete
..........
Epoch: 0 Train loss: 0.1327887773513794, 3.0% complete
..........
Epoch: 0 Train loss: 0.24554049968719482, 3.3% complete
..........
Epoch: 0 Train loss: 0.015591642819344997, 3.6% complete
..........
Epoch: 0 Train loss: 0.030056443065404892, 3.8% complete
..........
Epoch: 0 Train loss: 0.0367165170609951, 4.1% complete
..........
Epoch: 0 Train loss: 0.09786354750394821, 4.4% complete
..........
Epoch: 0 Train loss: 0.016135811805725098, 4.6% complete
..........
Epoch: 0 Train loss: 0.10790810734033585, 4.9% complete
..........
Epoch: 0 Train loss: 0.09049719572067261, 5.2% complete
..........
Epoch: 0 Train loss: 0.09359704703092575, 5.5% complete
..........
Epoch: 0 Train loss: 0.009205733425915241, 5.7% complete
..........
Epoch: 0 Train loss: 0.007887872867286205, 6.0% complete
..........
Epoch: 0 Train loss: 0.1052025854587555, 6.3% complete
..........
Epoch: 0 Train loss: 0.030014783143997192, 6.5% complete
..........
Epoch: 0 Train loss: 0.014177021570503712, 6.8% complete
..........
Epoch: 0 Train loss: 0.12404613941907883, 7.1% complete
..........
Epoch: 0 Train loss: 0.0835045725107193, 7.4% complete
..........
Epoch: 0 Train loss: 0.03455936163663864, 7.6% complete
..........
Epoch: 0 Train loss: 0.06582657247781754, 7.9% complete
..........
Epoch: 0 Train loss: 0.07446929812431335, 8.2% complete
..........
Epoch: 0 Train loss: 0.036406759172677994, 8.4% complete
..........
Epoch: 0 Train loss: 0.020922712981700897, 8.7% complete
..........
Epoch: 0 Train loss: 0.004036775324493647, 9.0% complete
..........
Epoch: 0 Train loss: 0.05213845521211624, 9.3% complete
..........
Epoch: 0 Train loss: 0.020618222653865814, 9.5% complete
..........
Epoch: 0 Train loss: 0.014011934399604797, 9.8% complete
..........
Epoch: 0 Train loss: 0.0562630333006382, 10.1% complete
..........
Epoch: 0 Train loss: 0.012316248379647732, 10.3% complete
..........
Epoch: 0 Train loss: 0.02244720607995987, 10.6% complete
..........
Epoch: 0 Train loss: 0.07977040112018585, 10.9% complete
..........
Epoch: 0 Train loss: 0.04801281541585922, 11.2% complete
..........
Epoch: 0 Train loss: 0.017472678795456886, 11.4% complete
..........
Epoch: 0 Train loss: 0.09761673957109451, 11.7% complete
..........
Epoch: 0 Train loss: 0.09064825624227524, 12.0% complete
..........
Epoch: 0 Train loss: 0.008094370365142822, 12.2% complete
..........
Epoch: 0 Train loss: 0.08718632906675339, 12.5% complete
..........
Epoch: 0 Train loss: 0.014988292008638382, 12.8% complete
..........
Epoch: 0 Train loss: 0.14770002663135529, 13.1% complete
..........
Epoch: 0 Train loss: 0.017080068588256836, 13.3% complete
..........
Epoch: 0 Train loss: 0.08655549585819244, 13.6% complete
..........
Epoch: 0 Train loss: 0.08932451903820038, 13.9% complete
..........
Epoch: 0 Train loss: 0.06723446398973465, 14.1% complete
..........
Epoch: 0 Train loss: 0.031033016741275787, 14.4% complete
..........
Epoch: 0 Train loss: 0.01910233125090599, 14.7% complete
..........
Epoch: 0 Train loss: 0.004760025069117546, 15.0% complete
..........
Epoch: 0 Train loss: 0.0063082207925617695, 15.2% complete
..........
Epoch: 0 Train loss: 0.012034640647470951, 15.5% complete
..........
Epoch: 0 Train loss: 0.006120556034147739, 15.8% complete
..........
Epoch: 0 Train loss: 0.009565752930939198, 16.1% complete
..........
Epoch: 0 Train loss: 0.03339311480522156, 16.3% complete
..........
Epoch: 0 Train loss: 0.0146408062428236, 16.6% complete
..........
Epoch: 0 Train loss: 0.007646671496331692, 16.9% complete
..........
Epoch: 0 Train loss: 0.006197030656039715, 17.1% complete
..........
Epoch: 0 Train loss: 0.04044054448604584, 17.4% complete
..........
Epoch: 0 Train loss: 0.09823955595493317, 17.7% complete
..........
Epoch: 0 Train loss: 0.09259513765573502, 18.0% complete
..........
Epoch: 0 Train loss: 0.053892508149147034, 18.2% complete
..........
Epoch: 0 Train loss: 0.007133238483220339, 18.5% complete
..........
Epoch: 0 Train loss: 0.07215274125337601, 18.8% complete
..........
Epoch: 0 Train loss: 0.0024449878837913275, 19.0% complete
..........
Epoch: 0 Train loss: 0.05227939784526825, 19.3% complete
..........
Epoch: 0 Train loss: 0.006190834566950798, 19.6% complete
..........
Epoch: 0 Train loss: 0.0419645756483078, 19.9% complete
..........
Epoch: 0 Train loss: 0.0012107549700886011, 20.1% complete
..........
Epoch: 0 Train loss: 0.0029929897282272577, 20.4% complete
..........
Epoch: 0 Train loss: 0.05221571400761604, 20.7% complete
..........
Epoch: 0 Train loss: 0.043466608971357346, 20.9% complete
..........
Epoch: 0 Train loss: 0.04999876767396927, 21.2% complete
..........
Epoch: 0 Train loss: 0.0815441757440567, 21.5% complete
..........
Epoch: 0 Train loss: 0.05712432414293289, 21.8% complete
..........
Epoch: 0 Train loss: 0.07834893465042114, 22.0% complete
..........
Epoch: 0 Train loss: 0.08053622394800186, 22.3% complete
..........
Epoch: 0 Train loss: 0.00047591148177161813, 22.6% complete
..........
Epoch: 0 Train loss: 0.017251281067728996, 22.8% complete
..........
Epoch: 0 Train loss: 0.019559577107429504, 23.1% complete
..........
Epoch: 0 Train loss: 0.00020070147002115846, 23.4% complete
..........
Epoch: 0 Train loss: 0.11392293870449066, 23.7% complete
..........
Epoch: 0 Train loss: 0.02951456606388092, 23.9% complete
..........
Epoch: 0 Train loss: 0.0314096137881279, 24.2% complete
..........
Epoch: 0 Train loss: 0.062457725405693054, 24.5% complete
..........
Epoch: 0 Train loss: 0.0714421421289444, 24.7% complete
..........
Epoch: 0 Train loss: 0.009673616848886013, 25.0% complete
..........
Epoch: 0 Train loss: 0.000983519246801734, 25.3% complete
..........
Epoch: 0 Train loss: 0.06567921489477158, 25.6% complete
..........
Epoch: 0 Train loss: 0.0010561643866822124, 25.8% complete
..........
Epoch: 0 Train loss: 0.0007522760424762964, 26.1% complete
..........
Epoch: 0 Train loss: 0.0614391528069973, 26.4% complete
..........
Epoch: 0 Train loss: 0.0006644849199801683, 26.6% complete
..........
Epoch: 0 Train loss: 0.05039989948272705, 26.9% complete
..........
Epoch: 0 Train loss: 0.0010005055228248239, 27.2% complete
..........
Epoch: 0 Train loss: 0.0002242893970105797, 27.5% complete
..........
Epoch: 0 Train loss: 0.0006682315142825246, 27.7% complete
..........
Epoch: 0 Train loss: 0.0654158741235733, 28.0% complete
..........
Epoch: 0 Train loss: 0.034294068813323975, 28.3% complete
..........
Epoch: 0 Train loss: 0.04420267418026924, 28.5% complete
..........
Epoch: 0 Train loss: 0.0008769959094934165, 28.8% complete
..........
Epoch: 0 Train loss: 0.016232850030064583, 29.1% complete
..........
Epoch: 0 Train loss: 0.0002999691932927817, 29.4% complete
..........
Epoch: 0 Train loss: 0.03918534144759178, 29.6% complete
..........
Epoch: 0 Train loss: 0.06944097578525543, 29.9% complete
..........
Epoch: 0 Train loss: 0.001808726112358272, 30.2% complete
..........
Epoch: 0 Train loss: 0.001318206312134862, 30.4% complete
..........
Epoch: 0 Train loss: 0.07051687687635422, 30.7% complete
..........
Epoch: 0 Train loss: 0.014969886280596256, 31.0% complete
..........
Epoch: 0 Train loss: 0.06435635685920715, 31.3% complete
..........
Epoch: 0 Train loss: 0.050340764224529266, 31.5% complete
..........
Epoch: 0 Train loss: 0.04413120076060295, 31.8% complete
..........
Epoch: 0 Train loss: 0.0006813807412981987, 32.1% complete
..........
Epoch: 0 Train loss: 0.0006383847212418914, 32.3% complete
..........
Epoch: 0 Train loss: 0.0285127405077219, 32.6% complete
..........
Epoch: 0 Train loss: 0.04114704951643944, 32.9% complete
..........
Epoch: 0 Train loss: 0.07059084624052048, 33.2% complete
..........
Epoch: 0 Train loss: 0.048703182488679886, 33.4% complete
..........
Epoch: 0 Train loss: 0.06834065914154053, 33.7% complete
..........
Epoch: 0 Train loss: 0.0010367012582719326, 34.0% complete
..........
Epoch: 0 Train loss: 0.039599400013685226, 34.2% complete
..........
Epoch: 0 Train loss: 0.004559840075671673, 34.5% complete
..........
Epoch: 0 Train loss: 0.0004117276694159955, 34.8% complete
..........
Epoch: 0 Train loss: 0.0001718899584375322, 35.1% complete
..........
Epoch: 0 Train loss: 0.03211253136396408, 35.3% complete
..........
Epoch: 0 Train loss: 7.518852362409234e-05, 35.6% complete
..........
Epoch: 0 Train loss: 0.02517816424369812, 35.9% complete
..........
Epoch: 0 Train loss: 0.0005687879165634513, 36.1% complete
..........
Epoch: 0 Train loss: 0.051010213792324066, 36.4% complete
..........
Epoch: 0 Train loss: 0.051697999238967896, 36.7% complete
..........
Epoch: 0 Train loss: 0.05189364776015282, 37.0% complete
..........
Epoch: 0 Train loss: 0.02453208900988102, 37.2% complete
..........
Epoch: 0 Train loss: 0.000365287356544286, 37.5% complete
..........
Epoch: 0 Train loss: 0.0042852796614170074, 37.8% complete
..........
Epoch: 0 Train loss: 0.0008035151986405253, 38.0% complete
..........
Epoch: 0 Train loss: 0.03110709972679615, 38.3% complete
..........
Epoch: 0 Train loss: 0.05013676732778549, 38.6% complete
..........
Epoch: 0 Train loss: 0.05245917662978172, 38.9% complete
..........
Epoch: 0 Train loss: 0.027724821120500565, 39.1% complete
..........
Epoch: 0 Train loss: 5.1852199248969555e-05, 39.4% complete
..........
Epoch: 0 Train loss: 0.05560971796512604, 39.7% complete
..........
Epoch: 0 Train loss: 0.012193836271762848, 40.0% complete
..........
Epoch: 0 Train loss: 0.04288136214017868, 40.2% complete
..........
Epoch: 0 Train loss: 0.05139878764748573, 40.5% complete
..........
Epoch: 0 Train loss: 0.007573903538286686, 40.8% complete
..........
Epoch: 0 Train loss: 0.00020666062482632697, 41.0% complete
..........
Epoch: 0 Train loss: 0.013529646210372448, 41.3% complete
..........
Epoch: 0 Train loss: 0.044435422867536545, 41.6% complete
..........
Epoch: 0 Train loss: 0.04935607314109802, 41.9% complete
..........
Epoch: 0 Train loss: 0.0034086776431649923, 42.1% complete
..........
Epoch: 0 Train loss: 0.04936586692929268, 42.4% complete
..........
Epoch: 0 Train loss: 0.01622265949845314, 42.7% complete
..........
Epoch: 0 Train loss: 0.03690733760595322, 42.9% complete
..........
Epoch: 0 Train loss: 0.016821546480059624, 43.2% complete
..........
Epoch: 0 Train loss: 0.041862890124320984, 43.5% complete
..........
Epoch: 0 Train loss: 0.034838635474443436, 43.8% complete
..........
Epoch: 0 Train loss: 0.061852335929870605, 44.0% complete
..........
Epoch: 0 Train loss: 0.03784096986055374, 44.3% complete
..........
Epoch: 0 Train loss: 0.0007018112810328603, 44.6% complete
..........
Epoch: 0 Train loss: 0.07142029702663422, 44.8% complete
..........
Epoch: 0 Train loss: 0.0002519127447158098, 45.1% complete
..........
Epoch: 0 Train loss: 0.020777050405740738, 45.4% complete
..........
Epoch: 0 Train loss: 0.012504752725362778, 45.7% complete
..........
Epoch: 0 Train loss: 0.00031789205968379974, 45.9% complete
..........
Epoch: 0 Train loss: 0.0004874922160524875, 46.2% complete
..........
Epoch: 0 Train loss: 0.0221992377191782, 46.5% complete
..........
Epoch: 0 Train loss: 0.04130137339234352, 46.7% complete
..........
Epoch: 0 Train loss: 0.00020215995027683675, 47.0% complete
..........
Epoch: 0 Train loss: 0.061449553817510605, 47.3% complete
..........
Epoch: 0 Train loss: 0.003515775315463543, 47.6% complete
..........
Epoch: 0 Train loss: 0.03718159720301628, 47.8% complete
..........
Epoch: 0 Train loss: 0.01722540892660618, 48.1% complete
..........
Epoch: 0 Train loss: 0.043654292821884155, 48.4% complete
..........
Epoch: 0 Train loss: 0.04060787707567215, 48.6% complete
..........
Epoch: 0 Train loss: 0.05879814550280571, 48.9% complete
..........
Epoch: 0 Train loss: 0.016412613913416862, 49.2% complete
..........
Epoch: 0 Train loss: 0.04971034452319145, 49.5% complete
..........
Epoch: 0 Train loss: 0.0012197453761473298, 49.7% complete
..........
Epoch: 0 Train loss: 0.047154854983091354, 50.0% complete
..........
Epoch: 0 Train loss: 4.426375380717218e-05, 50.3% complete
..........
Epoch: 0 Train loss: 0.043829210102558136, 50.5% complete
..........
Epoch: 0 Train loss: 0.0564056858420372, 50.8% complete
..........
Epoch: 0 Train loss: 0.02370080165565014, 51.1% complete
..........
Epoch: 0 Train loss: 0.00627165799960494, 51.4% complete
..........
Epoch: 0 Train loss: 0.05155966803431511, 51.6% complete
..........
Epoch: 0 Train loss: 0.0026033225003629923, 51.9% complete
..........
Epoch: 0 Train loss: 0.0005382988601922989, 52.2% complete
..........
Epoch: 0 Train loss: 0.0033403760753571987, 52.4% complete
..........
Epoch: 0 Train loss: 0.01701973006129265, 52.7% complete
..........
Epoch: 0 Train loss: 0.0002792706072796136, 53.0% complete
..........
Epoch: 0 Train loss: 0.009730860590934753, 53.3% complete
..........
Epoch: 0 Train loss: 0.11458169668912888, 53.5% complete
..........
Epoch: 0 Train loss: 0.02237650752067566, 53.8% complete
..........
Epoch: 0 Train loss: 0.001907791243866086, 54.1% complete
..........
Epoch: 0 Train loss: 0.029078692197799683, 54.3% complete
..........
Epoch: 0 Train loss: 0.005306066945195198, 54.6% complete
..........
Epoch: 0 Train loss: 0.00045992585364729166, 54.9% complete
..........
Epoch: 0 Train loss: 0.008995380252599716, 55.2% complete
..........
Epoch: 0 Train loss: 0.0004039197228848934, 55.4% complete
..........
Epoch: 0 Train loss: 0.0003006374463438988, 55.7% complete
..........
Epoch: 0 Train loss: 0.05855696275830269, 56.0% complete
..........
Epoch: 0 Train loss: 0.04067322984337807, 56.2% complete
..........
Epoch: 0 Train loss: 0.042681608349084854, 56.5% complete
..........
Epoch: 0 Train loss: 0.0355638787150383, 56.8% complete
..........
Epoch: 0 Train loss: 0.03295718505978584, 57.1% complete
..........
Epoch: 0 Train loss: 0.028449948877096176, 57.3% complete
..........
Epoch: 0 Train loss: 0.04505693167448044, 57.6% complete
..........
Epoch: 0 Train loss: 0.00033188480301760137, 57.9% complete
..........
Epoch: 0 Train loss: 0.04195864498615265, 58.1% complete
..........
Epoch: 0 Train loss: 0.00408094422891736, 58.4% complete
..........
Epoch: 0 Train loss: 0.02550181932747364, 58.7% complete
..........
Epoch: 0 Train loss: 0.02592509798705578, 59.0% complete
..........
Epoch: 0 Train loss: 0.0002979290729854256, 59.2% complete
..........
Epoch: 0 Train loss: 0.004855873994529247, 59.5% complete
..........
Epoch: 0 Train loss: 0.041175663471221924, 59.8% complete
..........
Epoch: 0 Train loss: 7.959495997056365e-05, 60.0% complete
..........
Epoch: 0 Train loss: 0.0030727111734449863, 60.3% complete
..........
Epoch: 0 Train loss: 0.04836686700582504, 60.6% complete
..........
Epoch: 0 Train loss: 0.030062701553106308, 60.9% complete
..........
Epoch: 0 Train loss: 0.03786732256412506, 61.1% complete
..........
Epoch: 0 Train loss: 0.010342593304812908, 61.4% complete
..........
Epoch: 0 Train loss: 0.052821606397628784, 61.7% complete
..........
Epoch: 0 Train loss: 0.011157139204442501, 62.0% complete
..........
Epoch: 0 Train loss: 0.036699213087558746, 62.2% complete
..........
Epoch: 0 Train loss: 0.029385816305875778, 62.5% complete
..........
Epoch: 0 Train loss: 0.0565330944955349, 62.8% complete
..........
Epoch: 0 Train loss: 0.000410488253692165, 63.0% complete
..........
Epoch: 0 Train loss: 0.0024364169221371412, 63.3% complete
..........
Epoch: 0 Train loss: 4.343359614722431e-05, 63.6% complete
..........
Epoch: 0 Train loss: 0.039092909544706345, 63.9% complete
..........
Epoch: 0 Train loss: 0.021019570529460907, 64.1% complete
..........
Epoch: 0 Train loss: 0.0007127051940187812, 64.4% complete
..........
Epoch: 0 Train loss: 0.04014338552951813, 64.7% complete
..........
Epoch: 0 Train loss: 9.701959788799286e-05, 64.9% complete
..........
Epoch: 0 Train loss: 0.026265962049365044, 65.2% complete
..........
Epoch: 0 Train loss: 0.009941793978214264, 65.5% complete
..........
Epoch: 0 Train loss: 0.0015524725895375013, 65.8% complete
..........
Epoch: 0 Train loss: 0.048228759318590164, 66.0% complete
..........
Epoch: 0 Train loss: 0.01989559642970562, 66.3% complete
..........
Epoch: 0 Train loss: 0.0005637696012854576, 66.6% complete
..........
Epoch: 0 Train loss: 0.03123108297586441, 66.8% complete
..........
Epoch: 0 Train loss: 0.016366908326745033, 67.1% complete
..........
Epoch: 0 Train loss: 0.024870799854397774, 67.4% complete
..........
Epoch: 0 Train loss: 0.04768292233347893, 67.7% complete
..........
Epoch: 0 Train loss: 0.048067327588796616, 67.9% complete
..........
Epoch: 0 Train loss: 0.05282864347100258, 68.2% complete
..........
Epoch: 0 Train loss: 0.04681256413459778, 68.5% complete
..........
Epoch: 0 Train loss: 0.00019047607202082872, 68.7% complete
..........
Epoch: 0 Train loss: 0.04732261970639229, 69.0% complete
..........
Epoch: 0 Train loss: 0.02378154546022415, 69.3% complete
..........
Epoch: 0 Train loss: 7.194219506345689e-05, 69.6% complete
..........
Epoch: 0 Train loss: 3.3543125027790666e-05, 69.8% complete
..........
Epoch: 0 Train loss: 0.033335164189338684, 70.1% complete
..........
Epoch: 0 Train loss: 0.03868895396590233, 70.4% complete
..........
Epoch: 0 Train loss: 0.03807981684803963, 70.6% complete
..........
Epoch: 0 Train loss: 7.285110768862069e-05, 70.9% complete
..........
Epoch: 0 Train loss: 0.03883247449994087, 71.2% complete
..........
Epoch: 0 Train loss: 0.0010065098758786917, 71.5% complete
..........
Epoch: 0 Train loss: 0.0004606065631378442, 71.7% complete
..........
Epoch: 0 Train loss: 0.0035911104641854763, 72.0% complete
..........
Epoch: 0 Train loss: 0.07395398616790771, 72.3% complete
..........
Epoch: 0 Train loss: 0.0011862386018037796, 72.5% complete
..........
Epoch: 0 Train loss: 3.853769158013165e-05, 72.8% complete
..........
Epoch: 0 Train loss: 0.021416956558823586, 73.1% complete
..........
Epoch: 0 Train loss: 0.038803767412900925, 73.4% complete
..........
Epoch: 0 Train loss: 0.026066068559885025, 73.6% complete
..........
Epoch: 0 Train loss: 0.023551607504487038, 73.9% complete
..........
Epoch: 0 Train loss: 0.04187604784965515, 74.2% complete
..........
Epoch: 0 Train loss: 0.00024499581195414066, 74.4% complete
..........
Epoch: 0 Train loss: 0.03503132611513138, 74.7% complete
..........
Epoch: 0 Train loss: 0.00028382460004650056, 75.0% complete
..........
Epoch: 0 Train loss: 0.0006526661454699934, 75.3% complete
..........
Epoch: 0 Train loss: 4.736401024274528e-05, 75.5% complete
..........
Epoch: 0 Train loss: 0.030683401972055435, 75.8% complete
..........
Epoch: 0 Train loss: 0.0006381525890901685, 76.1% complete
..........
Epoch: 0 Train loss: 0.030364444479346275, 76.3% complete
..........
Epoch: 0 Train loss: 5.416499334387481e-05, 76.6% complete
..........
Epoch: 0 Train loss: 1.8377875676378608e-05, 76.9% complete
..........
Epoch: 0 Train loss: 0.00028803388704545796, 77.2% complete
..........
Epoch: 0 Train loss: 0.009632040746510029, 77.4% complete
..........
Epoch: 0 Train loss: 1.4961318811401725e-05, 77.7% complete
..........
Epoch: 0 Train loss: 0.024809563532471657, 78.0% complete
..........
Epoch: 0 Train loss: 0.017025036737322807, 78.2% complete
..........
Epoch: 0 Train loss: 0.016567211598157883, 78.5% complete
..........
Epoch: 0 Train loss: 0.032721519470214844, 78.8% complete
..........
Epoch: 0 Train loss: 0.048611316829919815, 79.1% complete
..........
Epoch: 0 Train loss: 0.04112651199102402, 79.3% complete
..........
Epoch: 0 Train loss: 0.044567640870809555, 79.6% complete
..........
Epoch: 0 Train loss: 0.03761542588472366, 79.9% complete
..........
Epoch: 0 Train loss: 0.04167830944061279, 80.1% complete
..........
Epoch: 0 Train loss: 0.00024609456886537373, 80.4% complete
..........
Epoch: 0 Train loss: 0.030570466071367264, 80.7% complete
..........
Epoch: 0 Train loss: 0.03597152605652809, 81.0% complete
..........
Epoch: 0 Train loss: 4.383319173939526e-05, 81.2% complete
..........
Epoch: 0 Train loss: 0.001099754823371768, 81.5% complete
..........
Epoch: 0 Train loss: 0.06975191086530685, 81.8% complete
..........
Epoch: 0 Train loss: 0.04278317093849182, 82.0% complete
..........
Epoch: 0 Train loss: 0.0059926011599600315, 82.3% complete
..........
Epoch: 0 Train loss: 0.03402193263173103, 82.6% complete
..........
Epoch: 0 Train loss: 0.0006750901811756194, 82.9% complete
..........
Epoch: 0 Train loss: 0.00010535953333601356, 83.1% complete
..........
Epoch: 0 Train loss: 0.03431762009859085, 83.4% complete
..........
Epoch: 0 Train loss: 0.011695807799696922, 83.7% complete
..........
Epoch: 0 Train loss: 0.04777280613780022, 83.9% complete
..........
Epoch: 0 Train loss: 1.716107362881303e-05, 84.2% complete
..........
Epoch: 0 Train loss: 0.011901079677045345, 84.5% complete
..........
Epoch: 0 Train loss: 0.037542592734098434, 84.8% complete
..........
Epoch: 0 Train loss: 0.00011927966261282563, 85.0% complete
..........
Epoch: 0 Train loss: 6.755441427230835e-05, 85.3% complete
..........
Epoch: 0 Train loss: 0.02340543083846569, 85.6% complete
..........
Epoch: 0 Train loss: 0.002073626732453704, 85.9% complete
..........
Epoch: 0 Train loss: 0.028073444962501526, 86.1% complete
..........
Epoch: 0 Train loss: 0.032956916838884354, 86.4% complete
..........
Epoch: 0 Train loss: 0.0017584788147360086, 86.7% complete
..........
Epoch: 0 Train loss: 1.162756234407425e-05, 86.9% complete
..........
Epoch: 0 Train loss: 0.00021712316083721817, 87.2% complete
..........
Epoch: 0 Train loss: 0.0002670771791599691, 87.5% complete
..........
Epoch: 0 Train loss: 0.05059678480029106, 87.8% complete
..........
Epoch: 0 Train loss: 0.0016284511657431722, 88.0% complete
..........
Epoch: 0 Train loss: 0.028916269540786743, 88.3% complete
..........
Epoch: 0 Train loss: 0.033357348293066025, 88.6% complete
..........
Epoch: 0 Train loss: 0.0002217888250015676, 88.8% complete
..........
Epoch: 0 Train loss: 0.05278073623776436, 89.1% complete
..........
Epoch: 0 Train loss: 0.012081385590136051, 89.4% complete
..........
Epoch: 0 Train loss: 3.165731322951615e-05, 89.7% complete
..........
Epoch: 0 Train loss: 5.67145470995456e-05, 89.9% complete
..........
Epoch: 0 Train loss: 6.85571285430342e-05, 90.2% complete
..........
Epoch: 0 Train loss: 3.9265520172193646e-05, 90.5% complete
..........
Epoch: 0 Train loss: 0.022731028497219086, 90.7% complete
..........
Epoch: 0 Train loss: 2.3528060410171747e-05, 91.0% complete
..........
Epoch: 0 Train loss: 0.03838488087058067, 91.3% complete
..........
Epoch: 0 Train loss: 0.021789589896798134, 91.6% complete
..........
Epoch: 0 Train loss: 0.010933810845017433, 91.8% complete
..........
Epoch: 0 Train loss: 0.031624168157577515, 92.1% complete
..........
Epoch: 0 Train loss: 0.016347872093319893, 92.4% complete
..........
Epoch: 0 Train loss: 0.03155170753598213, 92.6% complete
..........
Epoch: 0 Train loss: 0.03938962519168854, 92.9% complete
..........
Epoch: 0 Train loss: 1.2203294318169355e-05, 93.2% complete
..........
Epoch: 0 Train loss: 0.00026190723292529583, 93.5% complete
..........
Epoch: 0 Train loss: 0.034471236169338226, 93.7% complete
..........
Epoch: 0 Train loss: 0.0012377304956316948, 94.0% complete
..........
Epoch: 0 Train loss: 0.01592675969004631, 94.3% complete
..........
Epoch: 0 Train loss: 0.03410038352012634, 94.5% complete
..........
Epoch: 0 Train loss: 0.026260411366820335, 94.8% complete
..........
Epoch: 0 Train loss: 0.046616315841674805, 95.1% complete
..........
Epoch: 0 Train loss: 0.01530429907143116, 95.4% complete
..........
Epoch: 0 Train loss: 0.0012354475911706686, 95.6% complete
..........
Epoch: 0 Train loss: 0.028598707169294357, 95.9% complete
..........
Epoch: 0 Train loss: 0.001568715670146048, 96.2% complete
..........
Epoch: 0 Train loss: 0.028548210859298706, 96.4% complete
..........
Epoch: 0 Train loss: 0.0007288870983757079, 96.7% complete
..........
Epoch: 0 Train loss: 0.025897085666656494, 97.0% complete
..........
Epoch: 0 Train loss: 1.7075100913643837e-05, 97.3% complete
..........
Epoch: 0 Train loss: 0.028073983266949654, 97.5% complete
..........
Epoch: 0 Train loss: 0.025555282831192017, 97.8% complete
..........
Epoch: 0 Train loss: 0.032002270221710205, 98.1% complete
..........
Epoch: 0 Train loss: 0.0002557837578933686, 98.3% complete
..........
Epoch: 0 Train loss: 0.028673842549324036, 98.6% complete
..........
Epoch: 0 Train loss: 0.008370477706193924, 98.9% complete
..........
Epoch: 0 Train loss: 0.020086754113435745, 99.2% complete
..........
Epoch: 0 Train loss: 0.006878640502691269, 99.4% complete
..........
Epoch: 0 Train loss: 5.1344919484108686e-05, 99.7% complete
..........
Epoch: 0 Train loss: 0.03529861941933632, 100.0% complete
..
Training complete
Validating epoch 0...
Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008168601430952549
Batch 0 Loss 0.008168601430952549
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006465421989560127
Batch 1 Loss 0.006465421989560127
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0072618527337908745
Batch 2 Loss 0.0072618527337908745
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006312462035566568
Batch 3 Loss 0.006312462035566568
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 3.07015361613594e-05
Batch 4 Loss 3.07015361613594e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 2.282969217048958e-05
Batch 5 Loss 2.282969217048958e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0069164372980594635
Batch 6 Loss 0.0069164372980594635
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008724030340090394
Batch 7 Loss 0.0008724030340090394
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008213984780013561
Batch 8 Loss 0.008213984780013561
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 4.307712515583262e-05
Batch 9 Loss 4.307712515583262e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001236937241628766
Batch 10 Loss 0.0001236937241628766
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003973711747676134
Batch 11 Loss 0.003973711747676134
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000832314311992377
Batch 12 Loss 0.000832314311992377
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 2.172782114939764e-05
Batch 13 Loss 2.172782114939764e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009264765307307243
Batch 14 Loss 0.009264765307307243
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008351746015250683
Batch 15 Loss 0.008351746015250683
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006050201132893562
Batch 16 Loss 0.006050201132893562
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004884524270892143
Batch 17 Loss 0.004884524270892143
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019206993747502565
Batch 18 Loss 0.0019206993747502565
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005596566945314407
Batch 19 Loss 0.005596566945314407
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 5.347812839318067e-05
Batch 20 Loss 5.347812839318067e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029827775433659554
Batch 21 Loss 0.0029827775433659554
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 3.0983130272943527e-05
Batch 22 Loss 3.0983130272943527e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014922776608727872
Batch 23 Loss 0.00014922776608727872
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00020857516210526228
Batch 24 Loss 0.00020857516210526228
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0059590633027255535
Batch 25 Loss 0.0059590633027255535
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006406755186617374
Batch 26 Loss 0.006406755186617374
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 6.899858999531716e-05
Batch 27 Loss 6.899858999531716e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006820095703005791
Batch 28 Loss 0.006820095703005791
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00158690195530653
Batch 29 Loss 0.00158690195530653
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1757085050921887e-05
Batch 30 Loss 2.1757085050921887e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01507346611469984
Batch 31 Loss 0.01507346611469984
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028545053210109472
Batch 32 Loss 0.0028545053210109472
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8413596737664193e-05
Batch 33 Loss 2.8413596737664193e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006911640986800194
Batch 34 Loss 0.006911640986800194
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006365921813994646
Batch 35 Loss 0.006365921813994646
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007888948544859886
Batch 36 Loss 0.007888948544859886
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006344674155116081
Batch 37 Loss 0.006344674155116081
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2964806703384966e-05
Batch 38 Loss 2.2964806703384966e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0031819327268749475
Batch 39 Loss 0.0031819327268749475
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006875183898955584
Batch 40 Loss 0.006875183898955584
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00330787873826921
Batch 41 Loss 0.00330787873826921
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1276245610788465e-05
Batch 42 Loss 4.1276245610788465e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 2.7389272872824222e-05
Batch 43 Loss 2.7389272872824222e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00719651160761714
Batch 44 Loss 0.00719651160761714
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006289574783295393
Batch 45 Loss 0.006289574783295393
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 3.471597301540896e-05
Batch 46 Loss 3.471597301540896e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008664418011903763
Batch 47 Loss 0.008664418011903763
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 3.2638272386975586e-05
Batch 48 Loss 3.2638272386975586e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007823426276445389
Batch 49 Loss 0.007823426276445389
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0021394321229308844
Batch 50 Loss 0.0021394321229308844
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002542713191360235
Batch 51 Loss 0.002542713191360235
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00605854531750083
Batch 52 Loss 0.00605854531750083
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009624019265174866
Batch 53 Loss 0.009624019265174866
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036491926293820143
Batch 54 Loss 0.0036491926293820143
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007066285237669945
Batch 55 Loss 0.007066285237669945
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 5.819228681502864e-05
Batch 56 Loss 5.819228681502864e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005572640802711248
Batch 57 Loss 0.0005572640802711248
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0043902210891246796
Batch 58 Loss 0.0043902210891246796
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0062554460018873215
Batch 59 Loss 0.0062554460018873215
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005532507319003344
Batch 60 Loss 0.005532507319003344
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007505375426262617
Batch 61 Loss 0.007505375426262617
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00823256280273199
Batch 62 Loss 0.00823256280273199
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004734981805086136
Batch 63 Loss 0.004734981805086136
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002171913511119783
Batch 64 Loss 0.0002171913511119783
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000291159754851833
Batch 65 Loss 0.000291159754851833
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008304687216877937
Batch 66 Loss 0.008304687216877937
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005116294603794813
Batch 67 Loss 0.005116294603794813
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005308032967150211
Batch 68 Loss 0.005308032967150211
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007276482880115509
Batch 69 Loss 0.007276482880115509
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029104305431246758
Batch 70 Loss 0.0029104305431246758
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00677860900759697
Batch 71 Loss 0.00677860900759697
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0020161170978099108
Batch 72 Loss 0.0020161170978099108
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 2.104191662510857e-05
Batch 73 Loss 2.104191662510857e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007194304373115301
Batch 74 Loss 0.007194304373115301
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00538581982254982
Batch 75 Loss 0.00538581982254982
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007350116968154907
Batch 76 Loss 0.007350116968154907
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006052930373698473
Batch 77 Loss 0.006052930373698473
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00039769287104718387
Batch 78 Loss 0.00039769287104718387
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 7.248718611663207e-05
Batch 79 Loss 7.248718611663207e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004313815850764513
Batch 80 Loss 0.004313815850764513
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00023066438734531403
Batch 81 Loss 0.00023066438734531403
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0051919445395469666
Batch 82 Loss 0.0051919445395469666
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006484779994934797
Batch 83 Loss 0.006484779994934797
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004110313486307859
Batch 84 Loss 0.004110313486307859
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010851899161934853
Batch 85 Loss 0.010851899161934853
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003770390059798956
Batch 86 Loss 0.003770390059798956
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010778474621474743
Batch 87 Loss 0.010778474621474743
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011124982498586178
Batch 88 Loss 0.011124982498586178
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 4.510596772888675e-05
Batch 89 Loss 4.510596772888675e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 2.3745735234115273e-05
Batch 90 Loss 2.3745735234115273e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00835318025201559
Batch 91 Loss 0.00835318025201559
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008478271774947643
Batch 92 Loss 0.008478271774947643
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 9.71101617324166e-05
Batch 93 Loss 9.71101617324166e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014724198263138533
Batch 94 Loss 0.0014724198263138533
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006265218369662762
Batch 95 Loss 0.006265218369662762
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004820359870791435
Batch 96 Loss 0.004820359870791435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006901378743350506
Batch 97 Loss 0.006901378743350506
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01248050294816494
Batch 98 Loss 0.01248050294816494
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005991524551063776
Batch 99 Loss 0.005991524551063776
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9480845367070287e-05
Batch 100 Loss 1.9480845367070287e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00517767108976841
Batch 101 Loss 0.00517767108976841
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017107437597587705
Batch 102 Loss 0.0017107437597587705
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 4.744663601741195e-05
Batch 103 Loss 4.744663601741195e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006746141705662012
Batch 104 Loss 0.006746141705662012
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 2.275083534186706e-05
Batch 105 Loss 2.275083534186706e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008358112536370754
Batch 106 Loss 0.008358112536370754
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036529824137687683
Batch 107 Loss 0.0036529824137687683
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0002985372266266495
Batch 108 Loss 0.0002985372266266495
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0017312385607510805
Batch 109 Loss 0.0017312385607510805
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008802250027656555
Batch 110 Loss 0.008802250027656555
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006501424126327038
Batch 111 Loss 0.006501424126327038
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 6.752322224201635e-05
Batch 112 Loss 6.752322224201635e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006989386864006519
Batch 113 Loss 0.006989386864006519
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 114. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006359860301017761
Batch 114 Loss 0.006359860301017761
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 115. Data shape torch.Size([8, 1, 64, 64]) Loss 6.074983684811741e-05
Batch 115 Loss 6.074983684811741e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 116. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002152085304260254
Batch 116 Loss 0.002152085304260254
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 117. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1886240574531257e-05
Batch 117 Loss 2.1886240574531257e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 118. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010396307334303856
Batch 118 Loss 0.010396307334303856
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 119. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00079402542905882
Batch 119 Loss 0.00079402542905882
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 120. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008858348242938519
Batch 120 Loss 0.008858348242938519
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 121. Data shape torch.Size([8, 1, 64, 64]) Loss 4.967045970261097e-05
Batch 121 Loss 4.967045970261097e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 122. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002392654772847891
Batch 122 Loss 0.002392654772847891
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 123. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009847162291407585
Batch 123 Loss 0.009847162291407585
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 124. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006052748300135136
Batch 124 Loss 0.006052748300135136
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 125. Data shape torch.Size([8, 1, 64, 64]) Loss 4.334549157647416e-05
Batch 125 Loss 4.334549157647416e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 126. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005721683148294687
Batch 126 Loss 0.005721683148294687
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 127. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1091072994749993e-05
Batch 127 Loss 2.1091072994749993e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 128. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0036755746696144342
Batch 128 Loss 0.0036755746696144342
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 129. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007250650320202112
Batch 129 Loss 0.007250650320202112
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 130. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006552203558385372
Batch 130 Loss 0.0006552203558385372
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 131. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4084110918920487e-05
Batch 131 Loss 2.4084110918920487e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 132. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0022498304024338722
Batch 132 Loss 0.0022498304024338722
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 133. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005401588510721922
Batch 133 Loss 0.005401588510721922
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 134. Data shape torch.Size([8, 1, 64, 64]) Loss 3.4543234505690634e-05
Batch 134 Loss 3.4543234505690634e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 135. Data shape torch.Size([8, 1, 64, 64]) Loss 5.631289241136983e-05
Batch 135 Loss 5.631289241136983e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 136. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1415631130803376e-05
Batch 136 Loss 4.1415631130803376e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 137. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003916756249964237
Batch 137 Loss 0.003916756249964237
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 138. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007532050833106041
Batch 138 Loss 0.007532050833106041
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 139. Data shape torch.Size([8, 1, 64, 64]) Loss 3.584937076084316e-05
Batch 139 Loss 3.584937076084316e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 140. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005002404563128948
Batch 140 Loss 0.005002404563128948
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 141. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0020855150651186705
Batch 141 Loss 0.0020855150651186705
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 142. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0035852224100381136
Batch 142 Loss 0.0035852224100381136
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 143. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006547372322529554
Batch 143 Loss 0.006547372322529554
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 144. Data shape torch.Size([8, 1, 64, 64]) Loss 2.917551319114864e-05
Batch 144 Loss 2.917551319114864e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 145. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00796306412667036
Batch 145 Loss 0.00796306412667036
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 146. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0027736129704862833
Batch 146 Loss 0.0027736129704862833
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 147. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006032226141542196
Batch 147 Loss 0.006032226141542196
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 148. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0054679443128407
Batch 148 Loss 0.0054679443128407
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 149. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009248100221157074
Batch 149 Loss 0.009248100221157074
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 150. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009520364925265312
Batch 150 Loss 0.009520364925265312
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 151. Data shape torch.Size([8, 1, 64, 64]) Loss 9.941065945895389e-05
Batch 151 Loss 9.941065945895389e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 152. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011118382215499878
Batch 152 Loss 0.011118382215499878
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 153. Data shape torch.Size([8, 1, 64, 64]) Loss 3.654909960459918e-05
Batch 153 Loss 3.654909960459918e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 154. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0077552227303385735
Batch 154 Loss 0.0077552227303385735
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 155. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004935481119900942
Batch 155 Loss 0.004935481119900942
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 156. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006171694025397301
Batch 156 Loss 0.006171694025397301
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 157. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005974535830318928
Batch 157 Loss 0.005974535830318928
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 158. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004243630915880203
Batch 158 Loss 0.004243630915880203
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 159. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011321274563670158
Batch 159 Loss 0.011321274563670158
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 160. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005587354768067598
Batch 160 Loss 0.005587354768067598
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 161. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007422461174428463
Batch 161 Loss 0.007422461174428463
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 162. Data shape torch.Size([8, 1, 64, 64]) Loss 2.8236107027623802e-05
Batch 162 Loss 2.8236107027623802e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 163. Data shape torch.Size([8, 1, 64, 64]) Loss 3.8625912566203624e-05
Batch 163 Loss 3.8625912566203624e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 164. Data shape torch.Size([8, 1, 64, 64]) Loss 7.334653491852805e-05
Batch 164 Loss 7.334653491852805e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 165. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008871255442500114
Batch 165 Loss 0.008871255442500114
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 166. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00575434323400259
Batch 166 Loss 0.00575434323400259
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 167. Data shape torch.Size([8, 1, 64, 64]) Loss 6.651642615906894e-05
Batch 167 Loss 6.651642615906894e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 168. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005158532876521349
Batch 168 Loss 0.005158532876521349
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 169. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011758033186197281
Batch 169 Loss 0.011758033186197281
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 170. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004557463806122541
Batch 170 Loss 0.004557463806122541
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 171. Data shape torch.Size([8, 1, 64, 64]) Loss 2.1313717297744006e-05
Batch 171 Loss 2.1313717297744006e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 172. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00013704554294236004
Batch 172 Loss 0.00013704554294236004
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 173. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001813418639358133
Batch 173 Loss 0.0001813418639358133
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 174. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007839936763048172
Batch 174 Loss 0.007839936763048172
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 175. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004096996039152145
Batch 175 Loss 0.004096996039152145
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 176. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008934174664318562
Batch 176 Loss 0.008934174664318562
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 177. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008356854668818414
Batch 177 Loss 0.0008356854668818414
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 178. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005882652942091227
Batch 178 Loss 0.005882652942091227
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 179. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007545545697212219
Batch 179 Loss 0.007545545697212219
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 180. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005418329034000635
Batch 180 Loss 0.005418329034000635
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 181. Data shape torch.Size([8, 1, 64, 64]) Loss 3.441674925852567e-05
Batch 181 Loss 3.441674925852567e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 182. Data shape torch.Size([8, 1, 64, 64]) Loss 3.4373137168586254e-05
Batch 182 Loss 3.4373137168586254e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 183. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0103836078196764
Batch 183 Loss 0.0103836078196764
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 184. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007142309565097094
Batch 184 Loss 0.007142309565097094
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 185. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003623975906521082
Batch 185 Loss 0.003623975906521082
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 186. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007405650801956654
Batch 186 Loss 0.007405650801956654
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 187. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0071682496927678585
Batch 187 Loss 0.0071682496927678585
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 188. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010451017878949642
Batch 188 Loss 0.010451017878949642
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 189. Data shape torch.Size([8, 1, 64, 64]) Loss 3.731877222890034e-05
Batch 189 Loss 3.731877222890034e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 190. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0024200561456382275
Batch 190 Loss 0.0024200561456382275
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 191. Data shape torch.Size([8, 1, 64, 64]) Loss 3.192418807884678e-05
Batch 191 Loss 3.192418807884678e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 192. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004495539236813784
Batch 192 Loss 0.004495539236813784
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 193. Data shape torch.Size([8, 1, 64, 64]) Loss 2.985628816531971e-05
Batch 193 Loss 2.985628816531971e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 194. Data shape torch.Size([8, 1, 64, 64]) Loss 3.288787411293015e-05
Batch 194 Loss 3.288787411293015e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 195. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006867659743875265
Batch 195 Loss 0.006867659743875265
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 196. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067330170422792435
Batch 196 Loss 0.0067330170422792435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 197. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0023939514067023993
Batch 197 Loss 0.0023939514067023993
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 198. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007032430148683488
Batch 198 Loss 0.0007032430148683488
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 199. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003682629205286503
Batch 199 Loss 0.003682629205286503
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 200. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019349107751622796
Batch 200 Loss 0.0019349107751622796
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 201. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004706248175352812
Batch 201 Loss 0.004706248175352812
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 202. Data shape torch.Size([8, 1, 64, 64]) Loss 3.550734254531562e-05
Batch 202 Loss 3.550734254531562e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 203. Data shape torch.Size([8, 1, 64, 64]) Loss 9.49124078033492e-05
Batch 203 Loss 9.49124078033492e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 204. Data shape torch.Size([8, 1, 64, 64]) Loss 3.660657966975123e-05
Batch 204 Loss 3.660657966975123e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 205. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005529417656362057
Batch 205 Loss 0.005529417656362057
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 206. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008278108201920986
Batch 206 Loss 0.008278108201920986
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 207. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0056595150381326675
Batch 207 Loss 0.0056595150381326675
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 208. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00712137296795845
Batch 208 Loss 0.00712137296795845
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 209. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009921730495989323
Batch 209 Loss 0.009921730495989323
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 210. Data shape torch.Size([8, 1, 64, 64]) Loss 3.512524563120678e-05
Batch 210 Loss 3.512524563120678e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 211. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2428568627219647e-05
Batch 211 Loss 2.2428568627219647e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 212. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008081471547484398
Batch 212 Loss 0.008081471547484398
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 213. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008370568975806236
Batch 213 Loss 0.008370568975806236
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 214. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003214799799025059
Batch 214 Loss 0.0003214799799025059
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 215. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00016752639203332365
Batch 215 Loss 0.00016752639203332365
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 216. Data shape torch.Size([8, 1, 64, 64]) Loss 3.035007102880627e-05
Batch 216 Loss 3.035007102880627e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 217. Data shape torch.Size([8, 1, 64, 64]) Loss 3.637719783000648e-05
Batch 217 Loss 3.637719783000648e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 218. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001686426403466612
Batch 218 Loss 0.0001686426403466612
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 219. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2406806237995625e-05
Batch 219 Loss 2.2406806237995625e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 220. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010528335347771645
Batch 220 Loss 0.010528335347771645
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 221. Data shape torch.Size([8, 1, 64, 64]) Loss 3.203748929081485e-05
Batch 221 Loss 3.203748929081485e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 222. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0006082556792534888
Batch 222 Loss 0.0006082556792534888
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 223. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028435627464205027
Batch 223 Loss 0.0028435627464205027
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 224. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007613733876496553
Batch 224 Loss 0.007613733876496553
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 225. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006853360217064619
Batch 225 Loss 0.006853360217064619
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 226. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003623517695814371
Batch 226 Loss 0.003623517695814371
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 227. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006825163960456848
Batch 227 Loss 0.006825163960456848
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 228. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01147636491805315
Batch 228 Loss 0.01147636491805315
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 229. Data shape torch.Size([8, 1, 64, 64]) Loss 5.390057776821777e-05
Batch 229 Loss 5.390057776821777e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 230. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00018154093413613737
Batch 230 Loss 0.00018154093413613737
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 231. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005653467029333115
Batch 231 Loss 0.005653467029333115
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 232. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006994859781116247
Batch 232 Loss 0.006994859781116247
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 233. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00588796054944396
Batch 233 Loss 0.00588796054944396
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 234. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00019736518152058125
Batch 234 Loss 0.00019736518152058125
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 235. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0038222232833504677
Batch 235 Loss 0.0038222232833504677
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 236. Data shape torch.Size([8, 1, 64, 64]) Loss 2.607973146950826e-05
Batch 236 Loss 2.607973146950826e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 237. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011474297381937504
Batch 237 Loss 0.011474297381937504
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 238. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011427248828113079
Batch 238 Loss 0.011427248828113079
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 239. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0057884627021849155
Batch 239 Loss 0.0057884627021849155
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 240. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007419195491820574
Batch 240 Loss 0.007419195491820574
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 241. Data shape torch.Size([8, 1, 64, 64]) Loss 7.098982314346358e-05
Batch 241 Loss 7.098982314346358e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 242. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008520870469510555
Batch 242 Loss 0.008520870469510555
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 243. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0083945756778121
Batch 243 Loss 0.0083945756778121
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 244. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005065031349658966
Batch 244 Loss 0.005065031349658966
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 245. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007210387382656336
Batch 245 Loss 0.007210387382656336
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 246. Data shape torch.Size([8, 1, 64, 64]) Loss 5.615480768028647e-05
Batch 246 Loss 5.615480768028647e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 247. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007795820478349924
Batch 247 Loss 0.007795820478349924
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 248. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003211151110008359
Batch 248 Loss 0.003211151110008359
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 249. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007619502954185009
Batch 249 Loss 0.007619502954185009
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 250. Data shape torch.Size([8, 1, 64, 64]) Loss 3.5108401789329946e-05
Batch 250 Loss 3.5108401789329946e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 251. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007695574313402176
Batch 251 Loss 0.007695574313402176
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 252. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005514211370609701
Batch 252 Loss 0.0005514211370609701
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 253. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00010890101111726835
Batch 253 Loss 0.00010890101111726835
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 254. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067249443382024765
Batch 254 Loss 0.0067249443382024765
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 255. Data shape torch.Size([8, 1, 64, 64]) Loss 7.903702498879284e-05
Batch 255 Loss 7.903702498879284e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 256. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008447212749160826
Batch 256 Loss 0.0008447212749160826
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 257. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007619622629135847
Batch 257 Loss 0.007619622629135847
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 258. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006548427045345306
Batch 258 Loss 0.006548427045345306
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 259. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009721328504383564
Batch 259 Loss 0.009721328504383564
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 260. Data shape torch.Size([8, 1, 64, 64]) Loss 5.903953569941223e-05
Batch 260 Loss 5.903953569941223e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 261. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008292878977954388
Batch 261 Loss 0.008292878977954388
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 262. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005983264651149511
Batch 262 Loss 0.005983264651149511
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 263. Data shape torch.Size([8, 1, 64, 64]) Loss 2.119789132848382e-05
Batch 263 Loss 2.119789132848382e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 264. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007189664989709854
Batch 264 Loss 0.007189664989709854
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 265. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00312270550057292
Batch 265 Loss 0.00312270550057292
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 266. Data shape torch.Size([8, 1, 64, 64]) Loss 3.912317333742976e-05
Batch 266 Loss 3.912317333742976e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 267. Data shape torch.Size([8, 1, 64, 64]) Loss 8.171456283889711e-05
Batch 267 Loss 8.171456283889711e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 268. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0032772491686046124
Batch 268 Loss 0.0032772491686046124
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 269. Data shape torch.Size([8, 1, 64, 64]) Loss 2.9290989914443344e-05
Batch 269 Loss 2.9290989914443344e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 270. Data shape torch.Size([8, 1, 64, 64]) Loss 0.001432171557098627
Batch 270 Loss 0.001432171557098627
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 271. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007130753714591265
Batch 271 Loss 0.007130753714591265
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 272. Data shape torch.Size([8, 1, 64, 64]) Loss 9.143378701992333e-05
Batch 272 Loss 9.143378701992333e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 273. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0003055267734453082
Batch 273 Loss 0.0003055267734453082
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 274. Data shape torch.Size([8, 1, 64, 64]) Loss 4.4822772906627506e-05
Batch 274 Loss 4.4822772906627506e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 275. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008410912938416004
Batch 275 Loss 0.008410912938416004
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 276. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005902689881622791
Batch 276 Loss 0.005902689881622791
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 277. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010223735123872757
Batch 277 Loss 0.010223735123872757
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 278. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0070869410410523415
Batch 278 Loss 0.0070869410410523415
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 279. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010467472486197948
Batch 279 Loss 0.010467472486197948
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 280. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00020450909505598247
Batch 280 Loss 0.00020450909505598247
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 281. Data shape torch.Size([8, 1, 64, 64]) Loss 9.156487067230046e-05
Batch 281 Loss 9.156487067230046e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 282. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0029074244666844606
Batch 282 Loss 0.0029074244666844606
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 283. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006745725404471159
Batch 283 Loss 0.006745725404471159
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 284. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008552643470466137
Batch 284 Loss 0.008552643470466137
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 285. Data shape torch.Size([8, 1, 64, 64]) Loss 3.622553776949644e-05
Batch 285 Loss 3.622553776949644e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 286. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008289901539683342
Batch 286 Loss 0.008289901539683342
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 287. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00894208624958992
Batch 287 Loss 0.00894208624958992
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 288. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028356241527944803
Batch 288 Loss 0.0028356241527944803
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 289. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067467824555933475
Batch 289 Loss 0.0067467824555933475
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 290. Data shape torch.Size([8, 1, 64, 64]) Loss 2.4492874217685312e-05
Batch 290 Loss 2.4492874217685312e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 291. Data shape torch.Size([8, 1, 64, 64]) Loss 3.696330531965941e-05
Batch 291 Loss 3.696330531965941e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 292. Data shape torch.Size([8, 1, 64, 64]) Loss 3.604721132433042e-05
Batch 292 Loss 3.604721132433042e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 293. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004844007082283497
Batch 293 Loss 0.004844007082283497
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 294. Data shape torch.Size([8, 1, 64, 64]) Loss 5.2241230150684714e-05
Batch 294 Loss 5.2241230150684714e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 295. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011482782429084182
Batch 295 Loss 0.0011482782429084182
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 296. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009595873998478055
Batch 296 Loss 0.0009595873998478055
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 297. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005687017925083637
Batch 297 Loss 0.005687017925083637
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 298. Data shape torch.Size([8, 1, 64, 64]) Loss 3.129815013380721e-05
Batch 298 Loss 3.129815013380721e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 299. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007294270675629377
Batch 299 Loss 0.007294270675629377
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 300. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013096336275339127
Batch 300 Loss 0.013096336275339127
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 301. Data shape torch.Size([8, 1, 64, 64]) Loss 2.2336833353620023e-05
Batch 301 Loss 2.2336833353620023e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 302. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009069274179637432
Batch 302 Loss 0.009069274179637432
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 303. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006519881542772055
Batch 303 Loss 0.006519881542772055
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 304. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011409171856939793
Batch 304 Loss 0.0011409171856939793
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 305. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008618869818747044
Batch 305 Loss 0.008618869818747044
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 306. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003340019378811121
Batch 306 Loss 0.003340019378811121
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 307. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005817476660013199
Batch 307 Loss 0.005817476660013199
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 308. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00839806068688631
Batch 308 Loss 0.00839806068688631
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 309. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006796895992010832
Batch 309 Loss 0.006796895992010832
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 310. Data shape torch.Size([8, 1, 64, 64]) Loss 2.121211582561955e-05
Batch 310 Loss 2.121211582561955e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 311. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0096763139590621
Batch 311 Loss 0.0096763139590621
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 312. Data shape torch.Size([8, 1, 64, 64]) Loss 4.43293247371912e-05
Batch 312 Loss 4.43293247371912e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 313. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00021404368453659117
Batch 313 Loss 0.00021404368453659117
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 314. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0010395536664873362
Batch 314 Loss 0.0010395536664873362
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 315. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011788280680775642
Batch 315 Loss 0.011788280680775642
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 316. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01206532958894968
Batch 316 Loss 0.01206532958894968
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 317. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00015111593529582024
Batch 317 Loss 0.00015111593529582024
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 318. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008140160702168941
Batch 318 Loss 0.008140160702168941
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 319. Data shape torch.Size([8, 1, 64, 64]) Loss 2.377277269260958e-05
Batch 319 Loss 2.377277269260958e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 320. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00021151703549548984
Batch 320 Loss 0.00021151703549548984
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 321. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002088879933580756
Batch 321 Loss 0.002088879933580756
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 322. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008783659897744656
Batch 322 Loss 0.008783659897744656
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 323. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009149047546088696
Batch 323 Loss 0.009149047546088696
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 324. Data shape torch.Size([8, 1, 64, 64]) Loss 0.014571870677173138
Batch 324 Loss 0.014571870677173138
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 325. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008326138369739056
Batch 325 Loss 0.008326138369739056
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 326. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005454491823911667
Batch 326 Loss 0.005454491823911667
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 327. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009164116345345974
Batch 327 Loss 0.009164116345345974
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 328. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00035282448516227305
Batch 328 Loss 0.00035282448516227305
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 329. Data shape torch.Size([8, 1, 64, 64]) Loss 0.000146455509820953
Batch 329 Loss 0.000146455509820953
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 330. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005165452603250742
Batch 330 Loss 0.005165452603250742
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 331. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008735724724829197
Batch 331 Loss 0.008735724724829197
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 332. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0061259036883711815
Batch 332 Loss 0.0061259036883711815
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 333. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00798020139336586
Batch 333 Loss 0.00798020139336586
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 334. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0001223467115778476
Batch 334 Loss 0.0001223467115778476
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 335. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002375143812969327
Batch 335 Loss 0.002375143812969327
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 336. Data shape torch.Size([8, 1, 64, 64]) Loss 3.920443123206496e-05
Batch 336 Loss 3.920443123206496e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 337. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00817535538226366
Batch 337 Loss 0.00817535538226366
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 338. Data shape torch.Size([8, 1, 64, 64]) Loss 4.291061486583203e-05
Batch 338 Loss 4.291061486583203e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 339. Data shape torch.Size([8, 1, 64, 64]) Loss 3.618618939071894e-05
Batch 339 Loss 3.618618939071894e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 340. Data shape torch.Size([8, 1, 64, 64]) Loss 3.616346657508984e-05
Batch 340 Loss 3.616346657508984e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 341. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006871366407722235
Batch 341 Loss 0.006871366407722235
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 342. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005396498832851648
Batch 342 Loss 0.005396498832851648
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 343. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0024511658120900393
Batch 343 Loss 0.0024511658120900393
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 344. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003565188031643629
Batch 344 Loss 0.003565188031643629
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 345. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008577996864914894
Batch 345 Loss 0.008577996864914894
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 346. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007882680743932724
Batch 346 Loss 0.007882680743932724
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 347. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005327272694557905
Batch 347 Loss 0.005327272694557905
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 348. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014037045184522867
Batch 348 Loss 0.00014037045184522867
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 349. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0007994707557372749
Batch 349 Loss 0.0007994707557372749
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 350. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004686781205236912
Batch 350 Loss 0.004686781205236912
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 351. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007099040783941746
Batch 351 Loss 0.007099040783941746
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 352. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0083808284252882
Batch 352 Loss 0.0083808284252882
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 353. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008604445494711399
Batch 353 Loss 0.008604445494711399
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 354. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002843981608748436
Batch 354 Loss 0.002843981608748436
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 355. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007309306412935257
Batch 355 Loss 0.007309306412935257
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 356. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007422123569995165
Batch 356 Loss 0.007422123569995165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 357. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0042745135724544525
Batch 357 Loss 0.0042745135724544525
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 358. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008877096697688103
Batch 358 Loss 0.008877096697688103
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 359. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007439689245074987
Batch 359 Loss 0.007439689245074987
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 360. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0012228902196511626
Batch 360 Loss 0.0012228902196511626
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 361. Data shape torch.Size([8, 1, 64, 64]) Loss 4.623197310138494e-05
Batch 361 Loss 4.623197310138494e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 362. Data shape torch.Size([8, 1, 64, 64]) Loss 4.1421939386054873e-05
Batch 362 Loss 4.1421939386054873e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 363. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0005400342051871121
Batch 363 Loss 0.0005400342051871121
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 364. Data shape torch.Size([8, 1, 64, 64]) Loss 3.0471972422674298e-05
Batch 364 Loss 3.0471972422674298e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 365. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0030821161344647408
Batch 365 Loss 0.0030821161344647408
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 366. Data shape torch.Size([8, 1, 64, 64]) Loss 9.722317918203771e-05
Batch 366 Loss 9.722317918203771e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 367. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0009651248692534864
Batch 367 Loss 0.0009651248692534864
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 368. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002294407458975911
Batch 368 Loss 0.002294407458975911
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 369. Data shape torch.Size([8, 1, 64, 64]) Loss 2.7002162823919207e-05
Batch 369 Loss 2.7002162823919207e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 370. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009221662767231464
Batch 370 Loss 0.009221662767231464
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 371. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007234987802803516
Batch 371 Loss 0.007234987802803516
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 372. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00611613504588604
Batch 372 Loss 0.00611613504588604
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 373. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007127345539629459
Batch 373 Loss 0.007127345539629459
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 374. Data shape torch.Size([8, 1, 64, 64]) Loss 5.624062760034576e-05
Batch 374 Loss 5.624062760034576e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 375. Data shape torch.Size([8, 1, 64, 64]) Loss 4.7680907300673425e-05
Batch 375 Loss 4.7680907300673425e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 376. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00041429302655160427
Batch 376 Loss 0.00041429302655160427
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 377. Data shape torch.Size([8, 1, 64, 64]) Loss 6.01989813731052e-05
Batch 377 Loss 6.01989813731052e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 378. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006631061900407076
Batch 378 Loss 0.006631061900407076
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 379. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002308852272108197
Batch 379 Loss 0.002308852272108197
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 380. Data shape torch.Size([8, 1, 64, 64]) Loss 5.176034028409049e-05
Batch 380 Loss 5.176034028409049e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 381. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0011850392911583185
Batch 381 Loss 0.0011850392911583185
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 382. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0067588468082249165
Batch 382 Loss 0.0067588468082249165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 383. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005600184202194214
Batch 383 Loss 0.005600184202194214
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 384. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008458182215690613
Batch 384 Loss 0.008458182215690613
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 385. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010097338818013668
Batch 385 Loss 0.010097338818013668
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 386. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004686505999416113
Batch 386 Loss 0.004686505999416113
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 387. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00637020543217659
Batch 387 Loss 0.00637020543217659
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 388. Data shape torch.Size([8, 1, 64, 64]) Loss 0.002564371330663562
Batch 388 Loss 0.002564371330663562
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 389. Data shape torch.Size([8, 1, 64, 64]) Loss 7.867338717915118e-05
Batch 389 Loss 7.867338717915118e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 390. Data shape torch.Size([8, 1, 64, 64]) Loss 0.012270547449588776
Batch 390 Loss 0.012270547449588776
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 391. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010295814834535122
Batch 391 Loss 0.010295814834535122
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 392. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00717568164691329
Batch 392 Loss 0.00717568164691329
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 393. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007286964915692806
Batch 393 Loss 0.007286964915692806
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 394. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006813183426856995
Batch 394 Loss 0.006813183426856995
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 395. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0008316179155372083
Batch 395 Loss 0.0008316179155372083
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 396. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008967832662165165
Batch 396 Loss 0.008967832662165165
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 397. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007769363932311535
Batch 397 Loss 0.007769363932311535
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 398. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005651854909956455
Batch 398 Loss 0.005651854909956455
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 399. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009595879353582859
Batch 399 Loss 0.009595879353582859
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 400. Data shape torch.Size([8, 1, 64, 64]) Loss 0.01161147654056549
Batch 400 Loss 0.01161147654056549
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 401. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00046045551425777376
Batch 401 Loss 0.00046045551425777376
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 402. Data shape torch.Size([8, 1, 64, 64]) Loss 0.003470475785434246
Batch 402 Loss 0.003470475785434246
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 403. Data shape torch.Size([8, 1, 64, 64]) Loss 2.9601120331790298e-05
Batch 403 Loss 2.9601120331790298e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 404. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007633904926478863
Batch 404 Loss 0.007633904926478863
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 405. Data shape torch.Size([8, 1, 64, 64]) Loss 0.008453773334622383
Batch 405 Loss 0.008453773334622383
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 406. Data shape torch.Size([8, 1, 64, 64]) Loss 4.552467726171017e-05
Batch 406 Loss 4.552467726171017e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 407. Data shape torch.Size([8, 1, 64, 64]) Loss 6.718738586641848e-05
Batch 407 Loss 6.718738586641848e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 408. Data shape torch.Size([8, 1, 64, 64]) Loss 3.114267747150734e-05
Batch 408 Loss 3.114267747150734e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 409. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00017763848882168531
Batch 409 Loss 0.00017763848882168531
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 410. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011657959781587124
Batch 410 Loss 0.011657959781587124
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 411. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007422635331749916
Batch 411 Loss 0.007422635331749916
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 412. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00023808976402506232
Batch 412 Loss 0.00023808976402506232
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 413. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007155865430831909
Batch 413 Loss 0.007155865430831909
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 414. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0014190978836268187
Batch 414 Loss 0.0014190978836268187
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 415. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00679470831528306
Batch 415 Loss 0.00679470831528306
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 416. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009193559177219868
Batch 416 Loss 0.009193559177219868
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 417. Data shape torch.Size([8, 1, 64, 64]) Loss 0.010922965593636036
Batch 417 Loss 0.010922965593636036
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 418. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0034214514307677746
Batch 418 Loss 0.0034214514307677746
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 419. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006031975615769625
Batch 419 Loss 0.006031975615769625
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 420. Data shape torch.Size([8, 1, 64, 64]) Loss 7.462617213604972e-05
Batch 420 Loss 7.462617213604972e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 421. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00516008585691452
Batch 421 Loss 0.00516008585691452
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 422. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00265966122969985
Batch 422 Loss 0.00265966122969985
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 423. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0057989852502942085
Batch 423 Loss 0.0057989852502942085
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 424. Data shape torch.Size([8, 1, 64, 64]) Loss 3.721355460584164e-05
Batch 424 Loss 3.721355460584164e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 425. Data shape torch.Size([8, 1, 64, 64]) Loss 0.009006454609334469
Batch 425 Loss 0.009006454609334469
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 426. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004248376935720444
Batch 426 Loss 0.004248376935720444
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 427. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025595324113965034
Batch 427 Loss 0.0025595324113965034
target unique values: [0 1]
prediction softmax sample: [1. 0. 0.]
Batch 428. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005785802844911814
Batch 428 Loss 0.005785802844911814
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 429. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00674794614315033
Batch 429 Loss 0.00674794614315033
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 430. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007359025068581104
Batch 430 Loss 0.007359025068581104
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 431. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0028486207593232393
Batch 431 Loss 0.0028486207593232393
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 432. Data shape torch.Size([8, 1, 64, 64]) Loss 3.406368341529742e-05
Batch 432 Loss 3.406368341529742e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 433. Data shape torch.Size([8, 1, 64, 64]) Loss 4.3856853153556585e-05
Batch 433 Loss 4.3856853153556585e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 434. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00014082965208217502
Batch 434 Loss 0.00014082965208217502
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 435. Data shape torch.Size([8, 1, 64, 64]) Loss 0.013932340778410435
Batch 435 Loss 0.013932340778410435
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 436. Data shape torch.Size([8, 1, 64, 64]) Loss 0.006452345289289951
Batch 436 Loss 0.006452345289289951
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 437. Data shape torch.Size([8, 1, 64, 64]) Loss 3.0196868465282023e-05
Batch 437 Loss 3.0196868465282023e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 438. Data shape torch.Size([8, 1, 64, 64]) Loss 4.2521729483269155e-05
Batch 438 Loss 4.2521729483269155e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 439. Data shape torch.Size([8, 1, 64, 64]) Loss 4.6215289330575615e-05
Batch 439 Loss 4.6215289330575615e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 440. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005988435819745064
Batch 440 Loss 0.005988435819745064
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 441. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005179116502404213
Batch 441 Loss 0.005179116502404213
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 442. Data shape torch.Size([8, 1, 64, 64]) Loss 1.9790975784417242e-05
Batch 442 Loss 1.9790975784417242e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 443. Data shape torch.Size([8, 1, 64, 64]) Loss 2.5510300474707037e-05
Batch 443 Loss 2.5510300474707037e-05
target unique values: [0]
prediction softmax sample: [1. 0. 0.]
Batch 444. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004377398174256086
Batch 444 Loss 0.004377398174256086
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 445. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0019733184017241
Batch 445 Loss 0.0019733184017241
target unique values: [0 2]
prediction softmax sample: [1. 0. 0.]
Batch 446. Data shape torch.Size([8, 1, 64, 64]) Loss 0.011885824613273144
Batch 446 Loss 0.011885824613273144
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 447. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00809855293482542
Batch 447 Loss 0.00809855293482542
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 448. Data shape torch.Size([8, 1, 64, 64]) Loss 0.005968101788312197
Batch 448 Loss 0.005968101788312197
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 449. Data shape torch.Size([8, 1, 64, 64]) Loss 0.007940024137496948
Batch 449 Loss 0.007940024137496948
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 450. Data shape torch.Size([8, 1, 64, 64]) Loss 0.00663289288058877
Batch 450 Loss 0.00663289288058877
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 451. Data shape torch.Size([8, 1, 64, 64]) Loss 0.004940781742334366
Batch 451 Loss 0.004940781742334366
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Batch 452. Data shape torch.Size([8, 1, 64, 64]) Loss 0.0025069324765354395
Batch 452 Loss 0.0025069324765354395
target unique values: [0 1 2]
prediction softmax sample: [1. 0. 0.]
Validation complete
Run complete. Total time: 00:03:23
Testing...
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 0: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_318.nii.gz Dice 0.7687. 3.85% complete
Jaccard: 0.6242690058479532 sensitivity : 0.7286689419795221 specificity :0.9960279931908455
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 1: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_197.nii.gz Dice 0.7430. 7.69% complete
Jaccard: 0.5910815939278937 sensitivity : 0.7379330766952917 specificity :0.9944900867532229
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 2: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_233.nii.gz Dice 0.8174. 11.54% complete
Jaccard: 0.6912 sensitivity : 0.8184401641932428 specificity :0.9955833667926758
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (32, 64, 64)
🛠 Test Sample 3: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_123.nii.gz Dice 0.8094. 15.38% complete
Jaccard: 0.6798488664987405 sensitivity : 0.8358624961288325 specificity :0.9942038281329443
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 4: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_390.nii.gz Dice 0.7909. 19.23% complete
Jaccard: 0.654178674351585 sensitivity : 0.7678351783517835 specificity :0.996292553610331
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 5: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_024.nii.gz Dice 0.8109. 23.08% complete
Jaccard: 0.6819444444444445 sensitivity : 0.7310173697270471 specificity :0.9980872983418855
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 6: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_101.nii.gz Dice 0.7673. 26.92% complete
Jaccard: 0.6224660147865491 sensitivity : 0.7262103505843072 specificity :0.9958362875533497
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (32, 64, 64)
🛠 Test Sample 7: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_135.nii.gz Dice 0.8305. 30.77% complete
Jaccard: 0.7100987575661039 sensitivity : 0.8478508938759984 specificity :0.9960293671122599
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 8: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_276.nii.gz Dice 0.8106. 34.62% complete
Jaccard: 0.6814516129032258 sensitivity : 0.7422454021410925 specificity :0.9976738693215571
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 9: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_177.nii.gz Dice 0.8246. 38.46% complete
Jaccard: 0.701507882111035 sensitivity : 0.7894330890860007 specificity :0.9975485574203281
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 10: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_042.nii.gz Dice 0.7777. 42.31% complete
Jaccard: 0.636215334420881 sensitivity : 0.7096438783467637 specificity :0.9969940083274094
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 11: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_282.nii.gz Dice 0.7629. 46.15% complete
Jaccard: 0.6166934189406099 sensitivity : 0.7951158940397351 specificity :0.9953130028966849
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 12: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_092.nii.gz Dice 0.7809. 50.00% complete
Jaccard: 0.640581330352152 sensitivity : 0.7294716740929345 specificity :0.9971410960880228
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (36, 64, 64)
🛠 Test Sample 13: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_108.nii.gz Dice 0.7649. 53.85% complete
Jaccard: 0.6193005754758742 sensitivity : 0.7101522842639594 specificity :0.9959725744864684
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 14: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_252.nii.gz Dice 0.7540. 57.69% complete
Jaccard: 0.6051524710830705 sensitivity : 0.6439160839160839 specificity :0.9984524622069646
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 15: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_094.nii.gz Dice 0.8190. 61.54% complete
Jaccard: 0.6934537246049661 sensitivity : 0.7624720774385704 specificity :0.99735521273719
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 16: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_017.nii.gz Dice 0.7948. 65.38% complete
Jaccard: 0.659541591552923 sensitivity : 0.7363427257044278 specificity :0.9971047025349937
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 17: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_001.nii.gz Dice 0.7320. 69.23% complete
Jaccard: 0.5772442588726514 sensitivity : 0.7503392130257802 specificity :0.9937042418026949
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 18: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_154.nii.gz Dice 0.8077. 73.08% complete
Jaccard: 0.677365799667958 sensitivity : 0.7516119128031932 specificity :0.9974518746921908
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (33, 64, 64)
🛠 Test Sample 19: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_259.nii.gz Dice 0.6950. 76.92% complete
Jaccard: 0.532603025560772 sensitivity : 0.6993150684931507 specificity :0.9930887423628335
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (38, 64, 64)
🛠 Test Sample 20: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_158.nii.gz Dice 0.7955. 80.77% complete
Jaccard: 0.6604846686449061 sensitivity : 0.7828253223915592 specificity :0.9958485509340761
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 21: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_107.nii.gz Dice 0.7780. 84.62% complete
Jaccard: 0.6366415804327376 sensitivity : 0.6860111505321845 specificity :0.997805098483653
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 22: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_310.nii.gz Dice 0.7011. 88.46% complete
Jaccard: 0.5397627416520211 sensitivity : 0.6512059369202227 specificity :0.9944192510763896
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (37, 64, 64)
🛠 Test Sample 23: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_280.nii.gz Dice 0.7079. 92.31% complete
Jaccard: 0.5478134110787172 sensitivity : 0.7190968235744355 specificity :0.9945145327953054
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 24: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_380.nii.gz Dice 0.7721. 96.15% complete
Jaccard: 0.6287581699346405 sensitivity : 0.7001455604075691 specificity :0.9972127925674469
[TEST] pred_label type: <class 'numpy.ndarray'>, shape: (35, 64, 64)
🛠 Test Sample 25: pred_label=<class 'numpy.ndarray'>, x['seg']=<class 'numpy.ndarray'>
hippocampus_006.nii.gz Dice 0.8238. 100.00% complete
Jaccard: 0.7003899480069324 sensitivity : 0.758386113065916 specificity :0.9974622026355708

Testing complete.
(medai) root@85308d459df9725d6ae1cfd5cbadd4e98c4702dc-d98cddf88-wsspd:/home/workspace/src# 